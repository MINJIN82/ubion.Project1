{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 파일 불러오기\n",
    "df = pd.read_excel(\"./xlsx/Join_data.xlsx\", index_col = 0)    \n",
    "df = df.set_index(\"DateTime\")\n",
    "\n",
    "# 대비 계산\n",
    "df['대비_irs_1Y'] = df['1Y_Mid_irs'] - df['1Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_2Y'] = df['2Y_Mid_irs'] - df['2Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_3Y'] = df['3Y_Mid_irs'] - df['3Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_5Y'] = df['5Y_Mid_irs'] - df['5Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_10Y'] = df['10Y_Mid_irs'] - df['10Y_Mid_irs'].shift(1) \n",
    "\n",
    "df['대비_crs_1Y'] = df['1Y_Mid_crs'] - df['1Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_2Y'] = df['2Y_Mid_crs'] - df['2Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_3Y'] = df['3Y_Mid_crs'] - df['3Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_5Y'] = df['5Y_Mid_crs'] - df['5Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_10Y'] = df['10Y_Mid_crs'] - df['10Y_Mid_crs'].shift(1)\n",
    "\n",
    "df['대비_swapbasis_1Y'] = df['1Y_베이시스']-df['1Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_2Y'] = df['2Y_베이시스']-df['2Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_3Y'] = df['3Y_베이시스']-df['3Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_5Y'] = df['5Y_베이시스']-df['5Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_10Y'] = df['10Y_베이시스']-df['10Y_베이시스'].shift(1)\n",
    "\n",
    "df['대비_국고_1Y'] = df['국고1년']-df['국고1년'].shift(1)\n",
    "df['대비_국고_3Y'] = df['국고3년']-df['국고3년'].shift(1)\n",
    "df['대비_국고_5Y'] = df['국고5년']-df['국고5년'].shift(1)\n",
    "df['대비_국고_10Y'] = df['국고10년']-df['국고10년'].shift(1)\n",
    "\n",
    "df['대비_통안_1Y'] = df['통안364일']-df['통안364일'].shift(1)\n",
    "df['대비_통안_2Y'] = df['통안2년']-df['통안2년'].shift(1)\n",
    "\n",
    "df['대비_ndf'] = df['Mid_ndf']-df['Mid_ndf'].shift(1)\n",
    "df['스왑포인트_1M'] = df[\"M1_스왑포인트\"]/100 \n",
    "df['전일종가_ex'] = df['종가_ex'].shift(1)\n",
    "df['종가_NDF_차이'] = df['전일종가_ex'] - df['Mid_ndf']\n",
    "\n",
    "# 필요한 칼럼만 추출\n",
    "df_1 = df[['대비_irs_1Y', '대비_irs_2Y', '대비_irs_3Y', '대비_irs_5Y', '대비_irs_10Y',\n",
    "           '대비_crs_1Y', '대비_crs_2Y', '대비_crs_3Y', '대비_crs_5Y', '대비_crs_10Y', \n",
    "           '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y', '대비_swapbasis_10Y',\n",
    "           '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y', \n",
    "           '대비_통안_1Y', '대비_통안_2Y', '대비_ndf', '스왑포인트_1M', '전일종가_ex', \n",
    "           '종가_ex', '종가_NDF_차이' ]] \n",
    "\n",
    "# 결측치 제거\n",
    "df_1 = df_1.dropna()                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_swapbasis_1Y</th>\n",
       "      <th>대비_swapbasis_2Y</th>\n",
       "      <th>대비_swapbasis_3Y</th>\n",
       "      <th>대비_swapbasis_5Y</th>\n",
       "      <th>대비_swapbasis_10Y</th>\n",
       "      <th>대비_국고_1Y</th>\n",
       "      <th>대비_국고_3Y</th>\n",
       "      <th>대비_국고_5Y</th>\n",
       "      <th>대비_국고_10Y</th>\n",
       "      <th>대비_통안_1Y</th>\n",
       "      <th>대비_통안_2Y</th>\n",
       "      <th>스왑포인트_1M</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_NDF_차이</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>1.544080</td>\n",
       "      <td>1.437744</td>\n",
       "      <td>1.627899</td>\n",
       "      <td>1.488363</td>\n",
       "      <td>-1.698057</td>\n",
       "      <td>-0.646622</td>\n",
       "      <td>-1.079749</td>\n",
       "      <td>-1.027569</td>\n",
       "      <td>-0.325920</td>\n",
       "      <td>-0.625160</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>-0.495597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.286831</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>-0.910410</td>\n",
       "      <td>-2.158344</td>\n",
       "      <td>-1.132651</td>\n",
       "      <td>-0.323869</td>\n",
       "      <td>-1.890219</td>\n",
       "      <td>-1.798403</td>\n",
       "      <td>-0.217574</td>\n",
       "      <td>-0.125723</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>-0.415698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350946</td>\n",
       "      <td>-0.873707</td>\n",
       "      <td>-0.803456</td>\n",
       "      <td>-1.091718</td>\n",
       "      <td>-0.832269</td>\n",
       "      <td>0.563566</td>\n",
       "      <td>0.160261</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.107465</td>\n",
       "      <td>0.123996</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.423243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>0.286831</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>-1.454333</td>\n",
       "      <td>-1.661066</td>\n",
       "      <td>-0.567245</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>-0.109228</td>\n",
       "      <td>-0.125723</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>0.003773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.390150</td>\n",
       "      <td>-0.323199</td>\n",
       "      <td>-0.729102</td>\n",
       "      <td>-1.163788</td>\n",
       "      <td>-0.567245</td>\n",
       "      <td>-0.323869</td>\n",
       "      <td>-0.539435</td>\n",
       "      <td>-0.513680</td>\n",
       "      <td>-0.109228</td>\n",
       "      <td>-0.125723</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>-0.092772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-0.196727</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>-0.366487</td>\n",
       "      <td>-0.003472</td>\n",
       "      <td>-1.132651</td>\n",
       "      <td>-0.969375</td>\n",
       "      <td>-1.890219</td>\n",
       "      <td>-2.312292</td>\n",
       "      <td>-0.109228</td>\n",
       "      <td>-0.625160</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>0.213508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.196727</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>0.177437</td>\n",
       "      <td>0.162288</td>\n",
       "      <td>0.563566</td>\n",
       "      <td>-0.485246</td>\n",
       "      <td>-0.539435</td>\n",
       "      <td>-0.770625</td>\n",
       "      <td>0.107465</td>\n",
       "      <td>-0.000863</td>\n",
       "      <td>-0.987488</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>0.183546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.770388</td>\n",
       "      <td>0.797401</td>\n",
       "      <td>0.902668</td>\n",
       "      <td>0.825326</td>\n",
       "      <td>-0.001839</td>\n",
       "      <td>-0.485246</td>\n",
       "      <td>-1.349905</td>\n",
       "      <td>-1.541458</td>\n",
       "      <td>0.215812</td>\n",
       "      <td>-0.125723</td>\n",
       "      <td>-0.851696</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>-0.189317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>-0.003304</td>\n",
       "      <td>-0.163113</td>\n",
       "      <td>-0.366487</td>\n",
       "      <td>-0.832269</td>\n",
       "      <td>0.563566</td>\n",
       "      <td>0.644391</td>\n",
       "      <td>0.811350</td>\n",
       "      <td>2.055766</td>\n",
       "      <td>0.215812</td>\n",
       "      <td>0.373715</td>\n",
       "      <td>-0.942224</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>0.489826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-0.583573</td>\n",
       "      <td>-0.163113</td>\n",
       "      <td>0.902668</td>\n",
       "      <td>0.493807</td>\n",
       "      <td>-2.263463</td>\n",
       "      <td>-2.099012</td>\n",
       "      <td>-3.241004</td>\n",
       "      <td>-2.055348</td>\n",
       "      <td>-0.325920</td>\n",
       "      <td>-1.374316</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>0.027076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_swapbasis_1Y  대비_swapbasis_2Y  대비_swapbasis_3Y  \\\n",
       "DateTime                                                        \n",
       "2012-08-02         0.348741         1.544080         1.437744   \n",
       "2012-08-03         0.348741         0.286831         0.157058   \n",
       "2012-08-06        -0.350946        -0.873707        -0.803456   \n",
       "2012-08-07         0.173819         0.286831        -0.003027   \n",
       "2012-08-08        -0.001103        -0.390150        -0.323199   \n",
       "...                     ...              ...              ...   \n",
       "2022-07-25        -0.700790        -0.196727        -0.003027   \n",
       "2022-07-26         0.348741        -0.196727        -0.003027   \n",
       "2022-07-27         0.348741         0.770388         0.797401   \n",
       "2022-07-28         0.173819        -0.003304        -0.163113   \n",
       "2022-07-29        -0.700790        -0.583573        -0.163113   \n",
       "\n",
       "            대비_swapbasis_5Y  대비_swapbasis_10Y  대비_국고_1Y  대비_국고_3Y  대비_국고_5Y  \\\n",
       "DateTime                                                                      \n",
       "2012-08-02         1.627899          1.488363 -1.698057 -0.646622 -1.079749   \n",
       "2012-08-03        -0.910410         -2.158344 -1.132651 -0.323869 -1.890219   \n",
       "2012-08-06        -1.091718         -0.832269  0.563566  0.160261  0.000879   \n",
       "2012-08-07        -1.454333         -1.661066 -0.567245 -0.001116  0.000879   \n",
       "2012-08-08        -0.729102         -1.163788 -0.567245 -0.323869 -0.539435   \n",
       "...                     ...               ...       ...       ...       ...   \n",
       "2022-07-25        -0.366487         -0.003472 -1.132651 -0.969375 -1.890219   \n",
       "2022-07-26         0.177437          0.162288  0.563566 -0.485246 -0.539435   \n",
       "2022-07-27         0.902668          0.825326 -0.001839 -0.485246 -1.349905   \n",
       "2022-07-28        -0.366487         -0.832269  0.563566  0.644391  0.811350   \n",
       "2022-07-29         0.902668          0.493807 -2.263463 -2.099012 -3.241004   \n",
       "\n",
       "            대비_국고_10Y  대비_통안_1Y  대비_통안_2Y  스왑포인트_1M   전일종가_ex  종가_NDF_차이  \n",
       "DateTime                                                                  \n",
       "2012-08-02  -1.027569 -0.325920 -0.625160  1.909409 -0.149841  -0.495597  \n",
       "2012-08-03  -1.798403 -0.217574 -0.125723  1.818881 -0.056232  -0.415698  \n",
       "2012-08-06   0.000209  0.107465  0.123996  1.818881 -0.000426   0.423243  \n",
       "2012-08-07   0.000209 -0.109228 -0.125723  1.909409 -0.104837   0.003773  \n",
       "2012-08-08  -0.513680 -0.109228 -0.125723  1.818881 -0.108437  -0.092772  \n",
       "...               ...       ...       ...       ...       ...        ...  \n",
       "2022-07-25  -2.312292 -0.109228 -0.625160 -0.896960  3.207485   0.213508  \n",
       "2022-07-26  -0.770625  0.107465 -0.000863 -0.987488  3.220086   0.183546  \n",
       "2022-07-27  -1.541458  0.215812 -0.125723 -0.851696  3.110275  -0.189317  \n",
       "2022-07-28   2.055766  0.215812  0.373715 -0.942224  3.212885   0.489826  \n",
       "2022-07-29  -2.055348 -0.325920 -1.374316 -0.896960  2.903255   0.027076  \n",
       "\n",
       "[2459 rows x 14 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 쓸 칼럼만 남기고 feature, target 분리해 각각 x,y 에 저장\n",
    "x = df_1[[ '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y', '대비_swapbasis_10Y',\n",
    "           '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y', \n",
    "           '대비_통안_1Y', '대비_통안_2Y', '스왑포인트_1M', '전일종가_ex', \n",
    "           '종가_NDF_차이']]\n",
    "y = df_1[['종가_ex']]\n",
    "\n",
    "# 이건 이렇게 해야 밑에 코드 8번째 줄 columns에 들어갈 수 있다고 하네요!\n",
    "x.feature = x.columns \n",
    "x.feature\n",
    "\n",
    "# scaling 진행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor           Feature\n",
      "0     3.152522   대비_swapbasis_1Y\n",
      "1     5.907361   대비_swapbasis_2Y\n",
      "2     2.407209   대비_swapbasis_3Y\n",
      "3     4.767295   대비_swapbasis_5Y\n",
      "4     3.269497  대비_swapbasis_10Y\n",
      "5     1.830961          대비_국고_1Y\n",
      "6     1.244140          대비_국고_3Y\n",
      "7     5.803258          대비_국고_5Y\n",
      "8     4.883202         대비_국고_10Y\n",
      "9     1.024620          대비_통안_1Y\n",
      "10    1.113948          대비_통안_2Y\n",
      "11    1.184437          스왑포인트_1M\n",
      "12    1.177857           전일종가_ex\n",
      "13    1.018710         종가_NDF_차이\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.806e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 28 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:04:24</td>     <th>  Log-Likelihood:    </th> <td> -7656.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.534e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2444</td>      <th>  BIC:               </th> <td>1.543e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    14</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8939</td> <td>    0.110</td> <td> 1.03e+04</td> <td> 0.000</td> <td> 1134.678</td> <td> 1135.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -1.2901</td> <td>    0.196</td> <td>   -6.598</td> <td> 0.000</td> <td>   -1.674</td> <td>   -0.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_2Y</th>  <td>   -0.2013</td> <td>    0.268</td> <td>   -0.752</td> <td> 0.452</td> <td>   -0.726</td> <td>    0.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.1287</td> <td>    0.171</td> <td>   -0.753</td> <td> 0.451</td> <td>   -0.464</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th>  <td>    0.1074</td> <td>    0.240</td> <td>    0.446</td> <td> 0.655</td> <td>   -0.364</td> <td>    0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.2865</td> <td>    0.199</td> <td>    1.439</td> <td> 0.150</td> <td>   -0.104</td> <td>    0.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>   -0.0666</td> <td>    0.149</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.359</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.3072</td> <td>    0.123</td> <td>   -2.501</td> <td> 0.012</td> <td>   -0.548</td> <td>   -0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>    0.5826</td> <td>    0.265</td> <td>    2.196</td> <td> 0.028</td> <td>    0.062</td> <td>    1.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.2463</td> <td>    0.243</td> <td>   -1.012</td> <td> 0.312</td> <td>   -0.724</td> <td>    0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_1Y</th>         <td>   -0.0098</td> <td>    0.111</td> <td>   -0.088</td> <td> 0.930</td> <td>   -0.228</td> <td>    0.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0857</td> <td>    0.116</td> <td>    0.737</td> <td> 0.461</td> <td>   -0.142</td> <td>    0.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -0.2611</td> <td>    0.120</td> <td>   -2.179</td> <td> 0.029</td> <td>   -0.496</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.3388</td> <td>    0.120</td> <td>  462.976</td> <td> 0.000</td> <td>   55.104</td> <td>   55.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -0.9423</td> <td>    0.111</td> <td>   -8.477</td> <td> 0.000</td> <td>   -1.160</td> <td>   -0.724</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>127.052</td> <th>  Durbin-Watson:     </th> <td>   2.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 381.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.208</td>  <th>  Prob(JB):          </th> <td>1.20e-83</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.885</td>  <th>  Cond. No.          </th> <td>    6.51</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.990\n",
       "Model:                            OLS   Adj. R-squared:                  0.990\n",
       "Method:                 Least Squares   F-statistic:                 1.806e+04\n",
       "Date:                Sun, 28 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        12:04:24   Log-Likelihood:                -7656.3\n",
       "No. Observations:                2459   AIC:                         1.534e+04\n",
       "Df Residuals:                    2444   BIC:                         1.543e+04\n",
       "Df Model:                          14                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8939      0.110   1.03e+04      0.000    1134.678    1135.110\n",
       "대비_swapbasis_1Y     -1.2901      0.196     -6.598      0.000      -1.674      -0.907\n",
       "대비_swapbasis_2Y     -0.2013      0.268     -0.752      0.452      -0.726       0.324\n",
       "대비_swapbasis_3Y     -0.1287      0.171     -0.753      0.451      -0.464       0.206\n",
       "대비_swapbasis_5Y      0.1074      0.240      0.446      0.655      -0.364       0.579\n",
       "대비_swapbasis_10Y     0.2865      0.199      1.439      0.150      -0.104       0.677\n",
       "대비_국고_1Y            -0.0666      0.149     -0.447      0.655      -0.359       0.226\n",
       "대비_국고_3Y            -0.3072      0.123     -2.501      0.012      -0.548      -0.066\n",
       "대비_국고_5Y             0.5826      0.265      2.196      0.028       0.062       1.103\n",
       "대비_국고_10Y           -0.2463      0.243     -1.012      0.312      -0.724       0.231\n",
       "대비_통안_1Y            -0.0098      0.111     -0.088      0.930      -0.228       0.209\n",
       "대비_통안_2Y             0.0857      0.116      0.737      0.461      -0.142       0.314\n",
       "스왑포인트_1M            -0.2611      0.120     -2.179      0.029      -0.496      -0.026\n",
       "전일종가_ex             55.3388      0.120    462.976      0.000      55.104      55.573\n",
       "종가_NDF_차이           -0.9423      0.111     -8.477      0.000      -1.160      -0.724\n",
       "==============================================================================\n",
       "Omnibus:                      127.052   Durbin-Watson:                   2.109\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              381.868\n",
       "Skew:                           0.208   Prob(JB):                     1.20e-83\n",
       "Kurtosis:                       4.885   Cond. No.                         6.51\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.300e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 28 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:04:24</td>     <th>  Log-Likelihood:    </th> <td> -7657.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.534e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2447</td>      <th>  BIC:               </th> <td>1.541e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8939</td> <td>    0.110</td> <td> 1.03e+04</td> <td> 0.000</td> <td> 1134.678</td> <td> 1135.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -1.4221</td> <td>    0.142</td> <td>   -9.992</td> <td> 0.000</td> <td>   -1.701</td> <td>   -1.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.2245</td> <td>    0.146</td> <td>    1.539</td> <td> 0.124</td> <td>   -0.062</td> <td>    0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>   -0.0585</td> <td>    0.149</td> <td>   -0.393</td> <td> 0.694</td> <td>   -0.350</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.3019</td> <td>    0.123</td> <td>   -2.460</td> <td> 0.014</td> <td>   -0.543</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>    0.5949</td> <td>    0.265</td> <td>    2.246</td> <td> 0.025</td> <td>    0.075</td> <td>    1.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.2544</td> <td>    0.243</td> <td>   -1.046</td> <td> 0.296</td> <td>   -0.731</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_1Y</th>         <td>   -0.0107</td> <td>    0.111</td> <td>   -0.096</td> <td> 0.923</td> <td>   -0.229</td> <td>    0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0888</td> <td>    0.116</td> <td>    0.764</td> <td> 0.445</td> <td>   -0.139</td> <td>    0.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -0.2620</td> <td>    0.120</td> <td>   -2.186</td> <td> 0.029</td> <td>   -0.497</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.3374</td> <td>    0.119</td> <td>  463.128</td> <td> 0.000</td> <td>   55.103</td> <td>   55.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -0.9439</td> <td>    0.111</td> <td>   -8.494</td> <td> 0.000</td> <td>   -1.162</td> <td>   -0.726</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>126.607</td> <th>  Durbin-Watson:     </th> <td>   2.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 374.936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.214</td>  <th>  Prob(JB):          </th> <td>3.83e-82</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.865</td>  <th>  Cond. No.          </th> <td>    5.41</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.990\n",
       "Model:                            OLS   Adj. R-squared:                  0.990\n",
       "Method:                 Least Squares   F-statistic:                 2.300e+04\n",
       "Date:                Sun, 28 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        12:04:24   Log-Likelihood:                -7657.1\n",
       "No. Observations:                2459   AIC:                         1.534e+04\n",
       "Df Residuals:                    2447   BIC:                         1.541e+04\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8939      0.110   1.03e+04      0.000    1134.678    1135.110\n",
       "대비_swapbasis_1Y     -1.4221      0.142     -9.992      0.000      -1.701      -1.143\n",
       "대비_swapbasis_10Y     0.2245      0.146      1.539      0.124      -0.062       0.511\n",
       "대비_국고_1Y            -0.0585      0.149     -0.393      0.694      -0.350       0.233\n",
       "대비_국고_3Y            -0.3019      0.123     -2.460      0.014      -0.543      -0.061\n",
       "대비_국고_5Y             0.5949      0.265      2.246      0.025       0.075       1.114\n",
       "대비_국고_10Y           -0.2544      0.243     -1.046      0.296      -0.731       0.222\n",
       "대비_통안_1Y            -0.0107      0.111     -0.096      0.923      -0.229       0.208\n",
       "대비_통안_2Y             0.0888      0.116      0.764      0.445      -0.139       0.317\n",
       "스왑포인트_1M            -0.2620      0.120     -2.186      0.029      -0.497      -0.027\n",
       "전일종가_ex             55.3374      0.119    463.128      0.000      55.103      55.572\n",
       "종가_NDF_차이           -0.9439      0.111     -8.494      0.000      -1.162      -0.726\n",
       "==============================================================================\n",
       "Omnibus:                      126.607   Durbin-Watson:                   2.107\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              374.936\n",
       "Skew:                           0.214   Prob(JB):                     3.83e-82\n",
       "Kurtosis:                       4.865   Cond. No.                         5.41\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VIF_Factor           Feature\n",
      "0    1.670973   대비_swapbasis_1Y\n",
      "1    1.756510  대비_swapbasis_10Y\n",
      "2    1.814149          대비_국고_1Y\n",
      "3    1.242209          대비_국고_3Y\n",
      "4    5.785639          대비_국고_5Y\n",
      "5    4.878547         대비_국고_10Y\n",
      "6    1.113167          대비_통안_2Y\n",
      "7    1.184366          스왑포인트_1M\n",
      "8    1.177627           전일종가_ex\n",
      "9    1.018550         종가_NDF_차이\n"
     ]
    }
   ],
   "source": [
    "x_scaled.drop(['대비_통안_1Y'], axis=1, inplace=True)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.531e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 28 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:04:26</td>     <th>  Log-Likelihood:    </th> <td> -7657.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.534e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2448</td>      <th>  BIC:               </th> <td>1.540e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8939</td> <td>    0.110</td> <td> 1.03e+04</td> <td> 0.000</td> <td> 1134.678</td> <td> 1135.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -1.4221</td> <td>    0.142</td> <td>   -9.994</td> <td> 0.000</td> <td>   -1.701</td> <td>   -1.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.2245</td> <td>    0.146</td> <td>    1.539</td> <td> 0.124</td> <td>   -0.062</td> <td>    0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>   -0.0597</td> <td>    0.148</td> <td>   -0.403</td> <td> 0.687</td> <td>   -0.350</td> <td>    0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.3020</td> <td>    0.123</td> <td>   -2.461</td> <td> 0.014</td> <td>   -0.543</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>    0.5944</td> <td>    0.265</td> <td>    2.245</td> <td> 0.025</td> <td>    0.075</td> <td>    1.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.2544</td> <td>    0.243</td> <td>   -1.046</td> <td> 0.296</td> <td>   -0.731</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0887</td> <td>    0.116</td> <td>    0.763</td> <td> 0.445</td> <td>   -0.139</td> <td>    0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -0.2620</td> <td>    0.120</td> <td>   -2.187</td> <td> 0.029</td> <td>   -0.497</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.3373</td> <td>    0.119</td> <td>  463.242</td> <td> 0.000</td> <td>   55.103</td> <td>   55.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -0.9439</td> <td>    0.111</td> <td>   -8.496</td> <td> 0.000</td> <td>   -1.162</td> <td>   -0.726</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>126.541</td> <th>  Durbin-Watson:     </th> <td>   2.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 374.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.213</td>  <th>  Prob(JB):          </th> <td>4.28e-82</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.864</td>  <th>  Cond. No.          </th> <td>    5.38</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.990\n",
       "Model:                            OLS   Adj. R-squared:                  0.990\n",
       "Method:                 Least Squares   F-statistic:                 2.531e+04\n",
       "Date:                Sun, 28 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        12:04:26   Log-Likelihood:                -7657.1\n",
       "No. Observations:                2459   AIC:                         1.534e+04\n",
       "Df Residuals:                    2448   BIC:                         1.540e+04\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8939      0.110   1.03e+04      0.000    1134.678    1135.110\n",
       "대비_swapbasis_1Y     -1.4221      0.142     -9.994      0.000      -1.701      -1.143\n",
       "대비_swapbasis_10Y     0.2245      0.146      1.539      0.124      -0.062       0.511\n",
       "대비_국고_1Y            -0.0597      0.148     -0.403      0.687      -0.350       0.231\n",
       "대비_국고_3Y            -0.3020      0.123     -2.461      0.014      -0.543      -0.061\n",
       "대비_국고_5Y             0.5944      0.265      2.245      0.025       0.075       1.114\n",
       "대비_국고_10Y           -0.2544      0.243     -1.046      0.296      -0.731       0.222\n",
       "대비_통안_2Y             0.0887      0.116      0.763      0.445      -0.139       0.316\n",
       "스왑포인트_1M            -0.2620      0.120     -2.187      0.029      -0.497      -0.027\n",
       "전일종가_ex             55.3373      0.119    463.242      0.000      55.103      55.572\n",
       "종가_NDF_차이           -0.9439      0.111     -8.496      0.000      -1.162      -0.726\n",
       "==============================================================================\n",
       "Omnibus:                      126.541   Durbin-Watson:                   2.107\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              374.714\n",
       "Skew:                           0.213   Prob(JB):                     4.28e-82\n",
       "Kurtosis:                       4.864   Cond. No.                         5.38\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.166e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 28 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:04:26</td>     <th>  Log-Likelihood:    </th> <td> -7657.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.533e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2450</td>      <th>  BIC:               </th> <td>1.539e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8939</td> <td>    0.110</td> <td> 1.03e+04</td> <td> 0.000</td> <td> 1134.678</td> <td> 1135.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -1.4224</td> <td>    0.142</td> <td>  -10.007</td> <td> 0.000</td> <td>   -1.701</td> <td>   -1.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.2256</td> <td>    0.146</td> <td>    1.547</td> <td> 0.122</td> <td>   -0.060</td> <td>    0.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.3050</td> <td>    0.121</td> <td>   -2.514</td> <td> 0.012</td> <td>   -0.543</td> <td>   -0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>    0.5841</td> <td>    0.249</td> <td>    2.346</td> <td> 0.019</td> <td>    0.096</td> <td>    1.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.2561</td> <td>    0.243</td> <td>   -1.054</td> <td> 0.292</td> <td>   -0.733</td> <td>    0.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -0.2626</td> <td>    0.120</td> <td>   -2.192</td> <td> 0.028</td> <td>   -0.497</td> <td>   -0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.3338</td> <td>    0.119</td> <td>  465.049</td> <td> 0.000</td> <td>   55.101</td> <td>   55.567</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -0.9444</td> <td>    0.111</td> <td>   -8.503</td> <td> 0.000</td> <td>   -1.162</td> <td>   -0.727</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>126.544</td> <th>  Durbin-Watson:     </th> <td>   2.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 374.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.213</td>  <th>  Prob(JB):          </th> <td>3.83e-82</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.865</td>  <th>  Cond. No.          </th> <td>    4.68</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.990\n",
       "Model:                            OLS   Adj. R-squared:                  0.990\n",
       "Method:                 Least Squares   F-statistic:                 3.166e+04\n",
       "Date:                Sun, 28 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        12:04:26   Log-Likelihood:                -7657.4\n",
       "No. Observations:                2459   AIC:                         1.533e+04\n",
       "Df Residuals:                    2450   BIC:                         1.539e+04\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8939      0.110   1.03e+04      0.000    1134.678    1135.110\n",
       "대비_swapbasis_1Y     -1.4224      0.142    -10.007      0.000      -1.701      -1.144\n",
       "대비_swapbasis_10Y     0.2256      0.146      1.547      0.122      -0.060       0.512\n",
       "대비_국고_3Y            -0.3050      0.121     -2.514      0.012      -0.543      -0.067\n",
       "대비_국고_5Y             0.5841      0.249      2.346      0.019       0.096       1.072\n",
       "대비_국고_10Y           -0.2561      0.243     -1.054      0.292      -0.733       0.221\n",
       "스왑포인트_1M            -0.2626      0.120     -2.192      0.028      -0.497      -0.028\n",
       "전일종가_ex             55.3338      0.119    465.049      0.000      55.101      55.567\n",
       "종가_NDF_차이           -0.9444      0.111     -8.503      0.000      -1.162      -0.727\n",
       "==============================================================================\n",
       "Omnibus:                      126.544   Durbin-Watson:                   2.107\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              374.939\n",
       "Skew:                           0.213   Prob(JB):                     3.83e-82\n",
       "Kurtosis:                       4.865   Cond. No.                         4.68\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_통안_2Y','대비_국고_1Y' ], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.618e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 28 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:04:27</td>     <th>  Log-Likelihood:    </th> <td> -7658.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.533e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2451</td>      <th>  BIC:               </th> <td>1.538e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8939</td> <td>    0.110</td> <td> 1.03e+04</td> <td> 0.000</td> <td> 1134.678</td> <td> 1135.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -1.4305</td> <td>    0.142</td> <td>  -10.078</td> <td> 0.000</td> <td>   -1.709</td> <td>   -1.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.2375</td> <td>    0.145</td> <td>    1.633</td> <td> 0.102</td> <td>   -0.048</td> <td>    0.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.3028</td> <td>    0.121</td> <td>   -2.496</td> <td> 0.013</td> <td>   -0.541</td> <td>   -0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>    0.3570</td> <td>    0.125</td> <td>    2.863</td> <td> 0.004</td> <td>    0.113</td> <td>    0.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -0.2616</td> <td>    0.120</td> <td>   -2.184</td> <td> 0.029</td> <td>   -0.496</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.3340</td> <td>    0.119</td> <td>  465.040</td> <td> 0.000</td> <td>   55.101</td> <td>   55.567</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -0.9452</td> <td>    0.111</td> <td>   -8.510</td> <td> 0.000</td> <td>   -1.163</td> <td>   -0.727</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>126.780</td> <th>  Durbin-Watson:     </th> <td>   2.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 377.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.212</td>  <th>  Prob(JB):          </th> <td>1.05e-82</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.872</td>  <th>  Cond. No.          </th> <td>    2.27</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.990\n",
       "Model:                            OLS   Adj. R-squared:                  0.990\n",
       "Method:                 Least Squares   F-statistic:                 3.618e+04\n",
       "Date:                Sun, 28 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        12:04:27   Log-Likelihood:                -7658.0\n",
       "No. Observations:                2459   AIC:                         1.533e+04\n",
       "Df Residuals:                    2451   BIC:                         1.538e+04\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8939      0.110   1.03e+04      0.000    1134.678    1135.110\n",
       "대비_swapbasis_1Y     -1.4305      0.142    -10.078      0.000      -1.709      -1.152\n",
       "대비_swapbasis_10Y     0.2375      0.145      1.633      0.102      -0.048       0.523\n",
       "대비_국고_3Y            -0.3028      0.121     -2.496      0.013      -0.541      -0.065\n",
       "대비_국고_5Y             0.3570      0.125      2.863      0.004       0.113       0.602\n",
       "스왑포인트_1M            -0.2616      0.120     -2.184      0.029      -0.496      -0.027\n",
       "전일종가_ex             55.3340      0.119    465.040      0.000      55.101      55.567\n",
       "종가_NDF_차이           -0.9452      0.111     -8.510      0.000      -1.163      -0.727\n",
       "==============================================================================\n",
       "Omnibus:                      126.780   Durbin-Watson:                   2.106\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              377.529\n",
       "Skew:                           0.212   Prob(JB):                     1.05e-82\n",
       "Kurtosis:                       4.872   Cond. No.                         2.27\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_국고_10Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>4.218e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 28 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:04:28</td>     <th>  Log-Likelihood:    </th> <td> -7659.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.533e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2452</td>      <th>  BIC:               </th> <td>1.537e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td> 1134.8939</td> <td>    0.110</td> <td> 1.03e+04</td> <td> 0.000</td> <td> 1134.678</td> <td> 1135.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th> <td>   -1.2854</td> <td>    0.111</td> <td>  -11.605</td> <td> 0.000</td> <td>   -1.503</td> <td>   -1.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>        <td>   -0.3004</td> <td>    0.121</td> <td>   -2.476</td> <td> 0.013</td> <td>   -0.538</td> <td>   -0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>        <td>    0.3136</td> <td>    0.122</td> <td>    2.573</td> <td> 0.010</td> <td>    0.075</td> <td>    0.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>        <td>   -0.2641</td> <td>    0.120</td> <td>   -2.205</td> <td> 0.028</td> <td>   -0.499</td> <td>   -0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>         <td>   55.3313</td> <td>    0.119</td> <td>  464.904</td> <td> 0.000</td> <td>   55.098</td> <td>   55.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>       <td>   -0.9465</td> <td>    0.111</td> <td>   -8.520</td> <td> 0.000</td> <td>   -1.164</td> <td>   -0.729</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>131.253</td> <th>  Durbin-Watson:     </th> <td>   2.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 403.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.212</td>  <th>  Prob(JB):          </th> <td>2.37e-88</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.939</td>  <th>  Cond. No.          </th> <td>    1.59</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.990\n",
       "Model:                            OLS   Adj. R-squared:                  0.990\n",
       "Method:                 Least Squares   F-statistic:                 4.218e+04\n",
       "Date:                Sun, 28 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        12:04:28   Log-Likelihood:                -7659.3\n",
       "No. Observations:                2459   AIC:                         1.533e+04\n",
       "Df Residuals:                    2452   BIC:                         1.537e+04\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const            1134.8939      0.110   1.03e+04      0.000    1134.678    1135.110\n",
       "대비_swapbasis_1Y    -1.2854      0.111    -11.605      0.000      -1.503      -1.068\n",
       "대비_국고_3Y           -0.3004      0.121     -2.476      0.013      -0.538      -0.062\n",
       "대비_국고_5Y            0.3136      0.122      2.573      0.010       0.075       0.553\n",
       "스왑포인트_1M           -0.2641      0.120     -2.205      0.028      -0.499      -0.029\n",
       "전일종가_ex            55.3313      0.119    464.904      0.000      55.098      55.565\n",
       "종가_NDF_차이          -0.9465      0.111     -8.520      0.000      -1.164      -0.729\n",
       "==============================================================================\n",
       "Omnibus:                      131.253   Durbin-Watson:                   2.112\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              403.533\n",
       "Skew:                           0.212   Prob(JB):                     2.37e-88\n",
       "Kurtosis:                       4.939   Cond. No.                         1.59\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_10Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_swapbasis_1Y</th>\n",
       "      <th>대비_국고_3Y</th>\n",
       "      <th>대비_국고_5Y</th>\n",
       "      <th>스왑포인트_1M</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_NDF_차이</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.646622</td>\n",
       "      <td>-1.079749</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>-0.495597</td>\n",
       "      <td>1131.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.323869</td>\n",
       "      <td>-1.890219</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>-0.415698</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350946</td>\n",
       "      <td>0.160261</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.423243</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.323869</td>\n",
       "      <td>-0.539435</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>-0.092772</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-0.969375</td>\n",
       "      <td>-1.890219</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>0.213508</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.485246</td>\n",
       "      <td>-0.539435</td>\n",
       "      <td>-0.987488</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>0.183546</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.485246</td>\n",
       "      <td>-1.349905</td>\n",
       "      <td>-0.851696</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>-0.189317</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>0.644391</td>\n",
       "      <td>0.811350</td>\n",
       "      <td>-0.942224</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>0.489826</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-2.099012</td>\n",
       "      <td>-3.241004</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_swapbasis_1Y  대비_국고_3Y  대비_국고_5Y  스왑포인트_1M   전일종가_ex  \\\n",
       "DateTime                                                              \n",
       "2012-08-02         0.348741 -0.646622 -1.079749  1.909409 -0.149841   \n",
       "2012-08-03         0.348741 -0.323869 -1.890219  1.818881 -0.056232   \n",
       "2012-08-06        -0.350946  0.160261  0.000879  1.818881 -0.000426   \n",
       "2012-08-07         0.173819 -0.001116  0.000879  1.909409 -0.104837   \n",
       "2012-08-08        -0.001103 -0.323869 -0.539435  1.818881 -0.108437   \n",
       "...                     ...       ...       ...       ...       ...   \n",
       "2022-07-25        -0.700790 -0.969375 -1.890219 -0.896960  3.207485   \n",
       "2022-07-26         0.348741 -0.485246 -0.539435 -0.987488  3.220086   \n",
       "2022-07-27         0.348741 -0.485246 -1.349905 -0.851696  3.110275   \n",
       "2022-07-28         0.173819  0.644391  0.811350 -0.942224  3.212885   \n",
       "2022-07-29        -0.700790 -2.099012 -3.241004 -0.896960  2.903255   \n",
       "\n",
       "            종가_NDF_차이   종가_ex  \n",
       "DateTime                       \n",
       "2012-08-02  -0.495597  1131.7  \n",
       "2012-08-03  -0.415698  1134.8  \n",
       "2012-08-06   0.423243  1129.0  \n",
       "2012-08-07   0.003773  1128.8  \n",
       "2012-08-08  -0.092772  1128.3  \n",
       "...               ...     ...  \n",
       "2022-07-25   0.213508  1313.7  \n",
       "2022-07-26   0.183546  1307.6  \n",
       "2022-07-27  -0.189317  1313.3  \n",
       "2022-07-28   0.489826  1296.1  \n",
       "2022-07-29   0.027076  1299.1  \n",
       "\n",
       "[2459 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = pd.concat([x_scaled,y], axis=1)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_scaled[0:1945]\n",
    "test = df_scaled[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=1):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['대비_swapbasis_1Y', '대비_국고_3Y', '대비_국고_5Y', '스왑포인트_1M', '전일종가_ex', '종가_NDF_차이']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1555, 1, 6), (389, 1, 6))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.10259799e-03,  1.45127400e+00,  3.78307527e+00,\n",
       "         -1.39486408e+00,  1.01127176e+00,  8.19410217e-01]],\n",
       "\n",
       "       [[ 4.98416973e+00, -1.62492286e-01,  2.71035799e-01,\n",
       "         -1.07801602e+00,  1.70073867e+00,  4.06597669e-01]],\n",
       "\n",
       "       [[ 5.23662910e-01, -1.11565786e-03,  8.78916251e-04,\n",
       "          1.18518440e+00, -1.87620849e+00, -1.19405416e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 8.73506582e-01, -1.62492286e-01, -5.39434849e-01,\n",
       "         -1.34960007e+00,  4.31615498e-01, -1.19405416e-01]],\n",
       "\n",
       "       [[-7.88250860e-01,  4.83014227e-01,  1.35166333e+00,\n",
       "          1.77361651e+00, -1.33435590e+00, -4.82280963e-01]],\n",
       "\n",
       "       [[ 3.48741074e-01,  1.60260970e-01,  1.08150645e+00,\n",
       "         -1.39486408e+00, -1.15433843e+00,  3.03394532e-01]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((513, 1, 6), (513, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 1)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1266726.1250 - mae: 1124.3702\n",
      "Epoch 1: val_loss improved from inf to 1267311.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 12s 82ms/step - loss: 1266409.6250 - mae: 1124.2192 - val_loss: 1267311.0000 - val_mae: 1124.5833\n",
      "Epoch 2/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1264073.1250 - mae: 1123.1825\n",
      "Epoch 2: val_loss improved from 1267311.00000 to 1265964.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1265444.5000 - mae: 1123.7876 - val_loss: 1265964.2500 - val_mae: 1123.9807\n",
      "Epoch 3/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1262301.8750 - mae: 1122.3762\n",
      "Epoch 3: val_loss improved from 1265964.25000 to 1263379.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 1263567.6250 - mae: 1122.9446 - val_loss: 1263379.6250 - val_mae: 1122.8197\n",
      "Epoch 4/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1260843.0000 - mae: 1121.7185\n",
      "Epoch 4: val_loss improved from 1263379.62500 to 1259165.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 1260280.1250 - mae: 1121.4617 - val_loss: 1259165.5000 - val_mae: 1120.9219\n",
      "Epoch 5/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1255092.1250 - mae: 1119.1191\n",
      "Epoch 5: val_loss improved from 1259165.50000 to 1253060.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1255227.1250 - mae: 1119.1781 - val_loss: 1253060.0000 - val_mae: 1118.1643\n",
      "Epoch 6/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1248716.8750 - mae: 1116.2639\n",
      "Epoch 6: val_loss improved from 1253060.00000 to 1245269.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 1248416.7500 - mae: 1116.0903 - val_loss: 1245269.1250 - val_mae: 1114.6335\n",
      "Epoch 7/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1239973.8750 - mae: 1112.2488\n",
      "Epoch 7: val_loss improved from 1245269.12500 to 1236083.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 1240055.1250 - mae: 1112.2876 - val_loss: 1236083.1250 - val_mae: 1110.4572\n",
      "Epoch 8/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1232120.0000 - mae: 1108.6516\n",
      "Epoch 8: val_loss improved from 1236083.12500 to 1225642.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 1230361.2500 - mae: 1107.8551 - val_loss: 1225642.7500 - val_mae: 1105.6862\n",
      "Epoch 9/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1219780.3750 - mae: 1102.9817\n",
      "Epoch 9: val_loss improved from 1225642.75000 to 1214020.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 1219478.5000 - mae: 1102.8601 - val_loss: 1214020.6250 - val_mae: 1100.3446\n",
      "Epoch 10/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1206148.0000 - mae: 1096.7021\n",
      "Epoch 10: val_loss improved from 1214020.62500 to 1201327.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 1207484.8750 - mae: 1097.3169 - val_loss: 1201327.5000 - val_mae: 1094.4722\n",
      "Epoch 11/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1193232.8750 - mae: 1090.7423\n",
      "Epoch 11: val_loss improved from 1201327.50000 to 1187746.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 59ms/step - loss: 1194513.1250 - mae: 1091.2847 - val_loss: 1187746.5000 - val_mae: 1088.1522\n",
      "Epoch 12/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1180877.2500 - mae: 1084.8845\n",
      "Epoch 12: val_loss improved from 1187746.50000 to 1173274.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 1180665.8750 - mae: 1084.7957 - val_loss: 1173274.1250 - val_mae: 1081.3674\n",
      "Epoch 13/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1166895.2500 - mae: 1078.3153\n",
      "Epoch 13: val_loss improved from 1173274.12500 to 1158025.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 1165989.2500 - mae: 1077.8676 - val_loss: 1158025.6250 - val_mae: 1074.1615\n",
      "Epoch 14/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1151276.1250 - mae: 1070.8842\n",
      "Epoch 14: val_loss improved from 1158025.62500 to 1142096.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 1150566.2500 - mae: 1070.5326 - val_loss: 1142096.8750 - val_mae: 1066.5773\n",
      "Epoch 15/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1134463.5000 - mae: 1062.7963\n",
      "Epoch 15: val_loss improved from 1142096.87500 to 1125416.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 1134460.0000 - mae: 1062.8014 - val_loss: 1125416.7500 - val_mae: 1058.5614\n",
      "Epoch 16/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 1117188.6250 - mae: 1054.4706\n",
      "Epoch 16: val_loss improved from 1125416.75000 to 1108198.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1117770.0000 - mae: 1054.7108 - val_loss: 1108198.5000 - val_mae: 1050.2053\n",
      "Epoch 17/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1103577.3750 - mae: 1047.8307\n",
      "Epoch 17: val_loss improved from 1108198.50000 to 1090507.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 1100553.2500 - mae: 1046.2954 - val_loss: 1090507.6250 - val_mae: 1041.5492\n",
      "Epoch 18/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1081512.7500 - mae: 1036.8879\n",
      "Epoch 18: val_loss improved from 1090507.62500 to 1072215.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1082875.0000 - mae: 1037.5708 - val_loss: 1072215.0000 - val_mae: 1032.4957\n",
      "Epoch 19/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1064724.7500 - mae: 1028.5128\n",
      "Epoch 19: val_loss improved from 1072215.00000 to 1053628.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 1064724.7500 - mae: 1028.5128 - val_loss: 1053628.1250 - val_mae: 1023.2139\n",
      "Epoch 20/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1046761.6250 - mae: 1019.4680\n",
      "Epoch 20: val_loss improved from 1053628.12500 to 1034631.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 63ms/step - loss: 1046153.6250 - mae: 1019.1327 - val_loss: 1034631.6250 - val_mae: 1013.6210\n",
      "Epoch 21/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1029168.0000 - mae: 1010.4587\n",
      "Epoch 21: val_loss improved from 1034631.62500 to 1015172.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 60ms/step - loss: 1027205.8750 - mae: 1009.4774 - val_loss: 1015172.0000 - val_mae: 1003.6797\n",
      "Epoch 22/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1010813.8750 - mae: 1001.1475\n",
      "Epoch 22: val_loss improved from 1015172.00000 to 995476.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 1007922.6875 - mae: 999.5162 - val_loss: 995476.5000 - val_mae: 993.5043\n",
      "Epoch 23/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 989643.0625 - mae: 989.9068\n",
      "Epoch 23: val_loss improved from 995476.50000 to 975458.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 988405.8750 - mae: 989.3382 - val_loss: 975458.8750 - val_mae: 983.0250\n",
      "Epoch 24/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 969261.6250 - mae: 979.0898\n",
      "Epoch 24: val_loss improved from 975458.87500 to 955280.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 968646.6250 - mae: 978.8505 - val_loss: 955280.9375 - val_mae: 972.3328\n",
      "Epoch 25/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 949844.8125 - mae: 968.5893\n",
      "Epoch 25: val_loss improved from 955280.93750 to 934964.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 948736.3125 - mae: 968.1407 - val_loss: 934964.8750 - val_mae: 961.4241\n",
      "Epoch 26/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 926808.8750 - mae: 956.2272\n",
      "Epoch 26: val_loss improved from 934964.87500 to 914468.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 928705.9375 - mae: 957.2918 - val_loss: 914468.9375 - val_mae: 950.2750\n",
      "Epoch 27/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 908288.1875 - mae: 946.0585\n",
      "Epoch 27: val_loss improved from 914468.93750 to 893841.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 908518.6250 - mae: 946.2484 - val_loss: 893841.6875 - val_mae: 938.8912\n",
      "Epoch 28/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 887193.0625 - mae: 934.4028\n",
      "Epoch 28: val_loss improved from 893841.68750 to 873226.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 63ms/step - loss: 888287.8750 - mae: 935.0046 - val_loss: 873226.5000 - val_mae: 927.3596\n",
      "Epoch 29/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 869111.5625 - mae: 924.2407\n",
      "Epoch 29: val_loss improved from 873226.50000 to 852579.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 868000.9375 - mae: 923.5870 - val_loss: 852579.3125 - val_mae: 915.6516\n",
      "Epoch 30/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 849709.7500 - mae: 913.0781\n",
      "Epoch 30: val_loss improved from 852579.31250 to 832129.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 847872.4375 - mae: 912.1172 - val_loss: 832129.3750 - val_mae: 903.8878\n",
      "Epoch 31/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 828012.5625 - mae: 900.6121\n",
      "Epoch 31: val_loss improved from 832129.37500 to 811538.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 827626.0000 - mae: 900.4012 - val_loss: 811538.4375 - val_mae: 891.8699\n",
      "Epoch 32/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 807527.6875 - mae: 888.6302\n",
      "Epoch 32: val_loss improved from 811538.43750 to 790853.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 93ms/step - loss: 807364.3750 - mae: 888.5530 - val_loss: 790853.4375 - val_mae: 879.5912\n",
      "Epoch 33/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 787385.2500 - mae: 876.6320\n",
      "Epoch 33: val_loss improved from 790853.43750 to 770329.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 787186.8125 - mae: 876.5749 - val_loss: 770329.2500 - val_mae: 867.2145\n",
      "Epoch 34/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 765402.0625 - mae: 863.4291\n",
      "Epoch 34: val_loss improved from 770329.25000 to 749883.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 767066.4375 - mae: 864.3950 - val_loss: 749883.0625 - val_mae: 854.6803\n",
      "Epoch 35/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 748536.6875 - mae: 852.9564\n",
      "Epoch 35: val_loss improved from 749883.06250 to 729591.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 747077.3125 - mae: 852.0845 - val_loss: 729591.6250 - val_mae: 842.0475\n",
      "Epoch 36/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 727188.4375 - mae: 839.4662\n",
      "Epoch 36: val_loss improved from 729591.62500 to 709402.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 727217.9375 - mae: 839.6753 - val_loss: 709402.0000 - val_mae: 829.2602\n",
      "Epoch 37/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 705547.6250 - mae: 825.7604\n",
      "Epoch 37: val_loss improved from 709402.00000 to 689335.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 707520.6250 - mae: 827.1334 - val_loss: 689335.0625 - val_mae: 816.3181\n",
      "Epoch 38/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 687990.4375 - mae: 814.4924\n",
      "Epoch 38: val_loss improved from 689335.06250 to 669580.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 687990.4375 - mae: 814.4924 - val_loss: 669580.3125 - val_mae: 803.3722\n",
      "Epoch 39/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 670485.8750 - mae: 803.2076\n",
      "Epoch 39: val_loss improved from 669580.31250 to 649997.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 63ms/step - loss: 668621.4375 - mae: 801.7061 - val_loss: 649997.5625 - val_mae: 790.3278\n",
      "Epoch 40/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 649291.6250 - mae: 788.7621\n",
      "Epoch 40: val_loss improved from 649997.56250 to 630604.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 95ms/step - loss: 649471.1250 - mae: 788.8958 - val_loss: 630604.1875 - val_mae: 777.1528\n",
      "Epoch 41/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 632170.0000 - mae: 776.9487\n",
      "Epoch 41: val_loss improved from 630604.18750 to 611415.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 630538.6875 - mae: 775.9575 - val_loss: 611415.7500 - val_mae: 763.9009\n",
      "Epoch 42/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 615142.0625 - mae: 765.2322\n",
      "Epoch 42: val_loss improved from 611415.75000 to 592508.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 611802.8750 - mae: 762.9697 - val_loss: 592508.9375 - val_mae: 750.5965\n",
      "Epoch 43/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 592283.1875 - mae: 748.9581\n",
      "Epoch 43: val_loss improved from 592508.93750 to 573813.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 593318.1875 - mae: 749.8430 - val_loss: 573813.7500 - val_mae: 737.1146\n",
      "Epoch 44/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 576153.7500 - mae: 737.2285\n",
      "Epoch 44: val_loss improved from 573813.75000 to 555407.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 575066.1875 - mae: 736.6681 - val_loss: 555407.0625 - val_mae: 723.6496\n",
      "Epoch 45/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 557540.1875 - mae: 723.9954\n",
      "Epoch 45: val_loss improved from 555407.06250 to 537225.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 557064.5000 - mae: 723.4206 - val_loss: 537225.1875 - val_mae: 710.0480\n",
      "Epoch 46/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 540415.0625 - mae: 710.7407\n",
      "Epoch 46: val_loss improved from 537225.18750 to 519376.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 539336.6250 - mae: 710.0690 - val_loss: 519376.0938 - val_mae: 696.4417\n",
      "Epoch 47/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 524409.5000 - mae: 698.4798\n",
      "Epoch 47: val_loss improved from 519376.09375 to 501793.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 521898.3438 - mae: 696.7399 - val_loss: 501793.5625 - val_mae: 682.7528\n",
      "Epoch 48/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 511531.4375 - mae: 688.6812\n",
      "Epoch 48: val_loss improved from 501793.56250 to 484522.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 504733.0312 - mae: 683.3665 - val_loss: 484522.1250 - val_mae: 669.0900\n",
      "Epoch 49/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 488093.8125 - mae: 670.5607\n",
      "Epoch 49: val_loss improved from 484522.12500 to 467581.78125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 487881.7500 - mae: 670.0270 - val_loss: 467581.7812 - val_mae: 655.2988\n",
      "Epoch 50/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 472142.7812 - mae: 657.0752\n",
      "Epoch 50: val_loss improved from 467581.78125 to 450969.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 471312.1875 - mae: 656.6250 - val_loss: 450969.0000 - val_mae: 641.5739\n",
      "Epoch 51/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 455049.5000 - mae: 643.2552\n",
      "Epoch 51: val_loss improved from 450969.00000 to 434665.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 455049.5000 - mae: 643.2552 - val_loss: 434665.0625 - val_mae: 627.7990\n",
      "Epoch 52/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 437765.6562 - mae: 628.6590\n",
      "Epoch 52: val_loss improved from 434665.06250 to 418727.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 439099.7500 - mae: 629.9095 - val_loss: 418727.2500 - val_mae: 614.0169\n",
      "Epoch 53/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 423458.1562 - mae: 616.5691\n",
      "Epoch 53: val_loss improved from 418727.25000 to 403002.03125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 101ms/step - loss: 423458.1562 - mae: 616.5691 - val_loss: 403002.0312 - val_mae: 600.3271\n",
      "Epoch 54/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 409238.3125 - mae: 604.0334\n",
      "Epoch 54: val_loss improved from 403002.03125 to 387721.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 408153.0312 - mae: 603.3824 - val_loss: 387721.2188 - val_mae: 586.7980\n",
      "Epoch 55/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 394567.3125 - mae: 591.7318\n",
      "Epoch 55: val_loss improved from 387721.21875 to 372822.40625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 393139.1875 - mae: 590.3188 - val_loss: 372822.4062 - val_mae: 573.2725\n",
      "Epoch 56/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 377771.6250 - mae: 576.7469\n",
      "Epoch 56: val_loss improved from 372822.40625 to 358224.40625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 378483.1250 - mae: 577.2201 - val_loss: 358224.4062 - val_mae: 559.9250\n",
      "Epoch 57/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 364430.5000 - mae: 564.5861\n",
      "Epoch 57: val_loss improved from 358224.40625 to 343955.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 364116.6250 - mae: 564.2385 - val_loss: 343955.9375 - val_mae: 546.6369\n",
      "Epoch 58/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 350116.7500 - mae: 551.2590\n",
      "Epoch 58: val_loss improved from 343955.93750 to 330041.15625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 350059.0000 - mae: 551.2734 - val_loss: 330041.1562 - val_mae: 533.4756\n",
      "Epoch 59/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 336717.3125 - mae: 538.6516\n",
      "Epoch 59: val_loss improved from 330041.15625 to 316427.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 336370.0000 - mae: 538.3928 - val_loss: 316427.5312 - val_mae: 520.3153\n",
      "Epoch 60/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 323737.0938 - mae: 526.7263\n",
      "Epoch 60: val_loss improved from 316427.53125 to 303248.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 323020.8125 - mae: 525.8030 - val_loss: 303248.0938 - val_mae: 507.4706\n",
      "Epoch 61/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 312094.5625 - mae: 515.0647\n",
      "Epoch 61: val_loss improved from 303248.09375 to 290362.84375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 309948.3750 - mae: 513.2335 - val_loss: 290362.8438 - val_mae: 494.8568\n",
      "Epoch 62/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 296082.3750 - mae: 499.5745\n",
      "Epoch 62: val_loss improved from 290362.84375 to 277873.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 297237.3125 - mae: 500.9057 - val_loss: 277873.3438 - val_mae: 482.6289\n",
      "Epoch 63/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 284678.5312 - mae: 488.5687\n",
      "Epoch 63: val_loss improved from 277873.34375 to 265720.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 284858.9375 - mae: 488.6391 - val_loss: 265720.2188 - val_mae: 470.4507\n",
      "Epoch 64/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 272149.7812 - mae: 475.8837\n",
      "Epoch 64: val_loss improved from 265720.21875 to 253828.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 272845.6875 - mae: 476.6234 - val_loss: 253828.3125 - val_mae: 458.3867\n",
      "Epoch 65/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 260196.4219 - mae: 463.1707\n",
      "Epoch 65: val_loss improved from 253828.31250 to 242374.42188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 16s 162ms/step - loss: 261085.6875 - mae: 464.7128 - val_loss: 242374.4219 - val_mae: 446.5362\n",
      "Epoch 66/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 249194.1562 - mae: 452.5363\n",
      "Epoch 66: val_loss improved from 242374.42188 to 231285.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 14s 145ms/step - loss: 249682.2344 - mae: 453.1870 - val_loss: 231285.1875 - val_mae: 434.8743\n",
      "Epoch 67/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 237219.2188 - mae: 438.7425\n",
      "Epoch 67: val_loss improved from 231285.18750 to 220519.17188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 238605.8750 - mae: 441.6847 - val_loss: 220519.1719 - val_mae: 423.4146\n",
      "Epoch 68/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 227140.0469 - mae: 430.3335\n",
      "Epoch 68: val_loss improved from 220519.17188 to 210032.32812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 227911.4844 - mae: 430.3336 - val_loss: 210032.3281 - val_mae: 411.9952\n",
      "Epoch 69/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 219258.4062 - mae: 421.7217\n",
      "Epoch 69: val_loss improved from 210032.32812 to 199964.95312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 217516.7031 - mae: 419.3805 - val_loss: 199964.9531 - val_mae: 400.7900\n",
      "Epoch 70/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 207443.2344 - mae: 408.5703\n",
      "Epoch 70: val_loss improved from 199964.95312 to 190268.23438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 207443.2344 - mae: 408.5703 - val_loss: 190268.2344 - val_mae: 389.7791\n",
      "Epoch 71/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 194564.9688 - mae: 395.7133\n",
      "Epoch 71: val_loss improved from 190268.23438 to 180786.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 94ms/step - loss: 197684.5781 - mae: 397.8005 - val_loss: 180786.2188 - val_mae: 378.7681\n",
      "Epoch 72/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 188077.9219 - mae: 386.9037\n",
      "Epoch 72: val_loss improved from 180786.21875 to 171806.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 26s 268ms/step - loss: 188228.8906 - mae: 387.1989 - val_loss: 171806.6875 - val_mae: 368.3543\n",
      "Epoch 73/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 178756.6562 - mae: 376.4046\n",
      "Epoch 73: val_loss improved from 171806.68750 to 162987.57812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 34s 345ms/step - loss: 179042.1406 - mae: 376.7394 - val_loss: 162987.5781 - val_mae: 357.9712\n",
      "Epoch 74/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 167880.3281 - mae: 363.9773\n",
      "Epoch 74: val_loss improved from 162987.57812 to 154611.59375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 17s 172ms/step - loss: 170074.6250 - mae: 366.3607 - val_loss: 154611.5938 - val_mae: 347.9103\n",
      "Epoch 75/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 162647.5156 - mae: 357.4887\n",
      "Epoch 75: val_loss improved from 154611.59375 to 146425.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 161377.7656 - mae: 356.0074 - val_loss: 146425.9375 - val_mae: 337.9156\n",
      "Epoch 76/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 152732.0781 - mae: 345.6403\n",
      "Epoch 76: val_loss improved from 146425.93750 to 138507.95312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 152854.6406 - mae: 345.7256 - val_loss: 138507.9531 - val_mae: 327.9749\n",
      "Epoch 77/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 145525.9375 - mae: 336.3146\n",
      "Epoch 77: val_loss improved from 138507.95312 to 131039.38281, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 144605.8906 - mae: 335.4549 - val_loss: 131039.3828 - val_mae: 318.3978\n",
      "Epoch 78/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 136635.1094 - mae: 325.1336\n",
      "Epoch 78: val_loss improved from 131039.38281 to 123812.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 136733.7812 - mae: 325.4265 - val_loss: 123812.7188 - val_mae: 308.9009\n",
      "Epoch 79/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 130300.3594 - mae: 317.7996\n",
      "Epoch 79: val_loss improved from 123812.71875 to 116899.19531, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 67ms/step - loss: 129111.9922 - mae: 315.4787 - val_loss: 116899.1953 - val_mae: 299.6800\n",
      "Epoch 80/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 122021.1328 - mae: 305.7851\n",
      "Epoch 80: val_loss improved from 116899.19531 to 110318.95312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 121806.9609 - mae: 305.6648 - val_loss: 110318.9531 - val_mae: 290.7635\n",
      "Epoch 81/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 114887.8203 - mae: 296.4101\n",
      "Epoch 81: val_loss improved from 110318.95312 to 104039.32031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 114838.3359 - mae: 295.9969 - val_loss: 104039.3203 - val_mae: 282.1227\n",
      "Epoch 82/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 108677.1250 - mae: 286.9368\n",
      "Epoch 82: val_loss improved from 104039.32031 to 98007.85938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 108234.5703 - mae: 286.3591 - val_loss: 98007.8594 - val_mae: 273.5811\n",
      "Epoch 83/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 101743.8516 - mae: 276.6391\n",
      "Epoch 83: val_loss improved from 98007.85938 to 92316.94531, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 101889.2891 - mae: 276.8979 - val_loss: 92316.9453 - val_mae: 265.1462\n",
      "Epoch 84/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 96280.0781 - mae: 268.9990\n",
      "Epoch 84: val_loss improved from 92316.94531 to 86852.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 95895.4375 - mae: 267.7033 - val_loss: 86852.2188 - val_mae: 256.6944\n",
      "Epoch 85/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 90523.5781 - mae: 259.3310\n",
      "Epoch 85: val_loss improved from 86852.21875 to 81738.44531, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 90226.3750 - mae: 258.6534 - val_loss: 81738.4453 - val_mae: 248.3918\n",
      "Epoch 86/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 85334.2656 - mae: 250.6613\n",
      "Epoch 86: val_loss improved from 81738.44531 to 76844.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 84846.3516 - mae: 249.7947 - val_loss: 76844.5625 - val_mae: 240.3289\n",
      "Epoch 87/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 79537.6250 - mae: 241.4883\n",
      "Epoch 87: val_loss improved from 76844.56250 to 72288.85156, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 98ms/step - loss: 79721.3750 - mae: 241.1033 - val_loss: 72288.8516 - val_mae: 232.4288\n",
      "Epoch 88/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 74952.1016 - mae: 232.8010\n",
      "Epoch 88: val_loss improved from 72288.85156 to 68017.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 74848.7109 - mae: 232.6885 - val_loss: 68017.5312 - val_mae: 224.7427\n",
      "Epoch 89/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 70698.5469 - mae: 225.2009\n",
      "Epoch 89: val_loss improved from 68017.53125 to 63885.57812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 70288.3984 - mae: 224.3457 - val_loss: 63885.5781 - val_mae: 216.9830\n",
      "Epoch 90/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 66690.1328 - mae: 217.0194\n",
      "Epoch 90: val_loss improved from 63885.57812 to 60056.08203, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 65931.6250 - mae: 216.1875 - val_loss: 60056.0820 - val_mae: 209.4560\n",
      "Epoch 91/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 61807.9023 - mae: 208.0582\n",
      "Epoch 91: val_loss improved from 60056.08203 to 56471.32031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 61856.9453 - mae: 208.2082 - val_loss: 56471.3203 - val_mae: 202.1598\n",
      "Epoch 92/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 58040.0820 - mae: 200.5477\n",
      "Epoch 92: val_loss improved from 56471.32031 to 53139.13281, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 57985.7227 - mae: 200.4562 - val_loss: 53139.1328 - val_mae: 195.0323\n",
      "Epoch 93/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 54927.4531 - mae: 193.5998\n",
      "Epoch 93: val_loss improved from 53139.13281 to 50014.68359, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 54386.2578 - mae: 193.0559 - val_loss: 50014.6836 - val_mae: 188.1167\n",
      "Epoch 94/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 51703.7070 - mae: 186.7407\n",
      "Epoch 94: val_loss improved from 50014.68359 to 47096.51953, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 50997.0117 - mae: 185.8168 - val_loss: 47096.5195 - val_mae: 181.3064\n",
      "Epoch 95/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 47812.1055 - mae: 178.0481\n",
      "Epoch 95: val_loss improved from 47096.51953 to 44340.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 93ms/step - loss: 47783.8594 - mae: 178.5824 - val_loss: 44340.4688 - val_mae: 174.5888\n",
      "Epoch 96/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 44699.7461 - mae: 171.2323\n",
      "Epoch 96: val_loss improved from 44340.46875 to 41748.11719, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 13s 130ms/step - loss: 44702.2148 - mae: 171.8422 - val_loss: 41748.1172 - val_mae: 168.0026\n",
      "Epoch 97/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 41734.9336 - mae: 164.8673\n",
      "Epoch 97: val_loss improved from 41748.11719 to 39239.21484, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 41711.3438 - mae: 164.8384 - val_loss: 39239.2148 - val_mae: 161.5099\n",
      "Epoch 98/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 39284.6133 - mae: 158.7739\n",
      "Epoch 98: val_loss improved from 39239.21484 to 36941.70703, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 38741.4805 - mae: 157.8408 - val_loss: 36941.7070 - val_mae: 155.2187\n",
      "Epoch 99/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 36669.0781 - mae: 152.4757\n",
      "Epoch 99: val_loss improved from 36941.70703 to 34845.07422, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 35925.5117 - mae: 151.1831 - val_loss: 34845.0742 - val_mae: 149.2671\n",
      "Epoch 100/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 33167.5430 - mae: 144.6476\n",
      "Epoch 100: val_loss improved from 34845.07422 to 32946.92969, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 33137.8125 - mae: 144.7752 - val_loss: 32946.9297 - val_mae: 143.5907\n",
      "Epoch 101/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 31357.3887 - mae: 139.0509\n",
      "Epoch 101: val_loss improved from 32946.92969 to 31179.22656, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 30691.1719 - mae: 138.6611 - val_loss: 31179.2266 - val_mae: 138.0947\n",
      "Epoch 102/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 28397.1875 - mae: 133.4788\n",
      "Epoch 102: val_loss improved from 31179.22656 to 29556.45703, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 28584.8281 - mae: 132.8239 - val_loss: 29556.4570 - val_mae: 132.7659\n",
      "Epoch 103/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 25849.4102 - mae: 126.9797\n",
      "Epoch 103: val_loss improved from 29556.45703 to 27967.70898, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 26627.8848 - mae: 127.2803 - val_loss: 27967.7090 - val_mae: 127.3484\n",
      "Epoch 104/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 24688.1875 - mae: 121.5875\n",
      "Epoch 104: val_loss improved from 27967.70898 to 26456.94922, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 24781.2148 - mae: 121.4686 - val_loss: 26456.9492 - val_mae: 121.6787\n",
      "Epoch 105/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 22483.2988 - mae: 115.7120\n",
      "Epoch 105: val_loss improved from 26456.94922 to 25034.42578, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 23013.2988 - mae: 115.6540 - val_loss: 25034.4258 - val_mae: 116.2254\n",
      "Epoch 106/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 21373.2930 - mae: 110.0073\n",
      "Epoch 106: val_loss improved from 25034.42578 to 23802.30859, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 21423.9941 - mae: 110.2034 - val_loss: 23802.3086 - val_mae: 111.6777\n",
      "Epoch 107/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 20479.0332 - mae: 106.4150\n",
      "Epoch 107: val_loss improved from 23802.30859 to 22675.27734, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 19975.3066 - mae: 105.2483 - val_loss: 22675.2773 - val_mae: 107.1995\n",
      "Epoch 108/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 18692.6152 - mae: 100.7618\n",
      "Epoch 108: val_loss improved from 22675.27734 to 21675.96484, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 18692.6152 - mae: 100.7618 - val_loss: 21675.9648 - val_mae: 103.0870\n",
      "Epoch 109/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 17913.9238 - mae: 97.0653\n",
      "Epoch 109: val_loss improved from 21675.96484 to 20752.77148, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 17557.8711 - mae: 96.4569 - val_loss: 20752.7715 - val_mae: 99.1804\n",
      "Epoch 110/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 17024.1543 - mae: 93.2605\n",
      "Epoch 110: val_loss improved from 20752.77148 to 19850.94141, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 16518.4277 - mae: 92.3254 - val_loss: 19850.9414 - val_mae: 95.1886\n",
      "Epoch 111/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 16284.0244 - mae: 89.2041\n",
      "Epoch 111: val_loss improved from 19850.94141 to 18985.62891, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 15533.4453 - mae: 88.1573 - val_loss: 18985.6289 - val_mae: 91.2227\n",
      "Epoch 112/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 14455.6318 - mae: 83.9650\n",
      "Epoch 112: val_loss improved from 18985.62891 to 18174.41016, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 14606.5264 - mae: 83.9166 - val_loss: 18174.4102 - val_mae: 87.2561\n",
      "Epoch 113/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 12944.4316 - mae: 78.9464\n",
      "Epoch 113: val_loss improved from 18174.41016 to 17432.97656, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 13753.8857 - mae: 79.7570 - val_loss: 17432.9766 - val_mae: 83.4269\n",
      "Epoch 114/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 12835.8320 - mae: 76.2075\n",
      "Epoch 114: val_loss improved from 17432.97656 to 16758.36914, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 12980.8887 - mae: 76.0465 - val_loss: 16758.3691 - val_mae: 80.1062\n",
      "Epoch 115/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 13097.2236 - mae: 73.8176\n",
      "Epoch 115: val_loss improved from 16758.36914 to 16140.54102, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 12296.4053 - mae: 72.8228 - val_loss: 16140.5410 - val_mae: 77.0528\n",
      "Epoch 116/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 12353.7900 - mae: 70.7922\n",
      "Epoch 116: val_loss improved from 16140.54102 to 15561.80469, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 11669.1670 - mae: 69.8540 - val_loss: 15561.8047 - val_mae: 74.2259\n",
      "Epoch 117/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 11437.3574 - mae: 67.6916\n",
      "Epoch 117: val_loss improved from 15561.80469 to 15051.12109, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 11093.5889 - mae: 66.9201 - val_loss: 15051.1211 - val_mae: 71.4888\n",
      "Epoch 118/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 9916.8438 - mae: 63.5719\n",
      "Epoch 118: val_loss improved from 15051.12109 to 14565.50293, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 10574.4570 - mae: 64.1228 - val_loss: 14565.5029 - val_mae: 68.9893\n",
      "Epoch 119/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 8983.5879 - mae: 60.8543\n",
      "Epoch 119: val_loss improved from 14565.50293 to 14107.74512, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 10098.2910 - mae: 61.6344 - val_loss: 14107.7451 - val_mae: 66.6284\n",
      "Epoch 120/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 9655.5576 - mae: 59.5694\n",
      "Epoch 120: val_loss improved from 14107.74512 to 13622.48730, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 107ms/step - loss: 9641.8555 - mae: 59.5290 - val_loss: 13622.4873 - val_mae: 64.4886\n",
      "Epoch 121/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 8582.8613 - mae: 56.0603\n",
      "Epoch 121: val_loss improved from 13622.48730 to 13227.53223, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 9228.8721 - mae: 57.0727 - val_loss: 13227.5322 - val_mae: 62.2671\n",
      "Epoch 122/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 8907.9346 - mae: 55.4579\n",
      "Epoch 122: val_loss improved from 13227.53223 to 12782.95410, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 8831.1836 - mae: 55.2246 - val_loss: 12782.9541 - val_mae: 60.2133\n",
      "Epoch 123/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 8772.2900 - mae: 53.4291\n",
      "Epoch 123: val_loss improved from 12782.95410 to 12379.00879, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 8443.9609 - mae: 53.0290 - val_loss: 12379.0088 - val_mae: 58.1912\n",
      "Epoch 124/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 8375.0088 - mae: 51.4895\n",
      "Epoch 124: val_loss improved from 12379.00879 to 11983.77344, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 8074.7236 - mae: 51.0678 - val_loss: 11983.7734 - val_mae: 56.3338\n",
      "Epoch 125/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 6884.8477 - mae: 48.0148\n",
      "Epoch 125: val_loss improved from 11983.77344 to 11620.00488, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 108ms/step - loss: 7701.3008 - mae: 49.0298 - val_loss: 11620.0049 - val_mae: 54.5016\n",
      "Epoch 126/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 6496.4082 - mae: 46.1878\n",
      "Epoch 126: val_loss improved from 11620.00488 to 11243.87305, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 101ms/step - loss: 7352.5747 - mae: 47.4397 - val_loss: 11243.8730 - val_mae: 52.7216\n",
      "Epoch 127/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 7276.4844 - mae: 45.8557\n",
      "Epoch 127: val_loss improved from 11243.87305 to 10857.79688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 93ms/step - loss: 7007.6997 - mae: 45.6407 - val_loss: 10857.7969 - val_mae: 51.0747\n",
      "Epoch 128/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 6938.7612 - mae: 44.3352\n",
      "Epoch 128: val_loss improved from 10857.79688 to 10486.16699, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 6688.7915 - mae: 44.2045 - val_loss: 10486.1670 - val_mae: 49.4770\n",
      "Epoch 129/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 6495.3037 - mae: 42.8944\n",
      "Epoch 129: val_loss improved from 10486.16699 to 10156.06738, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 6376.9458 - mae: 42.5115 - val_loss: 10156.0674 - val_mae: 47.8245\n",
      "Epoch 130/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 6155.1406 - mae: 41.1105\n",
      "Epoch 130: val_loss improved from 10156.06738 to 9818.48242, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 6096.1899 - mae: 40.9367 - val_loss: 9818.4824 - val_mae: 46.2987\n",
      "Epoch 131/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 6001.3491 - mae: 39.4748\n",
      "Epoch 131: val_loss improved from 9818.48242 to 9498.45703, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 5827.8867 - mae: 39.5790 - val_loss: 9498.4570 - val_mae: 44.8444\n",
      "Epoch 132/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 5889.6401 - mae: 39.0203\n",
      "Epoch 132: val_loss improved from 9498.45703 to 9170.99023, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 5574.2866 - mae: 38.3539 - val_loss: 9170.9902 - val_mae: 43.4342\n",
      "Epoch 133/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 5341.2808 - mae: 36.9929\n",
      "Epoch 133: val_loss improved from 9170.99023 to 8871.42188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 5331.1772 - mae: 36.9390 - val_loss: 8871.4219 - val_mae: 42.0236\n",
      "Epoch 134/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 5106.0093 - mae: 35.5974\n",
      "Epoch 134: val_loss improved from 8871.42188 to 8585.50098, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 95ms/step - loss: 5100.9380 - mae: 35.6154 - val_loss: 8585.5010 - val_mae: 40.6585\n",
      "Epoch 135/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 4967.7534 - mae: 34.7818\n",
      "Epoch 135: val_loss improved from 8585.50098 to 8283.61426, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 4878.7153 - mae: 34.5453 - val_loss: 8283.6143 - val_mae: 39.4039\n",
      "Epoch 136/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 4691.6162 - mae: 33.0414\n",
      "Epoch 136: val_loss improved from 8283.61426 to 8004.09961, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 4664.6934 - mae: 33.2237 - val_loss: 8004.0996 - val_mae: 38.1030\n",
      "Epoch 137/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 4507.9141 - mae: 32.0853\n",
      "Epoch 137: val_loss improved from 8004.09961 to 7723.21484, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 4459.6313 - mae: 32.1892 - val_loss: 7723.2148 - val_mae: 36.9093\n",
      "Epoch 138/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 4266.3091 - mae: 31.1107\n",
      "Epoch 138: val_loss improved from 7723.21484 to 7455.36768, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 95ms/step - loss: 4266.3091 - mae: 31.1107 - val_loss: 7455.3677 - val_mae: 35.7327\n",
      "Epoch 139/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 4082.1167 - mae: 29.9387\n",
      "Epoch 139: val_loss improved from 7455.36768 to 7195.47754, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 4079.2891 - mae: 29.9641 - val_loss: 7195.4775 - val_mae: 34.6521\n",
      "Epoch 140/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 3948.2598 - mae: 29.2831\n",
      "Epoch 140: val_loss improved from 7195.47754 to 6943.42334, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 3904.1160 - mae: 29.1165 - val_loss: 6943.4233 - val_mae: 33.5906\n",
      "Epoch 141/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 3736.5991 - mae: 28.1622\n",
      "Epoch 141: val_loss improved from 6943.42334 to 6728.09521, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 3736.5991 - mae: 28.1622 - val_loss: 6728.0952 - val_mae: 32.4364\n",
      "Epoch 142/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 3684.9731 - mae: 27.1585\n",
      "Epoch 142: val_loss improved from 6728.09521 to 6463.98926, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 3572.6565 - mae: 27.1525 - val_loss: 6463.9893 - val_mae: 31.5592\n",
      "Epoch 143/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 3568.8203 - mae: 26.5620\n",
      "Epoch 143: val_loss improved from 6463.98926 to 6231.72217, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 102ms/step - loss: 3414.9636 - mae: 26.3676 - val_loss: 6231.7222 - val_mae: 30.5577\n",
      "Epoch 144/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 3269.1868 - mae: 25.3936\n",
      "Epoch 144: val_loss improved from 6231.72217 to 6003.78857, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 3263.8833 - mae: 25.3829 - val_loss: 6003.7886 - val_mae: 29.6232\n",
      "Epoch 145/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 3158.6326 - mae: 24.7027\n",
      "Epoch 145: val_loss improved from 6003.78857 to 5787.42676, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 3121.6011 - mae: 24.6825 - val_loss: 5787.4268 - val_mae: 28.7738\n",
      "Epoch 146/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 2999.2925 - mae: 23.8066\n",
      "Epoch 146: val_loss improved from 5787.42676 to 5578.04053, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 2983.4607 - mae: 23.8910 - val_loss: 5578.0405 - val_mae: 27.9159\n",
      "Epoch 147/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 3018.4741 - mae: 23.2281\n",
      "Epoch 147: val_loss improved from 5578.04053 to 5388.46582, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 2848.4321 - mae: 23.0918 - val_loss: 5388.4658 - val_mae: 27.0258\n",
      "Epoch 148/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 2876.0278 - mae: 22.5071\n",
      "Epoch 148: val_loss improved from 5388.46582 to 5190.61328, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 2721.3411 - mae: 22.4089 - val_loss: 5190.6133 - val_mae: 26.2767\n",
      "Epoch 149/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 2467.9358 - mae: 21.5286\n",
      "Epoch 149: val_loss improved from 5190.61328 to 5013.04834, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 2598.5129 - mae: 21.6378 - val_loss: 5013.0483 - val_mae: 25.4948\n",
      "Epoch 150/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 2651.2856 - mae: 21.5487\n",
      "Epoch 150: val_loss improved from 5013.04834 to 4818.45166, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 96ms/step - loss: 2487.1614 - mae: 21.0523 - val_loss: 4818.4517 - val_mae: 24.7119\n",
      "Epoch 151/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 2577.4670 - mae: 20.9955\n",
      "Epoch 151: val_loss improved from 4818.45166 to 4636.35938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 2374.4241 - mae: 20.5641 - val_loss: 4636.3594 - val_mae: 23.9546\n",
      "Epoch 152/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 2273.0190 - mae: 19.7520\n",
      "Epoch 152: val_loss improved from 4636.35938 to 4480.17334, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 2266.1174 - mae: 19.7937 - val_loss: 4480.1733 - val_mae: 23.2324\n",
      "Epoch 153/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 2193.6787 - mae: 19.2699\n",
      "Epoch 153: val_loss improved from 4480.17334 to 4326.90479, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 2168.7664 - mae: 19.3055 - val_loss: 4326.9048 - val_mae: 22.5290\n",
      "Epoch 154/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 2195.1719 - mae: 18.8060\n",
      "Epoch 154: val_loss improved from 4326.90479 to 4157.22803, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 2070.9912 - mae: 18.7350 - val_loss: 4157.2280 - val_mae: 21.8762\n",
      "Epoch 155/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 2029.9440 - mae: 18.4913\n",
      "Epoch 155: val_loss improved from 4157.22803 to 4007.91602, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 1977.6569 - mae: 18.3114 - val_loss: 4007.9160 - val_mae: 21.2372\n",
      "Epoch 156/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 2143.1938 - mae: 18.2597\n",
      "Epoch 156: val_loss improved from 4007.91602 to 3864.44409, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1892.7631 - mae: 17.8112 - val_loss: 3864.4441 - val_mae: 20.5950\n",
      "Epoch 157/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1531.6863 - mae: 16.9459\n",
      "Epoch 157: val_loss improved from 3864.44409 to 3758.39868, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1805.9071 - mae: 17.1568 - val_loss: 3758.3987 - val_mae: 20.0108\n",
      "Epoch 158/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1779.3756 - mae: 16.9345\n",
      "Epoch 158: val_loss improved from 3758.39868 to 3616.46143, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1729.1841 - mae: 16.8135 - val_loss: 3616.4614 - val_mae: 19.5320\n",
      "Epoch 159/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1671.4730 - mae: 16.5112\n",
      "Epoch 159: val_loss improved from 3616.46143 to 3501.01660, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 1653.5914 - mae: 16.4470 - val_loss: 3501.0166 - val_mae: 18.9613\n",
      "Epoch 160/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1506.7217 - mae: 15.7466\n",
      "Epoch 160: val_loss improved from 3501.01660 to 3400.93188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 98ms/step - loss: 1581.4847 - mae: 15.9201 - val_loss: 3400.9319 - val_mae: 18.4113\n",
      "Epoch 161/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1602.5676 - mae: 15.8817\n",
      "Epoch 161: val_loss improved from 3400.93188 to 3281.16235, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 1516.4387 - mae: 15.6983 - val_loss: 3281.1624 - val_mae: 17.9998\n",
      "Epoch 162/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1223.9890 - mae: 15.0143\n",
      "Epoch 162: val_loss improved from 3281.16235 to 3191.94824, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1448.9177 - mae: 15.2372 - val_loss: 3191.9482 - val_mae: 17.6102\n",
      "Epoch 163/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1426.8352 - mae: 15.3434\n",
      "Epoch 163: val_loss improved from 3191.94824 to 3048.42969, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 95ms/step - loss: 1381.5724 - mae: 15.2843 - val_loss: 3048.4297 - val_mae: 17.2423\n",
      "Epoch 164/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1244.7526 - mae: 14.6185\n",
      "Epoch 164: val_loss improved from 3048.42969 to 2974.94189, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1314.5470 - mae: 14.6887 - val_loss: 2974.9419 - val_mae: 16.8574\n",
      "Epoch 165/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1295.6362 - mae: 14.3319\n",
      "Epoch 165: val_loss improved from 2974.94189 to 2896.87671, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 99ms/step - loss: 1258.1300 - mae: 14.3192 - val_loss: 2896.8767 - val_mae: 16.5015\n",
      "Epoch 166/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1148.8551 - mae: 13.8585\n",
      "Epoch 166: val_loss improved from 2896.87671 to 2820.26001, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 102ms/step - loss: 1208.8170 - mae: 14.0263 - val_loss: 2820.2600 - val_mae: 16.2953\n",
      "Epoch 167/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1234.2943 - mae: 14.0296\n",
      "Epoch 167: val_loss improved from 2820.26001 to 2737.16333, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 1160.8993 - mae: 13.7851 - val_loss: 2737.1633 - val_mae: 16.0762\n",
      "Epoch 168/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1184.4457 - mae: 13.8470\n",
      "Epoch 168: val_loss improved from 2737.16333 to 2669.55737, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 1112.4036 - mae: 13.6319 - val_loss: 2669.5574 - val_mae: 15.8228\n",
      "Epoch 169/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1101.8846 - mae: 13.3160\n",
      "Epoch 169: val_loss improved from 2669.55737 to 2604.57788, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1067.8763 - mae: 13.2527 - val_loss: 2604.5779 - val_mae: 15.6053\n",
      "Epoch 170/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1148.4750 - mae: 13.4733\n",
      "Epoch 170: val_loss improved from 2604.57788 to 2536.09985, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1025.9098 - mae: 13.0275 - val_loss: 2536.0999 - val_mae: 15.4655\n",
      "Epoch 171/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1070.4960 - mae: 13.0795\n",
      "Epoch 171: val_loss improved from 2536.09985 to 2470.96558, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 985.8128 - mae: 12.8831 - val_loss: 2470.9656 - val_mae: 15.3194\n",
      "Epoch 172/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 986.4218 - mae: 12.7241 \n",
      "Epoch 172: val_loss improved from 2470.96558 to 2411.13818, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 949.6509 - mae: 12.7058 - val_loss: 2411.1382 - val_mae: 15.0646\n",
      "Epoch 173/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 917.5165 - mae: 12.6683\n",
      "Epoch 173: val_loss improved from 2411.13818 to 2349.60107, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 910.4576 - mae: 12.4712 - val_loss: 2349.6011 - val_mae: 14.9768\n",
      "Epoch 174/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 829.8969 - mae: 12.1111\n",
      "Epoch 174: val_loss improved from 2349.60107 to 2292.52808, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 873.0947 - mae: 12.2537 - val_loss: 2292.5281 - val_mae: 14.8138\n",
      "Epoch 175/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 848.3157 - mae: 12.2112\n",
      "Epoch 175: val_loss improved from 2292.52808 to 2231.02832, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 839.5865 - mae: 12.1793 - val_loss: 2231.0283 - val_mae: 14.7249\n",
      "Epoch 176/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 900.2346 - mae: 12.3478\n",
      "Epoch 176: val_loss improved from 2231.02832 to 2176.45752, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 807.7386 - mae: 11.9842 - val_loss: 2176.4575 - val_mae: 14.5835\n",
      "Epoch 177/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 807.6812 - mae: 11.8517\n",
      "Epoch 177: val_loss improved from 2176.45752 to 2122.98218, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 773.9150 - mae: 11.7611 - val_loss: 2122.9822 - val_mae: 14.4680\n",
      "Epoch 178/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 744.4072 - mae: 11.6922\n",
      "Epoch 178: val_loss improved from 2122.98218 to 2067.57568, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 745.1672 - mae: 11.7282 - val_loss: 2067.5757 - val_mae: 14.3512\n",
      "Epoch 179/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 732.6875 - mae: 11.6128\n",
      "Epoch 179: val_loss improved from 2067.57568 to 2019.63208, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 714.9186 - mae: 11.6076 - val_loss: 2019.6321 - val_mae: 14.2290\n",
      "Epoch 180/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 711.8621 - mae: 11.4334\n",
      "Epoch 180: val_loss improved from 2019.63208 to 1967.85657, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 687.6507 - mae: 11.3498 - val_loss: 1967.8566 - val_mae: 14.1350\n",
      "Epoch 181/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 705.6216 - mae: 11.3632\n",
      "Epoch 181: val_loss improved from 1967.85657 to 1921.12695, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 660.1735 - mae: 11.2529 - val_loss: 1921.1270 - val_mae: 14.0313\n",
      "Epoch 182/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 667.0284 - mae: 11.1936\n",
      "Epoch 182: val_loss improved from 1921.12695 to 1876.70410, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 633.4933 - mae: 11.0609 - val_loss: 1876.7041 - val_mae: 13.8424\n",
      "Epoch 183/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 469.7660 - mae: 10.6700\n",
      "Epoch 183: val_loss improved from 1876.70410 to 1838.71460, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 611.4800 - mae: 10.9190 - val_loss: 1838.7146 - val_mae: 13.7624\n",
      "Epoch 184/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 585.5690 - mae: 10.8904\n",
      "Epoch 184: val_loss improved from 1838.71460 to 1785.31262, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 586.9154 - mae: 10.8696 - val_loss: 1785.3126 - val_mae: 13.5815\n",
      "Epoch 185/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 590.8770 - mae: 10.7025\n",
      "Epoch 185: val_loss improved from 1785.31262 to 1740.49609, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 565.2883 - mae: 10.6772 - val_loss: 1740.4961 - val_mae: 13.4625\n",
      "Epoch 186/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 568.3380 - mae: 10.6857\n",
      "Epoch 186: val_loss improved from 1740.49609 to 1697.51111, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 543.6904 - mae: 10.5783 - val_loss: 1697.5111 - val_mae: 13.3031\n",
      "Epoch 187/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 500.0109 - mae: 10.2659\n",
      "Epoch 187: val_loss improved from 1697.51111 to 1662.85974, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 522.1885 - mae: 10.4529 - val_loss: 1662.8597 - val_mae: 13.1714\n",
      "Epoch 188/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 532.9602 - mae: 10.4699\n",
      "Epoch 188: val_loss improved from 1662.85974 to 1614.42749, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 63ms/step - loss: 501.2578 - mae: 10.2997 - val_loss: 1614.4275 - val_mae: 13.0068\n",
      "Epoch 189/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 483.8464 - mae: 10.2347\n",
      "Epoch 189: val_loss improved from 1614.42749 to 1575.17590, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 483.8464 - mae: 10.2347 - val_loss: 1575.1759 - val_mae: 12.8704\n",
      "Epoch 190/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 495.3277 - mae: 10.1605\n",
      "Epoch 190: val_loss improved from 1575.17590 to 1546.11035, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 59ms/step - loss: 466.3196 - mae: 10.1895 - val_loss: 1546.1104 - val_mae: 12.7401\n",
      "Epoch 191/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 453.1021 - mae: 9.9757\n",
      "Epoch 191: val_loss improved from 1546.11035 to 1503.96338, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 448.3794 - mae: 9.9401 - val_loss: 1503.9634 - val_mae: 12.5694\n",
      "Epoch 192/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 462.1592 - mae: 10.0176\n",
      "Epoch 192: val_loss improved from 1503.96338 to 1465.99841, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 432.7938 - mae: 9.9546 - val_loss: 1465.9984 - val_mae: 12.4658\n",
      "Epoch 193/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 319.8560 - mae: 9.2756\n",
      "Epoch 193: val_loss improved from 1465.99841 to 1443.06714, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 416.6648 - mae: 9.7945 - val_loss: 1443.0671 - val_mae: 12.3262\n",
      "Epoch 194/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 400.1605 - mae: 9.7435\n",
      "Epoch 194: val_loss improved from 1443.06714 to 1403.13245, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 400.1605 - mae: 9.7435 - val_loss: 1403.1324 - val_mae: 12.2343\n",
      "Epoch 195/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 395.0742 - mae: 9.6685\n",
      "Epoch 195: val_loss improved from 1403.13245 to 1369.59644, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 386.0676 - mae: 9.6395 - val_loss: 1369.5964 - val_mae: 12.1102\n",
      "Epoch 196/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 373.1701 - mae: 9.5312\n",
      "Epoch 196: val_loss improved from 1369.59644 to 1342.29358, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 373.1701 - mae: 9.5312 - val_loss: 1342.2936 - val_mae: 12.0079\n",
      "Epoch 197/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 211.3781 - mae: 8.8975\n",
      "Epoch 197: val_loss improved from 1342.29358 to 1311.63940, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 358.8428 - mae: 9.4346 - val_loss: 1311.6394 - val_mae: 11.9565\n",
      "Epoch 198/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 349.3832 - mae: 9.3782\n",
      "Epoch 198: val_loss improved from 1311.63940 to 1275.66980, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 347.2502 - mae: 9.3910 - val_loss: 1275.6698 - val_mae: 11.8927\n",
      "Epoch 199/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 346.5326 - mae: 9.3906\n",
      "Epoch 199: val_loss improved from 1275.66980 to 1248.43250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 63ms/step - loss: 335.9694 - mae: 9.3184 - val_loss: 1248.4325 - val_mae: 11.7936\n",
      "Epoch 200/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 346.7228 - mae: 9.2443\n",
      "Epoch 200: val_loss improved from 1248.43250 to 1225.04614, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 324.6129 - mae: 9.2371 - val_loss: 1225.0461 - val_mae: 11.7326\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일: 모델학습을 위한 학습과정 설정단계\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "# print(\"\\n Accuracy: %.4f\" % model.evaluate(x_valid, y_valid)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10770608b20>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_feature)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(test_label, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7adfe9f3775c954d01fc005199cfce03b4193bbc6ff21451e4d1eaf7785b4e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
