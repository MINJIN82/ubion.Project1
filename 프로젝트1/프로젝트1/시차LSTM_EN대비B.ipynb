{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>종가_ex</th>\n",
       "      <th>1Y_Mid_irs</th>\n",
       "      <th>2Y_Mid_irs</th>\n",
       "      <th>3Y_Mid_irs</th>\n",
       "      <th>5Y_Mid_irs</th>\n",
       "      <th>10Y_Mid_irs</th>\n",
       "      <th>1Y_Mid_crs</th>\n",
       "      <th>2Y_Mid_crs</th>\n",
       "      <th>3Y_Mid_crs</th>\n",
       "      <th>...</th>\n",
       "      <th>국고10년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>통안2년대비</th>\n",
       "      <th>전일비_1Y_베이시스</th>\n",
       "      <th>전일비_2Y_베이시스</th>\n",
       "      <th>전일비_3Y_베이시스</th>\n",
       "      <th>전일비_5Y_베이시스</th>\n",
       "      <th>전일비_10Y_베이시스</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_NDF차이</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>2.820</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.860</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.845</td>\n",
       "      <td>1.85</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1126.5</td>\n",
       "      <td>-7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>2</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>2.790</td>\n",
       "      <td>2.660</td>\n",
       "      <td>2.660</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.840</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.830</td>\n",
       "      <td>1.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>-6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>3</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>2.810</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.710</td>\n",
       "      <td>2.850</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.805</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>4</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>2.820</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.870</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.820</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>5</td>\n",
       "      <td>1128.3</td>\n",
       "      <td>2.830</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.740</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.820</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>-1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>2455</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.235</td>\n",
       "      <td>3.205</td>\n",
       "      <td>3.125</td>\n",
       "      <td>2.965</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.730</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>2456</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>3.155</td>\n",
       "      <td>3.215</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.095</td>\n",
       "      <td>2.935</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.68</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>2457</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>3.145</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.115</td>\n",
       "      <td>3.035</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>-2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>2458</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.205</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.085</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.730</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>7.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>2459</td>\n",
       "      <td>1299.1</td>\n",
       "      <td>3.105</td>\n",
       "      <td>3.065</td>\n",
       "      <td>3.025</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.825</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.560</td>\n",
       "      <td>2.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0   종가_ex  1Y_Mid_irs  2Y_Mid_irs  3Y_Mid_irs  \\\n",
       "DateTime                                                             \n",
       "2012-08-02           1  1131.7       2.820       2.690       2.690   \n",
       "2012-08-03           2  1134.8       2.790       2.660       2.660   \n",
       "2012-08-06           3  1129.0       2.810       2.680       2.680   \n",
       "2012-08-07           4  1128.8       2.820       2.680       2.680   \n",
       "2012-08-08           5  1128.3       2.830       2.700       2.700   \n",
       "...                ...     ...         ...         ...         ...   \n",
       "2022-07-25        2455  1313.7       3.165       3.235       3.205   \n",
       "2022-07-26        2456  1307.6       3.155       3.215       3.175   \n",
       "2022-07-27        2457  1313.3       3.145       3.165       3.115   \n",
       "2022-07-28        2458  1296.1       3.175       3.205       3.165   \n",
       "2022-07-29        2459  1299.1       3.105       3.065       3.025   \n",
       "\n",
       "            5Y_Mid_irs  10Y_Mid_irs  1Y_Mid_crs  2Y_Mid_crs  3Y_Mid_crs  ...  \\\n",
       "DateTime                                                                 ...   \n",
       "2012-08-02       2.720        2.860        2.08       1.845        1.85  ...   \n",
       "2012-08-03       2.690        2.840        2.07       1.830        1.83  ...   \n",
       "2012-08-06       2.710        2.850        2.07       1.805        1.80  ...   \n",
       "2012-08-07       2.720        2.870        2.09       1.820        1.80  ...   \n",
       "2012-08-08       2.740        2.900        2.10       1.820        1.80  ...   \n",
       "...                ...          ...         ...         ...         ...  ...   \n",
       "2022-07-25       3.125        2.965        2.55       2.730        2.71  ...   \n",
       "2022-07-26       3.095        2.935        2.56       2.700        2.68  ...   \n",
       "2022-07-27       3.035        2.875        2.57       2.690        2.67  ...   \n",
       "2022-07-28       3.085        2.945        2.61       2.730        2.71  ...   \n",
       "2022-07-29       2.945        2.825        2.50       2.560        2.56  ...   \n",
       "\n",
       "            국고10년대비  통안1년대비  통안2년대비  전일비_1Y_베이시스  전일비_2Y_베이시스  전일비_3Y_베이시스  \\\n",
       "DateTime                                                                     \n",
       "2012-08-02    -0.21   -0.03    0.02          2.0          8.0          9.0   \n",
       "2012-08-03    -0.03   -0.03    0.00          2.0          1.5          1.0   \n",
       "2012-08-06    -0.03   -0.01    0.02         -2.0         -4.5         -5.0   \n",
       "2012-08-07    -0.04   -0.01   -0.01          1.0          1.5          0.0   \n",
       "2012-08-08    -0.04   -0.06   -0.03          0.0         -2.0         -2.0   \n",
       "...             ...     ...     ...          ...          ...          ...   \n",
       "2022-07-25    -0.11    0.05    0.00         -4.0         -1.0          0.0   \n",
       "2022-07-26    -0.08    0.01   -0.05          2.0         -1.0          0.0   \n",
       "2022-07-27     0.01   -0.01   -0.02          2.0          4.0          5.0   \n",
       "2022-07-28     0.06    0.00    0.00          1.0          0.0         -1.0   \n",
       "2022-07-29     0.02    0.12    0.01         -4.0         -3.0         -1.0   \n",
       "\n",
       "            전일비_5Y_베이시스  전일비_10Y_베이시스  전날 종가_ex  종가_NDF차이  \n",
       "DateTime                                                   \n",
       "2012-08-02          9.0           9.0    1126.5     -7.50  \n",
       "2012-08-03         -5.0         -13.0    1131.7     -6.30  \n",
       "2012-08-06         -6.0          -5.0    1134.8      6.30  \n",
       "2012-08-07         -8.0         -10.0    1129.0      0.00  \n",
       "2012-08-08         -4.0          -7.0    1128.8     -1.45  \n",
       "...                 ...           ...       ...       ...  \n",
       "2022-07-25         -2.0           0.0    1313.0      3.15  \n",
       "2022-07-26          1.0           1.0    1313.7      2.70  \n",
       "2022-07-27          5.0           5.0    1307.6     -2.90  \n",
       "2022-07-28         -2.0          -5.0    1313.3      7.30  \n",
       "2022-07-29          5.0           3.0    1296.1      0.35  \n",
       "\n",
       "[2459 rows x 51 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 파일 불러오기\n",
    "df = pd.read_excel(\"./xlsx/시차상관분석4Data.xlsx\", index_col = 0)    \n",
    "\n",
    "\n",
    "df = df.set_index(\"DateTime\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', '종가_ex', '1Y_Mid_irs', '2Y_Mid_irs', '3Y_Mid_irs',\n",
       "       '5Y_Mid_irs', '10Y_Mid_irs', '1Y_Mid_crs', '2Y_Mid_crs', '3Y_Mid_crs',\n",
       "       '5Y_Mid_crs', '10Y_Mid_crs', '국고1년', '국고3년', '국고5년', '국고10년', '통안364일',\n",
       "       '통안2년', 'Mid_ndf', '전일비_ndf', '1Y_베이시스', '2Y_베이시스', '3Y_베이시스',\n",
       "       '5Y_베이시스', '10Y_베이시스', 'M1_스왑포인트', '전일대비_종가_ex', '등락률_종가_ex',\n",
       "       '전일비_1Y_irs', '전일비_2Y_irs', '전일비_3Y_irs', '전일비_5Y_irs', '전일비_10Y_irs',\n",
       "       '전일비_1Y_crs', '전일비_2Y_crs', '전일비_3Y_crs', '전일비_5Y_crs', '전일비_10Y_crs',\n",
       "       '국고1년대비', '국고3년대비', '국고5년대비', '국고10년대비', '통안1년대비', '통안2년대비',\n",
       "       '전일비_1Y_베이시스', '전일비_2Y_베이시스', '전일비_3Y_베이시스', '전일비_5Y_베이시스',\n",
       "       '전일비_10Y_베이시스', '전날 종가_ex', '종가_NDF차이'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['종가_NDF차이'] = df['전날 종가_ex'] - df['Mid_ndf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전일비_1Y_베이시스</th>\n",
       "      <th>국고3년대비</th>\n",
       "      <th>국고5년대비</th>\n",
       "      <th>M1_스왑포인트</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_NDF차이</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.488709</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>-0.495597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>-0.415698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350946</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>2.466603</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.423243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>-0.504502</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>0.003773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.324448</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>-0.092772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>1.975212</td>\n",
       "      <td>-0.234402</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>0.213508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.825382</td>\n",
       "      <td>-1.314804</td>\n",
       "      <td>-0.987488</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>0.183546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.661120</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>-0.851696</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>-0.189317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>1.153905</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.942224</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>0.489826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-0.652971</td>\n",
       "      <td>0.305799</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>0.027076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            전일비_1Y_베이시스    국고3년대비    국고5년대비  M1_스왑포인트  전날 종가_ex  종가_NDF차이\n",
       "DateTime                                                                 \n",
       "2012-08-02     0.348741 -0.488709 -1.584904  1.909409 -0.149841 -0.495597\n",
       "2012-08-03     0.348741  0.004075  0.035699  1.818881 -0.056232 -0.415698\n",
       "2012-08-06    -0.350946 -0.160187  2.466603  1.818881 -0.000426  0.423243\n",
       "2012-08-07     0.173819 -0.160187 -0.504502  1.909409 -0.104837  0.003773\n",
       "2012-08-08    -0.001103 -0.324448  0.035699  1.818881 -0.108437 -0.092772\n",
       "...                 ...       ...       ...       ...       ...       ...\n",
       "2022-07-25    -0.700790  1.975212 -0.234402 -0.896960  3.207485  0.213508\n",
       "2022-07-26     0.348741  0.825382 -1.314804 -0.987488  3.220086  0.183546\n",
       "2022-07-27     0.348741  0.661120 -1.584904 -0.851696  3.110275 -0.189317\n",
       "2022-07-28     0.173819  1.153905  0.035699 -0.942224  3.212885  0.489826\n",
       "2022-07-29    -0.700790 -0.652971  0.305799 -0.896960  2.903255  0.027076\n",
       "\n",
       "[2459 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 쓸 칼럼만 남기고 feature, target 분리해 각각 x,y 에 저장\n",
    "x = df[[ '전일비_1Y_베이시스', '국고3년대비', '국고5년대비',\n",
    "           'M1_스왑포인트', '전날 종가_ex',  '종가_NDF차이']]\n",
    "y = df[['종가_ex']]\n",
    "\n",
    "# 이건 이렇게 해야 밑에 코드 8번째 줄 columns에 들어갈 수 있다고 하네요!\n",
    "x.feature = x.columns \n",
    "x.feature\n",
    "\n",
    "# scaling 진행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전일비_1Y_베이시스</th>\n",
       "      <th>국고3년대비</th>\n",
       "      <th>국고5년대비</th>\n",
       "      <th>M1_스왑포인트</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_NDF차이</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.488709</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>-0.495597</td>\n",
       "      <td>1131.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>-0.415698</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350946</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>2.466603</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.423243</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>-0.504502</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.324448</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>-0.092772</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>1.975212</td>\n",
       "      <td>-0.234402</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>0.213508</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.825382</td>\n",
       "      <td>-1.314804</td>\n",
       "      <td>-0.987488</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>0.183546</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.661120</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>-0.851696</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>-0.189317</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>1.153905</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.942224</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>0.489826</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-0.652971</td>\n",
       "      <td>0.305799</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            전일비_1Y_베이시스    국고3년대비    국고5년대비  M1_스왑포인트  전날 종가_ex  종가_NDF차이  \\\n",
       "DateTime                                                                    \n",
       "2012-08-02     0.348741 -0.488709 -1.584904  1.909409 -0.149841 -0.495597   \n",
       "2012-08-03     0.348741  0.004075  0.035699  1.818881 -0.056232 -0.415698   \n",
       "2012-08-06    -0.350946 -0.160187  2.466603  1.818881 -0.000426  0.423243   \n",
       "2012-08-07     0.173819 -0.160187 -0.504502  1.909409 -0.104837  0.003773   \n",
       "2012-08-08    -0.001103 -0.324448  0.035699  1.818881 -0.108437 -0.092772   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "2022-07-25    -0.700790  1.975212 -0.234402 -0.896960  3.207485  0.213508   \n",
       "2022-07-26     0.348741  0.825382 -1.314804 -0.987488  3.220086  0.183546   \n",
       "2022-07-27     0.348741  0.661120 -1.584904 -0.851696  3.110275 -0.189317   \n",
       "2022-07-28     0.173819  1.153905  0.035699 -0.942224  3.212885  0.489826   \n",
       "2022-07-29    -0.700790 -0.652971  0.305799 -0.896960  2.903255  0.027076   \n",
       "\n",
       "             종가_ex  \n",
       "DateTime            \n",
       "2012-08-02  1131.7  \n",
       "2012-08-03  1134.8  \n",
       "2012-08-06  1129.0  \n",
       "2012-08-07  1128.8  \n",
       "2012-08-08  1128.3  \n",
       "...            ...  \n",
       "2022-07-25  1313.7  \n",
       "2022-07-26  1307.6  \n",
       "2022-07-27  1313.3  \n",
       "2022-07-28  1296.1  \n",
       "2022-07-29  1299.1  \n",
       "\n",
       "[2459 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = pd.concat([x_scaled,y], axis=1)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_scaled[0:1945]\n",
    "test = df_scaled[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=1):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['전일비_1Y_베이시스', '국고3년대비', '국고5년대비',\n",
    "           'M1_스왑포인트', '전날 종가_ex',  '종가_NDF차이']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.34874107,  0.0040748 , -0.23440197, -1.07801602,\n",
       "         -0.24885055,  0.3599898 ]],\n",
       "\n",
       "       [[ 0.34874107, -0.16018659,  0.03569852, -0.987488  ,\n",
       "         -0.45947099, -0.00954401]],\n",
       "\n",
       "       [[-0.87571178, -0.32444798,  0.03569852,  1.50203246,\n",
       "         -1.08593179, -0.60212977]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.17602443,  0.0040748 , -0.23440197, -1.25907205,\n",
       "         -0.18224408, -0.12273455]],\n",
       "\n",
       "       [[ 0.34874107, -0.32444798, -1.04470342, -0.85169598,\n",
       "         -1.13273633,  0.10031739]],\n",
       "\n",
       "       [[ 0.69858475, -0.32444798,  0.03569852,  1.27571242,\n",
       "         -0.42346749, -0.04283535]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((513, 1, 6), (513, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 1)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1266256.5000 - mae: 1124.1346\n",
      "Epoch 1: val_loss improved from inf to 1268771.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 48ms/step - loss: 1265969.0000 - mae: 1124.0126 - val_loss: 1268771.7500 - val_mae: 1125.2985\n",
      "Epoch 2/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1264802.3750 - mae: 1123.5009\n",
      "Epoch 2: val_loss improved from 1268771.75000 to 1267421.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 1264983.1250 - mae: 1123.5808 - val_loss: 1267421.3750 - val_mae: 1124.7090\n",
      "Epoch 3/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1263543.3750 - mae: 1122.9540\n",
      "Epoch 3: val_loss improved from 1267421.37500 to 1264911.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 1263130.3750 - mae: 1122.7699 - val_loss: 1264911.5000 - val_mae: 1123.6144\n",
      "Epoch 4/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1259685.2500 - mae: 1121.2677\n",
      "Epoch 4: val_loss improved from 1264911.50000 to 1260814.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 1259873.5000 - mae: 1121.3470 - val_loss: 1260814.0000 - val_mae: 1121.8251\n",
      "Epoch 5/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1253861.2500 - mae: 1118.7108\n",
      "Epoch 5: val_loss improved from 1260814.00000 to 1255004.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 1254889.3750 - mae: 1119.1663 - val_loss: 1255004.0000 - val_mae: 1119.2804\n",
      "Epoch 6/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1248229.3750 - mae: 1116.2390\n",
      "Epoch 6: val_loss improved from 1255004.00000 to 1247725.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 1248229.3750 - mae: 1116.2390 - val_loss: 1247725.0000 - val_mae: 1116.0784\n",
      "Epoch 7/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1240603.0000 - mae: 1112.8743\n",
      "Epoch 7: val_loss improved from 1247725.00000 to 1239020.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 1240133.7500 - mae: 1112.6652 - val_loss: 1239020.6250 - val_mae: 1112.2294\n",
      "Epoch 8/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1230530.1250 - mae: 1108.4031\n",
      "Epoch 8: val_loss improved from 1239020.62500 to 1228646.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 1230394.1250 - mae: 1108.3413 - val_loss: 1228646.6250 - val_mae: 1107.6145\n",
      "Epoch 9/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1218999.3750 - mae: 1103.2563\n",
      "Epoch 9: val_loss improved from 1228646.62500 to 1216942.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1219143.5000 - mae: 1103.3197 - val_loss: 1216942.5000 - val_mae: 1102.3705\n",
      "Epoch 10/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1205531.1250 - mae: 1097.1726\n",
      "Epoch 10: val_loss improved from 1216942.50000 to 1204228.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 1206604.7500 - mae: 1097.6760 - val_loss: 1204228.2500 - val_mae: 1096.6296\n",
      "Epoch 11/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1194053.0000 - mae: 1091.9801\n",
      "Epoch 11: val_loss improved from 1204228.25000 to 1190549.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 1193162.6250 - mae: 1091.5739 - val_loss: 1190549.0000 - val_mae: 1090.4003\n",
      "Epoch 12/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1178826.6250 - mae: 1085.0074\n",
      "Epoch 12: val_loss improved from 1190549.00000 to 1176258.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 46ms/step - loss: 1178826.6250 - mae: 1085.0074 - val_loss: 1176258.7500 - val_mae: 1083.8312\n",
      "Epoch 13/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1163393.2500 - mae: 1077.8700\n",
      "Epoch 13: val_loss improved from 1176258.75000 to 1161137.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 54ms/step - loss: 1163815.1250 - mae: 1078.0679 - val_loss: 1161137.0000 - val_mae: 1076.8109\n",
      "Epoch 14/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1148056.0000 - mae: 1070.6935\n",
      "Epoch 14: val_loss improved from 1161137.00000 to 1145401.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 1148056.0000 - mae: 1070.6935 - val_loss: 1145401.3750 - val_mae: 1069.4335\n",
      "Epoch 15/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1132841.8750 - mae: 1063.5200\n",
      "Epoch 15: val_loss improved from 1145401.37500 to 1129184.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 1131739.0000 - mae: 1062.9891 - val_loss: 1129184.2500 - val_mae: 1061.7465\n",
      "Epoch 16/200\n",
      "74/98 [=====================>........] - ETA: 0s - loss: 1114109.6250 - mae: 1054.5543\n",
      "Epoch 16: val_loss improved from 1129184.25000 to 1112431.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 1114852.1250 - mae: 1054.9319 - val_loss: 1112431.6250 - val_mae: 1053.7151\n",
      "Epoch 17/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1097635.6250 - mae: 1046.5995\n",
      "Epoch 17: val_loss improved from 1112431.62500 to 1095180.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 1097518.8750 - mae: 1046.5463 - val_loss: 1095180.6250 - val_mae: 1045.3484\n",
      "Epoch 18/200\n",
      "74/98 [=====================>........] - ETA: 0s - loss: 1082406.7500 - mae: 1039.0450\n",
      "Epoch 18: val_loss improved from 1095180.62500 to 1077744.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1079779.3750 - mae: 1037.8545 - val_loss: 1077744.8750 - val_mae: 1036.7883\n",
      "Epoch 19/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1061527.7500 - mae: 1028.7848\n",
      "Epoch 19: val_loss improved from 1077744.87500 to 1059702.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1061625.1250 - mae: 1028.8370 - val_loss: 1059702.2500 - val_mae: 1027.8112\n",
      "Epoch 20/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1043065.0625 - mae: 1019.4973\n",
      "Epoch 20: val_loss improved from 1059702.25000 to 1041393.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 1043219.6875 - mae: 1019.5870 - val_loss: 1041393.2500 - val_mae: 1018.5833\n",
      "Epoch 21/200\n",
      "77/98 [======================>.......] - ETA: 0s - loss: 1024690.3750 - mae: 1010.2916\n",
      "Epoch 21: val_loss improved from 1041393.25000 to 1022884.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 1024522.5625 - mae: 1010.0491 - val_loss: 1022884.1875 - val_mae: 1009.1235\n",
      "Epoch 22/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1005917.6875 - mae: 1000.4019\n",
      "Epoch 22: val_loss improved from 1022884.18750 to 1004265.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 1005593.4375 - mae: 1000.2505 - val_loss: 1004265.4375 - val_mae: 999.4690\n",
      "Epoch 23/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 985102.1250 - mae: 989.5198\n",
      "Epoch 23: val_loss improved from 1004265.43750 to 985424.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 986571.0000 - mae: 990.2999 - val_loss: 985424.0625 - val_mae: 989.5586\n",
      "Epoch 24/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 966244.0625 - mae: 979.3988\n",
      "Epoch 24: val_loss improved from 985424.06250 to 966377.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 967298.5625 - mae: 979.9965 - val_loss: 966377.8750 - val_mae: 979.3781\n",
      "Epoch 25/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 947930.3125 - mae: 969.4655\n",
      "Epoch 25: val_loss improved from 966377.87500 to 947305.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 46ms/step - loss: 947945.0625 - mae: 969.4911 - val_loss: 947305.9375 - val_mae: 969.0325\n",
      "Epoch 26/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 929642.1875 - mae: 959.5747\n",
      "Epoch 26: val_loss improved from 947305.93750 to 928274.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 59ms/step - loss: 928602.6875 - mae: 958.9581 - val_loss: 928274.6875 - val_mae: 958.5366\n",
      "Epoch 27/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 908309.3125 - mae: 947.7484\n",
      "Epoch 27: val_loss improved from 928274.68750 to 909120.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 909178.0625 - mae: 948.2454 - val_loss: 909120.7500 - val_mae: 947.7863\n",
      "Epoch 28/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 889105.8750 - mae: 936.8877\n",
      "Epoch 28: val_loss improved from 909120.75000 to 889934.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 889719.6250 - mae: 937.2837 - val_loss: 889934.6250 - val_mae: 936.8347\n",
      "Epoch 29/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 870208.1875 - mae: 926.0121\n",
      "Epoch 29: val_loss improved from 889934.62500 to 870947.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 870399.2500 - mae: 926.1785 - val_loss: 870947.4375 - val_mae: 925.8126\n",
      "Epoch 30/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 849567.5625 - mae: 914.0906\n",
      "Epoch 30: val_loss improved from 870947.43750 to 852017.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 851120.3125 - mae: 914.9397 - val_loss: 852017.0000 - val_mae: 914.6149\n",
      "Epoch 31/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 831957.5625 - mae: 903.5683\n",
      "Epoch 31: val_loss improved from 852017.00000 to 833308.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 832039.8125 - mae: 903.6086 - val_loss: 833308.1875 - val_mae: 903.3552\n",
      "Epoch 32/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 813562.0625 - mae: 892.4926\n",
      "Epoch 32: val_loss improved from 833308.18750 to 814511.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 812931.3750 - mae: 892.0327 - val_loss: 814511.8125 - val_mae: 891.8151\n",
      "Epoch 33/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 793788.4375 - mae: 880.1739\n",
      "Epoch 33: val_loss improved from 814511.81250 to 795972.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 793928.0000 - mae: 880.2652 - val_loss: 795972.2500 - val_mae: 880.2101\n",
      "Epoch 34/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 778198.8750 - mae: 870.2724\n",
      "Epoch 34: val_loss improved from 795972.25000 to 777693.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 775183.8750 - mae: 868.4877 - val_loss: 777693.7500 - val_mae: 868.5714\n",
      "Epoch 35/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 758254.1250 - mae: 857.5538\n",
      "Epoch 35: val_loss improved from 777693.75000 to 759483.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 756670.8750 - mae: 856.6828 - val_loss: 759483.5625 - val_mae: 856.7040\n",
      "Epoch 36/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 742232.8750 - mae: 847.2067\n",
      "Epoch 36: val_loss improved from 759483.56250 to 741564.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 738301.6875 - mae: 844.6712 - val_loss: 741564.1250 - val_mae: 844.8013\n",
      "Epoch 37/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 720225.9375 - mae: 832.8151\n",
      "Epoch 37: val_loss improved from 741564.12500 to 723769.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 720198.1875 - mae: 832.6566 - val_loss: 723769.9375 - val_mae: 832.7177\n",
      "Epoch 38/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 706258.6250 - mae: 823.4143\n",
      "Epoch 38: val_loss improved from 723769.93750 to 706292.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 702241.5625 - mae: 820.5948 - val_loss: 706292.5625 - val_mae: 820.7784\n",
      "Epoch 39/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 683594.6250 - mae: 807.8107\n",
      "Epoch 39: val_loss improved from 706292.56250 to 689015.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 684573.5000 - mae: 808.5608 - val_loss: 689015.3750 - val_mae: 808.7751\n",
      "Epoch 40/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 666250.5625 - mae: 795.7187\n",
      "Epoch 40: val_loss improved from 689015.37500 to 672194.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 667234.3750 - mae: 796.3790 - val_loss: 672194.0625 - val_mae: 796.8735\n",
      "Epoch 41/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 654282.3750 - mae: 787.4059\n",
      "Epoch 41: val_loss improved from 672194.06250 to 655545.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 650165.0625 - mae: 784.1681 - val_loss: 655545.4375 - val_mae: 784.8539\n",
      "Epoch 42/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 633669.0625 - mae: 772.0452\n",
      "Epoch 42: val_loss improved from 655545.43750 to 639155.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 633361.6875 - mae: 771.9891 - val_loss: 639155.3125 - val_mae: 772.9028\n",
      "Epoch 43/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 617962.6875 - mae: 760.7980\n",
      "Epoch 43: val_loss improved from 639155.31250 to 623174.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 616902.5625 - mae: 759.6765 - val_loss: 623174.1875 - val_mae: 761.0419\n",
      "Epoch 44/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 601601.8750 - mae: 748.3580\n",
      "Epoch 44: val_loss improved from 623174.18750 to 607375.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 600744.4375 - mae: 747.3704 - val_loss: 607375.3750 - val_mae: 749.0124\n",
      "Epoch 45/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 584431.4375 - mae: 734.7552\n",
      "Epoch 45: val_loss improved from 607375.37500 to 592026.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 584946.0625 - mae: 735.1503 - val_loss: 592026.4375 - val_mae: 737.0971\n",
      "Epoch 46/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 569525.5625 - mae: 722.8994\n",
      "Epoch 46: val_loss improved from 592026.43750 to 576925.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 569431.1875 - mae: 722.9363 - val_loss: 576925.4375 - val_mae: 725.2677\n",
      "Epoch 47/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 554485.0000 - mae: 710.5826\n",
      "Epoch 47: val_loss improved from 576925.43750 to 562091.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 554183.2500 - mae: 710.6102 - val_loss: 562091.0000 - val_mae: 713.4373\n",
      "Epoch 48/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 542167.8750 - mae: 701.0977\n",
      "Epoch 48: val_loss improved from 562091.00000 to 547497.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 60ms/step - loss: 539242.7500 - mae: 698.3179 - val_loss: 547497.3125 - val_mae: 701.6694\n",
      "Epoch 49/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 522710.4062 - mae: 684.8079\n",
      "Epoch 49: val_loss improved from 547497.31250 to 533340.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 524645.1875 - mae: 686.2008 - val_loss: 533340.4375 - val_mae: 690.2273\n",
      "Epoch 50/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 515720.0938 - mae: 678.8759\n",
      "Epoch 50: val_loss improved from 533340.43750 to 519372.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 510300.9688 - mae: 673.9881 - val_loss: 519372.3438 - val_mae: 678.9599\n",
      "Epoch 51/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 496333.8438 - mae: 662.0234\n",
      "Epoch 51: val_loss improved from 519372.34375 to 505790.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 496284.1250 - mae: 661.9900 - val_loss: 505790.3438 - val_mae: 667.7491\n",
      "Epoch 52/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 484681.4062 - mae: 651.8087\n",
      "Epoch 52: val_loss improved from 505790.34375 to 492301.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 482523.9062 - mae: 650.0563 - val_loss: 492301.5000 - val_mae: 656.5610\n",
      "Epoch 53/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 468575.7188 - mae: 637.8337\n",
      "Epoch 53: val_loss improved from 492301.50000 to 479154.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 468976.4062 - mae: 638.0024 - val_loss: 479154.2188 - val_mae: 645.2681\n",
      "Epoch 54/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 455584.1562 - mae: 626.1398\n",
      "Epoch 54: val_loss improved from 479154.21875 to 466103.78125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 455688.0312 - mae: 626.2377 - val_loss: 466103.7812 - val_mae: 634.1862\n",
      "Epoch 55/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 445952.2812 - mae: 617.0491\n",
      "Epoch 55: val_loss improved from 466103.78125 to 453283.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 442591.5625 - mae: 614.5249 - val_loss: 453283.3750 - val_mae: 623.1852\n",
      "Epoch 56/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 428174.4062 - mae: 602.0794\n",
      "Epoch 56: val_loss improved from 453283.37500 to 440675.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 429665.9688 - mae: 603.1359 - val_loss: 440675.0000 - val_mae: 612.1210\n",
      "Epoch 57/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 417254.7500 - mae: 592.3956\n",
      "Epoch 57: val_loss improved from 440675.00000 to 428133.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 416876.1562 - mae: 591.8262 - val_loss: 428133.7500 - val_mae: 601.1942\n",
      "Epoch 58/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 402603.5625 - mae: 579.1881\n",
      "Epoch 58: val_loss improved from 428133.75000 to 415710.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 404211.7812 - mae: 580.4984 - val_loss: 415710.8750 - val_mae: 590.3019\n",
      "Epoch 59/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 391667.4375 - mae: 569.1790\n",
      "Epoch 59: val_loss improved from 415710.87500 to 403303.03125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 391667.4375 - mae: 569.1790 - val_loss: 403303.0312 - val_mae: 579.5036\n",
      "Epoch 60/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 377109.4688 - mae: 556.1148\n",
      "Epoch 60: val_loss improved from 403303.03125 to 391118.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 379269.5938 - mae: 558.2067 - val_loss: 391118.8750 - val_mae: 568.9029\n",
      "Epoch 61/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 368334.1875 - mae: 549.0770\n",
      "Epoch 61: val_loss improved from 391118.87500 to 378720.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 366866.9688 - mae: 547.0646 - val_loss: 378720.5000 - val_mae: 558.0972\n",
      "Epoch 62/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 354881.8750 - mae: 536.4048\n",
      "Epoch 62: val_loss improved from 378720.50000 to 366557.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 354443.5625 - mae: 535.8204 - val_loss: 366557.4375 - val_mae: 547.3832\n",
      "Epoch 63/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 343375.9062 - mae: 526.0258\n",
      "Epoch 63: val_loss improved from 366557.43750 to 354347.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 341997.6562 - mae: 524.6521 - val_loss: 354347.6875 - val_mae: 536.6480\n",
      "Epoch 64/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 330446.0312 - mae: 514.2668\n",
      "Epoch 64: val_loss improved from 354347.68750 to 342088.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 329552.3438 - mae: 513.5107 - val_loss: 342088.5625 - val_mae: 525.9208\n",
      "Epoch 65/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 319774.6250 - mae: 504.7686\n",
      "Epoch 65: val_loss improved from 342088.56250 to 329791.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 317108.2500 - mae: 502.2477 - val_loss: 329791.1875 - val_mae: 515.0736\n",
      "Epoch 66/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 305309.4375 - mae: 491.4714\n",
      "Epoch 66: val_loss improved from 329791.18750 to 317440.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 304566.4062 - mae: 490.8607 - val_loss: 317440.9375 - val_mae: 504.1088\n",
      "Epoch 67/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 292449.0625 - mae: 479.9051\n",
      "Epoch 67: val_loss improved from 317440.93750 to 305077.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 292033.3125 - mae: 479.4591 - val_loss: 305077.5312 - val_mae: 493.0919\n",
      "Epoch 68/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 279367.1250 - mae: 467.6155\n",
      "Epoch 68: val_loss improved from 305077.53125 to 292702.59375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 279493.9688 - mae: 467.8269 - val_loss: 292702.5938 - val_mae: 481.8661\n",
      "Epoch 69/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 265212.0000 - mae: 454.8493\n",
      "Epoch 69: val_loss improved from 292702.59375 to 280178.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 266842.7812 - mae: 456.1097 - val_loss: 280178.6875 - val_mae: 470.3763\n",
      "Epoch 70/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 254100.5781 - mae: 444.0562\n",
      "Epoch 70: val_loss improved from 280178.68750 to 267626.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 254231.4531 - mae: 444.2221 - val_loss: 267626.3438 - val_mae: 458.7333\n",
      "Epoch 71/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 242262.2812 - mae: 431.8146\n",
      "Epoch 71: val_loss improved from 267626.34375 to 254976.10938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 241594.9844 - mae: 432.1785 - val_loss: 254976.1094 - val_mae: 446.8283\n",
      "Epoch 72/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 231479.6094 - mae: 423.0212\n",
      "Epoch 72: val_loss improved from 254976.10938 to 242378.67188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 229069.8438 - mae: 420.0589 - val_loss: 242378.6719 - val_mae: 434.6650\n",
      "Epoch 73/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 217713.4062 - mae: 408.5279\n",
      "Epoch 73: val_loss improved from 242378.67188 to 229807.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 216544.6562 - mae: 407.6327 - val_loss: 229807.8750 - val_mae: 422.2736\n",
      "Epoch 74/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 204514.0625 - mae: 395.1425\n",
      "Epoch 74: val_loss improved from 229807.87500 to 217196.60938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 204042.5625 - mae: 394.8994 - val_loss: 217196.6094 - val_mae: 409.7101\n",
      "Epoch 75/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 194103.3750 - mae: 385.2805\n",
      "Epoch 75: val_loss improved from 217196.60938 to 204591.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 191631.0469 - mae: 382.2316 - val_loss: 204591.0938 - val_mae: 397.0706\n",
      "Epoch 76/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 182380.1406 - mae: 372.9098\n",
      "Epoch 76: val_loss improved from 204591.09375 to 192427.04688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 179544.1406 - mae: 369.3607 - val_loss: 192427.0469 - val_mae: 384.2760\n",
      "Epoch 77/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 168400.4062 - mae: 357.1501\n",
      "Epoch 77: val_loss improved from 192427.04688 to 180427.95312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 167812.6875 - mae: 356.3167 - val_loss: 180427.9531 - val_mae: 371.2486\n",
      "Epoch 78/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 155369.3125 - mae: 342.0978\n",
      "Epoch 78: val_loss improved from 180427.95312 to 168754.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 156369.0938 - mae: 343.3138 - val_loss: 168754.6250 - val_mae: 358.2308\n",
      "Epoch 79/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 144454.7188 - mae: 329.5666\n",
      "Epoch 79: val_loss improved from 168754.62500 to 157314.28125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 145240.1562 - mae: 330.1328 - val_loss: 157314.2812 - val_mae: 345.1703\n",
      "Epoch 80/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 135510.0938 - mae: 318.3692\n",
      "Epoch 80: val_loss improved from 157314.28125 to 146064.96875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 134443.4531 - mae: 316.9854 - val_loss: 146064.9688 - val_mae: 332.0344\n",
      "Epoch 81/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 124415.8828 - mae: 305.1095\n",
      "Epoch 81: val_loss improved from 146064.96875 to 135415.54688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 124007.7656 - mae: 303.9703 - val_loss: 135415.5469 - val_mae: 318.9590\n",
      "Epoch 82/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 114545.9766 - mae: 291.5877\n",
      "Epoch 82: val_loss improved from 135415.54688 to 124970.73438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 113935.8984 - mae: 290.8830 - val_loss: 124970.7344 - val_mae: 305.6877\n",
      "Epoch 83/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 104981.3125 - mae: 278.5430\n",
      "Epoch 83: val_loss improved from 124970.73438 to 115096.90625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 104381.3438 - mae: 277.9330 - val_loss: 115096.9062 - val_mae: 292.7219\n",
      "Epoch 84/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 95549.7578 - mae: 265.5856\n",
      "Epoch 84: val_loss improved from 115096.90625 to 105634.92969, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 95344.6406 - mae: 265.2627 - val_loss: 105634.9297 - val_mae: 279.7813\n",
      "Epoch 85/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 86655.1562 - mae: 252.4474\n",
      "Epoch 85: val_loss improved from 105634.92969 to 96807.07812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 86796.6875 - mae: 252.5924 - val_loss: 96807.0781 - val_mae: 267.1497\n",
      "Epoch 86/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 77540.8516 - mae: 238.0209\n",
      "Epoch 86: val_loss improved from 96807.07812 to 88404.90625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 78703.0469 - mae: 239.9768 - val_loss: 88404.9062 - val_mae: 254.4176\n",
      "Epoch 87/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 70174.3594 - mae: 225.6791\n",
      "Epoch 87: val_loss improved from 88404.90625 to 80731.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 71194.8359 - mae: 227.7199 - val_loss: 80731.2500 - val_mae: 242.2410\n",
      "Epoch 88/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 64349.1367 - mae: 215.9917\n",
      "Epoch 88: val_loss improved from 80731.25000 to 73369.98438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 64145.9023 - mae: 215.4951 - val_loss: 73369.9844 - val_mae: 229.8173\n",
      "Epoch 89/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 57710.7891 - mae: 203.3884\n",
      "Epoch 89: val_loss improved from 73369.98438 to 66773.77344, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 57744.1953 - mae: 203.7520 - val_loss: 66773.7734 - val_mae: 218.0541\n",
      "Epoch 90/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 52595.7656 - mae: 193.6017\n",
      "Epoch 90: val_loss improved from 66773.77344 to 60677.03516, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 51901.6641 - mae: 192.3256 - val_loss: 60677.0352 - val_mae: 206.5859\n",
      "Epoch 91/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 46567.5938 - mae: 180.8313\n",
      "Epoch 91: val_loss improved from 60677.03516 to 55251.92969, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 46580.2969 - mae: 181.2099 - val_loss: 55251.9297 - val_mae: 195.7076\n",
      "Epoch 92/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 42081.2148 - mae: 171.0723\n",
      "Epoch 92: val_loss improved from 55251.92969 to 50147.26562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 41762.0898 - mae: 170.4553 - val_loss: 50147.2656 - val_mae: 184.9740\n",
      "Epoch 93/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 38152.0156 - mae: 161.6549\n",
      "Epoch 93: val_loss improved from 50147.26562 to 45678.63672, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 37430.4922 - mae: 160.4072 - val_loss: 45678.6367 - val_mae: 174.9690\n",
      "Epoch 94/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 33371.8047 - mae: 150.0951\n",
      "Epoch 94: val_loss improved from 45678.63672 to 41650.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 59ms/step - loss: 33545.3828 - mae: 150.7072 - val_loss: 41650.5625 - val_mae: 165.3386\n",
      "Epoch 95/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 30092.5449 - mae: 141.7836\n",
      "Epoch 95: val_loss improved from 41650.56250 to 37940.54688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 30096.4961 - mae: 141.4167 - val_loss: 37940.5469 - val_mae: 155.9417\n",
      "Epoch 96/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 26841.5801 - mae: 132.3447\n",
      "Epoch 96: val_loss improved from 37940.54688 to 34781.71094, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 27059.3613 - mae: 132.6921 - val_loss: 34781.7109 - val_mae: 147.2046\n",
      "Epoch 97/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 24472.8945 - mae: 124.5084\n",
      "Epoch 97: val_loss improved from 34781.71094 to 31874.04883, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 24417.0488 - mae: 124.6151 - val_loss: 31874.0488 - val_mae: 138.9430\n",
      "Epoch 98/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 22006.8848 - mae: 116.3503\n",
      "Epoch 98: val_loss improved from 31874.04883 to 29334.86914, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 57ms/step - loss: 22081.1758 - mae: 116.9460 - val_loss: 29334.8691 - val_mae: 131.0217\n",
      "Epoch 99/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 19942.1621 - mae: 109.6047\n",
      "Epoch 99: val_loss improved from 29334.86914 to 27083.98633, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 20060.3535 - mae: 109.5954 - val_loss: 27083.9863 - val_mae: 123.4889\n",
      "Epoch 100/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 18397.3301 - mae: 103.3208\n",
      "Epoch 100: val_loss improved from 27083.98633 to 25099.05469, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 18290.9785 - mae: 103.0402 - val_loss: 25099.0547 - val_mae: 116.4986\n",
      "Epoch 101/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 16101.9346 - mae: 96.5587\n",
      "Epoch 101: val_loss improved from 25099.05469 to 23354.56641, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 16784.3184 - mae: 96.9386 - val_loss: 23354.5664 - val_mae: 109.9814\n",
      "Epoch 102/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 15660.0527 - mae: 91.9204\n",
      "Epoch 102: val_loss improved from 23354.56641 to 21855.59375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 15467.6797 - mae: 91.5385 - val_loss: 21855.5938 - val_mae: 104.3452\n",
      "Epoch 103/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 14908.5410 - mae: 87.7457\n",
      "Epoch 103: val_loss improved from 21855.59375 to 20509.99023, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 14325.4639 - mae: 86.3605 - val_loss: 20509.9902 - val_mae: 98.9569\n",
      "Epoch 104/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 13393.8008 - mae: 81.9497\n",
      "Epoch 104: val_loss improved from 20509.99023 to 19333.85156, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 13325.6182 - mae: 81.8163 - val_loss: 19333.8516 - val_mae: 94.0134\n",
      "Epoch 105/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 12188.9600 - mae: 77.1884\n",
      "Epoch 105: val_loss improved from 19333.85156 to 18252.33203, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 12450.4473 - mae: 77.7475 - val_loss: 18252.3320 - val_mae: 89.5457\n",
      "Epoch 106/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 11781.5098 - mae: 74.7835\n",
      "Epoch 106: val_loss improved from 18252.33203 to 17283.25391, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 11671.1445 - mae: 74.1608 - val_loss: 17283.2539 - val_mae: 85.3902\n",
      "Epoch 107/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 10969.7324 - mae: 70.7549\n",
      "Epoch 107: val_loss improved from 17283.25391 to 16431.84570, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 10972.6621 - mae: 70.8039 - val_loss: 16431.8457 - val_mae: 81.5490\n",
      "Epoch 108/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 10045.7412 - mae: 67.1519\n",
      "Epoch 108: val_loss improved from 16431.84570 to 15640.04785, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 10341.4033 - mae: 67.7043 - val_loss: 15640.0479 - val_mae: 77.9459\n",
      "Epoch 109/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 9808.7334 - mae: 64.9882 \n",
      "Epoch 109: val_loss improved from 15640.04785 to 14874.86230, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 9763.5527 - mae: 65.0827 - val_loss: 14874.8623 - val_mae: 74.7920\n",
      "Epoch 110/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 9803.5684 - mae: 63.7817 \n",
      "Epoch 110: val_loss improved from 14874.86230 to 14178.69238, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 9259.1152 - mae: 62.5531 - val_loss: 14178.6924 - val_mae: 71.8043\n",
      "Epoch 111/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 9255.8906 - mae: 61.1031\n",
      "Epoch 111: val_loss improved from 14178.69238 to 13505.35645, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 8787.1426 - mae: 60.3229 - val_loss: 13505.3564 - val_mae: 69.0653\n",
      "Epoch 112/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 8284.1006 - mae: 58.0724\n",
      "Epoch 112: val_loss improved from 13505.35645 to 12812.17480, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 8350.1211 - mae: 58.2201 - val_loss: 12812.1748 - val_mae: 66.4223\n",
      "Epoch 113/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 8176.2734 - mae: 56.9370\n",
      "Epoch 113: val_loss improved from 12812.17480 to 12084.37109, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 7928.1562 - mae: 56.3725 - val_loss: 12084.3711 - val_mae: 64.2151\n",
      "Epoch 114/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 7679.0669 - mae: 54.9439\n",
      "Epoch 114: val_loss improved from 12084.37109 to 11381.93555, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 7528.2866 - mae: 54.5567 - val_loss: 11381.9355 - val_mae: 61.8725\n",
      "Epoch 115/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 7140.9355 - mae: 52.4190\n",
      "Epoch 115: val_loss improved from 11381.93555 to 10620.13281, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 7152.7588 - mae: 52.9317 - val_loss: 10620.1328 - val_mae: 59.6857\n",
      "Epoch 116/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 6666.8589 - mae: 51.0777\n",
      "Epoch 116: val_loss improved from 10620.13281 to 10008.97070, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 6798.7490 - mae: 51.2322 - val_loss: 10008.9707 - val_mae: 57.5763\n",
      "Epoch 117/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 6610.2007 - mae: 49.8933\n",
      "Epoch 117: val_loss improved from 10008.97070 to 9490.59082, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 6481.4204 - mae: 49.9117 - val_loss: 9490.5908 - val_mae: 55.7101\n",
      "Epoch 118/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 6345.3120 - mae: 48.7771\n",
      "Epoch 118: val_loss improved from 9490.59082 to 9052.17578, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 6195.7461 - mae: 48.5528 - val_loss: 9052.1758 - val_mae: 54.0514\n",
      "Epoch 119/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 6123.1948 - mae: 47.4030\n",
      "Epoch 119: val_loss improved from 9052.17578 to 8674.21680, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 5925.2329 - mae: 47.2309 - val_loss: 8674.2168 - val_mae: 52.3467\n",
      "Epoch 120/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 5870.4692 - mae: 46.4476\n",
      "Epoch 120: val_loss improved from 8674.21680 to 8349.16895, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 5681.8687 - mae: 45.9449 - val_loss: 8349.1689 - val_mae: 50.8682\n",
      "Epoch 121/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 5100.7969 - mae: 44.3652\n",
      "Epoch 121: val_loss improved from 8349.16895 to 8063.48096, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 59ms/step - loss: 5448.9751 - mae: 44.7380 - val_loss: 8063.4810 - val_mae: 49.5149\n",
      "Epoch 122/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 5019.7300 - mae: 42.9199\n",
      "Epoch 122: val_loss improved from 8063.48096 to 7809.43506, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 5232.1357 - mae: 43.6355 - val_loss: 7809.4351 - val_mae: 48.3009\n",
      "Epoch 123/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 4984.7217 - mae: 42.5056\n",
      "Epoch 123: val_loss improved from 7809.43506 to 7550.86426, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 5028.0186 - mae: 42.5943 - val_loss: 7550.8643 - val_mae: 47.1122\n",
      "Epoch 124/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 4825.8643 - mae: 41.5994\n",
      "Epoch 124: val_loss improved from 7550.86426 to 7312.02686, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 4825.8643 - mae: 41.5994 - val_loss: 7312.0269 - val_mae: 46.0717\n",
      "Epoch 125/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 4345.7134 - mae: 40.3488\n",
      "Epoch 125: val_loss improved from 7312.02686 to 7095.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 4641.4854 - mae: 40.7220 - val_loss: 7095.5000 - val_mae: 45.0013\n",
      "Epoch 126/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 4461.2139 - mae: 39.5816\n",
      "Epoch 126: val_loss improved from 7095.50000 to 6875.17090, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 54ms/step - loss: 4461.2139 - mae: 39.5816 - val_loss: 6875.1709 - val_mae: 43.9445\n",
      "Epoch 127/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 4528.7002 - mae: 39.0159\n",
      "Epoch 127: val_loss improved from 6875.17090 to 6647.87158, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 4283.3564 - mae: 38.8177 - val_loss: 6647.8716 - val_mae: 43.0385\n",
      "Epoch 128/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 4140.4878 - mae: 38.2878\n",
      "Epoch 128: val_loss improved from 6647.87158 to 6450.80420, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 4103.2510 - mae: 37.9120 - val_loss: 6450.8042 - val_mae: 42.1134\n",
      "Epoch 129/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 3938.0498 - mae: 37.0368\n",
      "Epoch 129: val_loss improved from 6450.80420 to 6249.95459, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 3938.0498 - mae: 37.0368 - val_loss: 6249.9546 - val_mae: 41.1601\n",
      "Epoch 130/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 3894.0359 - mae: 36.1631\n",
      "Epoch 130: val_loss improved from 6249.95459 to 6052.66455, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 3776.2913 - mae: 36.0616 - val_loss: 6052.6646 - val_mae: 40.1509\n",
      "Epoch 131/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 3506.5769 - mae: 34.9548\n",
      "Epoch 131: val_loss improved from 6052.66455 to 5855.86621, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 3627.3655 - mae: 35.2170 - val_loss: 5855.8662 - val_mae: 39.1758\n",
      "Epoch 132/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 3088.1653 - mae: 33.2016\n",
      "Epoch 132: val_loss improved from 5855.86621 to 5658.23926, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 3469.6848 - mae: 34.2177 - val_loss: 5658.2393 - val_mae: 38.1686\n",
      "Epoch 133/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 3591.9121 - mae: 34.2057\n",
      "Epoch 133: val_loss improved from 5658.23926 to 5460.49609, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 3313.1125 - mae: 33.2922 - val_loss: 5460.4961 - val_mae: 37.2169\n",
      "Epoch 134/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 3387.2732 - mae: 32.9160\n",
      "Epoch 134: val_loss improved from 5460.49609 to 5277.21289, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 3172.6152 - mae: 32.3975 - val_loss: 5277.2129 - val_mae: 36.2868\n",
      "Epoch 135/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 2430.9404 - mae: 30.3501\n",
      "Epoch 135: val_loss improved from 5277.21289 to 5119.23926, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 3027.1938 - mae: 31.4941 - val_loss: 5119.2393 - val_mae: 35.3560\n",
      "Epoch 136/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 2882.0767 - mae: 30.5091\n",
      "Epoch 136: val_loss improved from 5119.23926 to 4923.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 2893.1477 - mae: 30.5922 - val_loss: 4923.3438 - val_mae: 34.4293\n",
      "Epoch 137/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 2476.5657 - mae: 29.4608\n",
      "Epoch 137: val_loss improved from 4923.34375 to 4768.67383, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 2756.3354 - mae: 29.6918 - val_loss: 4768.6738 - val_mae: 33.5063\n",
      "Epoch 138/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 2574.3535 - mae: 28.4836\n",
      "Epoch 138: val_loss improved from 4768.67383 to 4589.21484, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 2639.3213 - mae: 28.7941 - val_loss: 4589.2148 - val_mae: 32.5421\n",
      "Epoch 139/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 2216.3066 - mae: 27.6784\n",
      "Epoch 139: val_loss improved from 4589.21484 to 4424.74561, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 2519.8416 - mae: 27.8329 - val_loss: 4424.7456 - val_mae: 31.5937\n",
      "Epoch 140/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 2458.0442 - mae: 27.1195\n",
      "Epoch 140: val_loss improved from 4424.74561 to 4269.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 2411.7632 - mae: 27.0167 - val_loss: 4269.6875 - val_mae: 30.6601\n",
      "Epoch 141/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 2338.5698 - mae: 26.2375\n",
      "Epoch 141: val_loss improved from 4269.68750 to 4110.74463, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 2303.1250 - mae: 26.1394 - val_loss: 4110.7446 - val_mae: 29.6520\n",
      "Epoch 142/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 2199.8132 - mae: 25.2198\n",
      "Epoch 142: val_loss improved from 4110.74463 to 3958.88818, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 2199.8132 - mae: 25.2198 - val_loss: 3958.8882 - val_mae: 28.7597\n",
      "Epoch 143/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 2102.3201 - mae: 24.3710\n",
      "Epoch 143: val_loss improved from 3958.88818 to 3812.89648, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 2102.3201 - mae: 24.3710 - val_loss: 3812.8965 - val_mae: 27.8394\n",
      "Epoch 144/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 2003.8606 - mae: 23.5717\n",
      "Epoch 144: val_loss improved from 3812.89648 to 3677.05908, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 2003.8606 - mae: 23.5717 - val_loss: 3677.0591 - val_mae: 27.0610\n",
      "Epoch 145/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 2052.6921 - mae: 23.1805\n",
      "Epoch 145: val_loss improved from 3677.05908 to 3536.12939, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 1906.9500 - mae: 22.7891 - val_loss: 3536.1294 - val_mae: 26.0624\n",
      "Epoch 146/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1803.7091 - mae: 21.7352\n",
      "Epoch 146: val_loss improved from 3536.12939 to 3398.97192, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 1806.3990 - mae: 21.8526 - val_loss: 3398.9719 - val_mae: 25.1690\n",
      "Epoch 147/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 1668.7107 - mae: 21.0040\n",
      "Epoch 147: val_loss improved from 3398.97192 to 3273.64941, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 1710.4248 - mae: 21.0448 - val_loss: 3273.6494 - val_mae: 24.4336\n",
      "Epoch 148/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1633.6143 - mae: 20.3783\n",
      "Epoch 148: val_loss improved from 3273.64941 to 3153.97632, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 1618.5828 - mae: 20.3103 - val_loss: 3153.9763 - val_mae: 23.5842\n",
      "Epoch 149/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1574.9127 - mae: 19.5208\n",
      "Epoch 149: val_loss improved from 3153.97632 to 3027.94067, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 1523.6077 - mae: 19.4186 - val_loss: 3027.9407 - val_mae: 22.7875\n",
      "Epoch 150/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1430.4489 - mae: 18.6858\n",
      "Epoch 150: val_loss improved from 3027.94067 to 2918.09985, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 1430.4489 - mae: 18.6858 - val_loss: 2918.0999 - val_mae: 22.1235\n",
      "Epoch 151/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 1200.4768 - mae: 18.0480\n",
      "Epoch 151: val_loss improved from 2918.09985 to 2809.25195, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 1337.1223 - mae: 18.0034 - val_loss: 2809.2520 - val_mae: 21.3575\n",
      "Epoch 152/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 1236.3269 - mae: 17.0851\n",
      "Epoch 152: val_loss improved from 2809.25195 to 2704.34863, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 1252.1917 - mae: 17.1989 - val_loss: 2704.3486 - val_mae: 20.7455\n",
      "Epoch 153/200\n",
      "78/98 [======================>.......] - ETA: 0s - loss: 1129.5081 - mae: 16.2521\n",
      "Epoch 153: val_loss improved from 2704.34863 to 2609.69336, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 1171.3146 - mae: 16.5635 - val_loss: 2609.6934 - val_mae: 20.0609\n",
      "Epoch 154/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1094.7780 - mae: 15.9087\n",
      "Epoch 154: val_loss improved from 2609.69336 to 2522.13281, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 46ms/step - loss: 1097.7415 - mae: 15.9848 - val_loss: 2522.1328 - val_mae: 19.5069\n",
      "Epoch 155/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1036.6921 - mae: 15.3232\n",
      "Epoch 155: val_loss improved from 2522.13281 to 2434.53027, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 1031.4342 - mae: 15.3463 - val_loss: 2434.5303 - val_mae: 18.9382\n",
      "Epoch 156/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 970.3724 - mae: 14.7530\n",
      "Epoch 156: val_loss improved from 2434.53027 to 2359.09839, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 970.3724 - mae: 14.7530 - val_loss: 2359.0984 - val_mae: 18.4048\n",
      "Epoch 157/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 943.1521 - mae: 14.2767\n",
      "Epoch 157: val_loss improved from 2359.09839 to 2287.07275, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 916.8040 - mae: 14.2032 - val_loss: 2287.0728 - val_mae: 17.9682\n",
      "Epoch 158/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 948.2429 - mae: 13.9484\n",
      "Epoch 158: val_loss improved from 2287.07275 to 2216.44141, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 63ms/step - loss: 866.5327 - mae: 13.8296 - val_loss: 2216.4414 - val_mae: 17.5594\n",
      "Epoch 159/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 823.1166 - mae: 13.3429\n",
      "Epoch 159: val_loss improved from 2216.44141 to 2149.67920, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 823.1166 - mae: 13.3429 - val_loss: 2149.6792 - val_mae: 17.1026\n",
      "Epoch 160/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 842.0276 - mae: 13.2171\n",
      "Epoch 160: val_loss improved from 2149.67920 to 2095.36523, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 784.3017 - mae: 13.0085 - val_loss: 2095.3652 - val_mae: 16.7170\n",
      "Epoch 161/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 811.9256 - mae: 12.6982\n",
      "Epoch 161: val_loss improved from 2095.36523 to 2038.85876, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 747.0913 - mae: 12.5535 - val_loss: 2038.8588 - val_mae: 16.3984\n",
      "Epoch 162/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 711.5756 - mae: 12.1495\n",
      "Epoch 162: val_loss improved from 2038.85876 to 1985.04895, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 713.8423 - mae: 12.2897 - val_loss: 1985.0490 - val_mae: 16.0721\n",
      "Epoch 163/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 702.6996 - mae: 12.1502\n",
      "Epoch 163: val_loss improved from 1985.04895 to 1931.90332, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 684.3657 - mae: 12.0226 - val_loss: 1931.9033 - val_mae: 15.7245\n",
      "Epoch 164/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 672.1917 - mae: 11.7997\n",
      "Epoch 164: val_loss improved from 1931.90332 to 1884.67419, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 654.7441 - mae: 11.7107 - val_loss: 1884.6742 - val_mae: 15.5056\n",
      "Epoch 165/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 657.4072 - mae: 11.3541\n",
      "Epoch 165: val_loss improved from 1884.67419 to 1838.15845, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 627.8536 - mae: 11.4644 - val_loss: 1838.1584 - val_mae: 15.2307\n",
      "Epoch 166/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 624.9091 - mae: 11.2563\n",
      "Epoch 166: val_loss improved from 1838.15845 to 1795.24744, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 603.5338 - mae: 11.1589 - val_loss: 1795.2474 - val_mae: 15.0214\n",
      "Epoch 167/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 594.2780 - mae: 11.0671\n",
      "Epoch 167: val_loss improved from 1795.24744 to 1747.43237, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 584.0442 - mae: 11.0277 - val_loss: 1747.4324 - val_mae: 14.7640\n",
      "Epoch 168/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 561.7552 - mae: 10.7560\n",
      "Epoch 168: val_loss improved from 1747.43237 to 1713.54102, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 560.8311 - mae: 10.7484 - val_loss: 1713.5410 - val_mae: 14.5717\n",
      "Epoch 169/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 565.5209 - mae: 10.6183\n",
      "Epoch 169: val_loss improved from 1713.54102 to 1674.19775, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 542.1849 - mae: 10.5689 - val_loss: 1674.1978 - val_mae: 14.3896\n",
      "Epoch 170/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 525.0527 - mae: 10.4131\n",
      "Epoch 170: val_loss improved from 1674.19775 to 1630.48267, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 525.0527 - mae: 10.4131 - val_loss: 1630.4827 - val_mae: 14.1812\n",
      "Epoch 171/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 369.2556 - mae: 9.7772\n",
      "Epoch 171: val_loss improved from 1630.48267 to 1606.24561, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 507.1081 - mae: 10.2239 - val_loss: 1606.2456 - val_mae: 14.1182\n",
      "Epoch 172/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 495.0191 - mae: 10.0612\n",
      "Epoch 172: val_loss improved from 1606.24561 to 1568.04919, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 491.5393 - mae: 10.1707 - val_loss: 1568.0492 - val_mae: 13.9110\n",
      "Epoch 173/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 488.9604 - mae: 10.0591\n",
      "Epoch 173: val_loss improved from 1568.04919 to 1536.54712, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 475.6612 - mae: 9.9727 - val_loss: 1536.5471 - val_mae: 13.8032\n",
      "Epoch 174/200\n",
      "78/98 [======================>.......] - ETA: 0s - loss: 537.6862 - mae: 10.1040\n",
      "Epoch 174: val_loss improved from 1536.54712 to 1497.83276, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 461.5511 - mae: 9.8724 - val_loss: 1497.8328 - val_mae: 13.5747\n",
      "Epoch 175/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 333.1206 - mae: 9.3730\n",
      "Epoch 175: val_loss improved from 1497.83276 to 1468.36731, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 447.6345 - mae: 9.7344 - val_loss: 1468.3673 - val_mae: 13.5073\n",
      "Epoch 176/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 434.2934 - mae: 9.7017\n",
      "Epoch 176: val_loss improved from 1468.36731 to 1437.56592, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 434.2934 - mae: 9.7017 - val_loss: 1437.5659 - val_mae: 13.3140\n",
      "Epoch 177/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 460.6977 - mae: 9.6780\n",
      "Epoch 177: val_loss improved from 1437.56592 to 1409.41821, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 421.0584 - mae: 9.5378 - val_loss: 1409.4182 - val_mae: 13.2429\n",
      "Epoch 178/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 417.5734 - mae: 9.4312\n",
      "Epoch 178: val_loss improved from 1409.41821 to 1374.89905, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 409.0387 - mae: 9.4449 - val_loss: 1374.8990 - val_mae: 13.0616\n",
      "Epoch 179/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 320.6710 - mae: 9.1801\n",
      "Epoch 179: val_loss improved from 1374.89905 to 1350.74341, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 397.6094 - mae: 9.3310 - val_loss: 1350.7434 - val_mae: 13.0123\n",
      "Epoch 180/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 390.6909 - mae: 9.2654\n",
      "Epoch 180: val_loss improved from 1350.74341 to 1322.19324, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 390.2408 - mae: 9.2708 - val_loss: 1322.1932 - val_mae: 12.9914\n",
      "Epoch 181/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 376.9837 - mae: 9.1436\n",
      "Epoch 181: val_loss improved from 1322.19324 to 1292.60425, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 377.2113 - mae: 9.1607 - val_loss: 1292.6042 - val_mae: 12.7279\n",
      "Epoch 182/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 375.0213 - mae: 8.9519\n",
      "Epoch 182: val_loss improved from 1292.60425 to 1262.69629, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 365.7568 - mae: 9.0829 - val_loss: 1262.6963 - val_mae: 12.6259\n",
      "Epoch 183/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 340.6170 - mae: 8.6822\n",
      "Epoch 183: val_loss improved from 1262.69629 to 1245.95752, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 355.5681 - mae: 8.9512 - val_loss: 1245.9575 - val_mae: 12.5758\n",
      "Epoch 184/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 354.4418 - mae: 8.7659\n",
      "Epoch 184: val_loss improved from 1245.95752 to 1220.97571, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 346.0816 - mae: 8.8734 - val_loss: 1220.9757 - val_mae: 12.4502\n",
      "Epoch 185/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 335.6648 - mae: 8.7841\n",
      "Epoch 185: val_loss improved from 1220.97571 to 1194.67639, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 335.6648 - mae: 8.7841 - val_loss: 1194.6764 - val_mae: 12.3394\n",
      "Epoch 186/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 331.7010 - mae: 8.7216\n",
      "Epoch 186: val_loss improved from 1194.67639 to 1170.85718, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 326.9182 - mae: 8.7208 - val_loss: 1170.8572 - val_mae: 12.1746\n",
      "Epoch 187/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 319.1051 - mae: 8.6145\n",
      "Epoch 187: val_loss improved from 1170.85718 to 1146.84314, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 318.7017 - mae: 8.6143 - val_loss: 1146.8431 - val_mae: 12.1816\n",
      "Epoch 188/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 320.2723 - mae: 8.5920\n",
      "Epoch 188: val_loss improved from 1146.84314 to 1129.21570, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 308.7348 - mae: 8.5280 - val_loss: 1129.2157 - val_mae: 12.0189\n",
      "Epoch 189/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 328.4892 - mae: 8.5095\n",
      "Epoch 189: val_loss improved from 1129.21570 to 1108.05530, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 300.2290 - mae: 8.4340 - val_loss: 1108.0553 - val_mae: 11.9242\n",
      "Epoch 190/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 151.8146 - mae: 7.9753\n",
      "Epoch 190: val_loss improved from 1108.05530 to 1068.63953, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 292.1322 - mae: 8.4059 - val_loss: 1068.6395 - val_mae: 11.7850\n",
      "Epoch 191/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 293.9364 - mae: 8.4235\n",
      "Epoch 191: val_loss did not improve from 1068.63953\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 284.9437 - mae: 8.3747 - val_loss: 1078.3752 - val_mae: 11.7605\n",
      "Epoch 192/200\n",
      "79/98 [=======================>......] - ETA: 0s - loss: 220.9407 - mae: 8.0789\n",
      "Epoch 192: val_loss improved from 1068.63953 to 1044.12585, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 276.7207 - mae: 8.2526 - val_loss: 1044.1259 - val_mae: 11.6164\n",
      "Epoch 193/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 202.2315 - mae: 7.9628\n",
      "Epoch 193: val_loss improved from 1044.12585 to 1027.60706, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 269.2400 - mae: 8.1900 - val_loss: 1027.6071 - val_mae: 11.6241\n",
      "Epoch 194/200\n",
      "79/98 [=======================>......] - ETA: 0s - loss: 297.4977 - mae: 8.2999\n",
      "Epoch 194: val_loss improved from 1027.60706 to 1004.97961, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 263.8221 - mae: 8.1556 - val_loss: 1004.9796 - val_mae: 11.4518\n",
      "Epoch 195/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 264.8494 - mae: 8.1570\n",
      "Epoch 195: val_loss improved from 1004.97961 to 980.96564, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 257.0734 - mae: 8.1127 - val_loss: 980.9656 - val_mae: 11.2950\n",
      "Epoch 196/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 240.4936 - mae: 7.9823\n",
      "Epoch 196: val_loss improved from 980.96564 to 969.37036, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 46ms/step - loss: 251.4382 - mae: 8.0633 - val_loss: 969.3704 - val_mae: 11.2660\n",
      "Epoch 197/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 274.7962 - mae: 8.0984\n",
      "Epoch 197: val_loss improved from 969.37036 to 949.92310, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 243.9174 - mae: 7.9893 - val_loss: 949.9231 - val_mae: 11.2015\n",
      "Epoch 198/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 238.1351 - mae: 7.9217\n",
      "Epoch 198: val_loss improved from 949.92310 to 941.33746, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 236.0434 - mae: 7.9093 - val_loss: 941.3375 - val_mae: 11.1218\n",
      "Epoch 199/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 241.4629 - mae: 7.8098\n",
      "Epoch 199: val_loss improved from 941.33746 to 901.08417, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 230.1674 - mae: 7.8407 - val_loss: 901.0842 - val_mae: 10.9196\n",
      "Epoch 200/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 223.9674 - mae: 7.7663\n",
      "Epoch 200: val_loss did not improve from 901.08417\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 223.9674 - mae: 7.7663 - val_loss: 908.1827 - val_mae: 10.9832\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일: 모델학습을 위한 학습과정 설정단계\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "# print(\"\\n Accuracy: %.4f\" % model.evaluate(x_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x169e423a4f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_feature)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(test_label, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7adfe9f3775c954d01fc005199cfce03b4193bbc6ff21451e4d1eaf7785b4e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
