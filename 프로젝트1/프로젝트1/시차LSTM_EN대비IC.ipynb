{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>종가_ex</th>\n",
       "      <th>1Y_Mid_irs</th>\n",
       "      <th>2Y_Mid_irs</th>\n",
       "      <th>3Y_Mid_irs</th>\n",
       "      <th>5Y_Mid_irs</th>\n",
       "      <th>10Y_Mid_irs</th>\n",
       "      <th>1Y_Mid_crs</th>\n",
       "      <th>2Y_Mid_crs</th>\n",
       "      <th>3Y_Mid_crs</th>\n",
       "      <th>...</th>\n",
       "      <th>국고10년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>통안2년대비</th>\n",
       "      <th>전일비_1Y_베이시스</th>\n",
       "      <th>전일비_2Y_베이시스</th>\n",
       "      <th>전일비_3Y_베이시스</th>\n",
       "      <th>전일비_5Y_베이시스</th>\n",
       "      <th>전일비_10Y_베이시스</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_NDF차이</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>2.820</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.860</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.845</td>\n",
       "      <td>1.85</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1126.5</td>\n",
       "      <td>-7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>2</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>2.790</td>\n",
       "      <td>2.660</td>\n",
       "      <td>2.660</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.840</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.830</td>\n",
       "      <td>1.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>-6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>3</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>2.810</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.710</td>\n",
       "      <td>2.850</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.805</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>4</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>2.820</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.870</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.820</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>5</td>\n",
       "      <td>1128.3</td>\n",
       "      <td>2.830</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.740</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.820</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>-1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>2455</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.235</td>\n",
       "      <td>3.205</td>\n",
       "      <td>3.125</td>\n",
       "      <td>2.965</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.730</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>2456</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>3.155</td>\n",
       "      <td>3.215</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.095</td>\n",
       "      <td>2.935</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.68</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>2457</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>3.145</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.115</td>\n",
       "      <td>3.035</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>-2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>2458</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.205</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.085</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.730</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>7.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>2459</td>\n",
       "      <td>1299.1</td>\n",
       "      <td>3.105</td>\n",
       "      <td>3.065</td>\n",
       "      <td>3.025</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.825</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.560</td>\n",
       "      <td>2.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0   종가_ex  1Y_Mid_irs  2Y_Mid_irs  3Y_Mid_irs  \\\n",
       "DateTime                                                             \n",
       "2012-08-02           1  1131.7       2.820       2.690       2.690   \n",
       "2012-08-03           2  1134.8       2.790       2.660       2.660   \n",
       "2012-08-06           3  1129.0       2.810       2.680       2.680   \n",
       "2012-08-07           4  1128.8       2.820       2.680       2.680   \n",
       "2012-08-08           5  1128.3       2.830       2.700       2.700   \n",
       "...                ...     ...         ...         ...         ...   \n",
       "2022-07-25        2455  1313.7       3.165       3.235       3.205   \n",
       "2022-07-26        2456  1307.6       3.155       3.215       3.175   \n",
       "2022-07-27        2457  1313.3       3.145       3.165       3.115   \n",
       "2022-07-28        2458  1296.1       3.175       3.205       3.165   \n",
       "2022-07-29        2459  1299.1       3.105       3.065       3.025   \n",
       "\n",
       "            5Y_Mid_irs  10Y_Mid_irs  1Y_Mid_crs  2Y_Mid_crs  3Y_Mid_crs  ...  \\\n",
       "DateTime                                                                 ...   \n",
       "2012-08-02       2.720        2.860        2.08       1.845        1.85  ...   \n",
       "2012-08-03       2.690        2.840        2.07       1.830        1.83  ...   \n",
       "2012-08-06       2.710        2.850        2.07       1.805        1.80  ...   \n",
       "2012-08-07       2.720        2.870        2.09       1.820        1.80  ...   \n",
       "2012-08-08       2.740        2.900        2.10       1.820        1.80  ...   \n",
       "...                ...          ...         ...         ...         ...  ...   \n",
       "2022-07-25       3.125        2.965        2.55       2.730        2.71  ...   \n",
       "2022-07-26       3.095        2.935        2.56       2.700        2.68  ...   \n",
       "2022-07-27       3.035        2.875        2.57       2.690        2.67  ...   \n",
       "2022-07-28       3.085        2.945        2.61       2.730        2.71  ...   \n",
       "2022-07-29       2.945        2.825        2.50       2.560        2.56  ...   \n",
       "\n",
       "            국고10년대비  통안1년대비  통안2년대비  전일비_1Y_베이시스  전일비_2Y_베이시스  전일비_3Y_베이시스  \\\n",
       "DateTime                                                                     \n",
       "2012-08-02    -0.21   -0.03    0.02          2.0          8.0          9.0   \n",
       "2012-08-03    -0.03   -0.03    0.00          2.0          1.5          1.0   \n",
       "2012-08-06    -0.03   -0.01    0.02         -2.0         -4.5         -5.0   \n",
       "2012-08-07    -0.04   -0.01   -0.01          1.0          1.5          0.0   \n",
       "2012-08-08    -0.04   -0.06   -0.03          0.0         -2.0         -2.0   \n",
       "...             ...     ...     ...          ...          ...          ...   \n",
       "2022-07-25    -0.11    0.05    0.00         -4.0         -1.0          0.0   \n",
       "2022-07-26    -0.08    0.01   -0.05          2.0         -1.0          0.0   \n",
       "2022-07-27     0.01   -0.01   -0.02          2.0          4.0          5.0   \n",
       "2022-07-28     0.06    0.00    0.00          1.0          0.0         -1.0   \n",
       "2022-07-29     0.02    0.12    0.01         -4.0         -3.0         -1.0   \n",
       "\n",
       "            전일비_5Y_베이시스  전일비_10Y_베이시스  전날 종가_ex  종가_NDF차이  \n",
       "DateTime                                                   \n",
       "2012-08-02          9.0           9.0    1126.5     -7.50  \n",
       "2012-08-03         -5.0         -13.0    1131.7     -6.30  \n",
       "2012-08-06         -6.0          -5.0    1134.8      6.30  \n",
       "2012-08-07         -8.0         -10.0    1129.0      0.00  \n",
       "2012-08-08         -4.0          -7.0    1128.8     -1.45  \n",
       "...                 ...           ...       ...       ...  \n",
       "2022-07-25         -2.0           0.0    1313.0      3.15  \n",
       "2022-07-26          1.0           1.0    1313.7      2.70  \n",
       "2022-07-27          5.0           5.0    1307.6     -2.90  \n",
       "2022-07-28         -2.0          -5.0    1313.3      7.30  \n",
       "2022-07-29          5.0           3.0    1296.1      0.35  \n",
       "\n",
       "[2459 rows x 51 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 파일 불러오기\n",
    "df = pd.read_excel(\"./xlsx/시차상관분석4Data.xlsx\", index_col = 0)    \n",
    "\n",
    "df = df.set_index(\"DateTime\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['종가_NDF차이'] = df['전날 종가_ex'] - df['Mid_ndf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', '종가_ex', '1Y_Mid_irs', '2Y_Mid_irs', '3Y_Mid_irs',\n",
       "       '5Y_Mid_irs', '10Y_Mid_irs', '1Y_Mid_crs', '2Y_Mid_crs', '3Y_Mid_crs',\n",
       "       '5Y_Mid_crs', '10Y_Mid_crs', '국고1년', '국고3년', '국고5년', '국고10년', '통안364일',\n",
       "       '통안2년', 'Mid_ndf', '전일비_ndf', '1Y_베이시스', '2Y_베이시스', '3Y_베이시스',\n",
       "       '5Y_베이시스', '10Y_베이시스', 'M1_스왑포인트', '전일대비_종가_ex', '등락률_종가_ex',\n",
       "       '전일비_1Y_irs', '전일비_2Y_irs', '전일비_3Y_irs', '전일비_5Y_irs', '전일비_10Y_irs',\n",
       "       '전일비_1Y_crs', '전일비_2Y_crs', '전일비_3Y_crs', '전일비_5Y_crs', '전일비_10Y_crs',\n",
       "       '국고1년대비', '국고3년대비', '국고5년대비', '국고10년대비', '통안1년대비', '통안2년대비',\n",
       "       '전일비_1Y_베이시스', '전일비_2Y_베이시스', '전일비_3Y_베이시스', '전일비_5Y_베이시스',\n",
       "       '전일비_10Y_베이시스', '전날 종가_ex', '종가_NDF차이'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전일비_1Y_irs</th>\n",
       "      <th>전일비_2Y_irs</th>\n",
       "      <th>전일비_3Y_irs</th>\n",
       "      <th>전일비_10Y_irs</th>\n",
       "      <th>전일비_1Y_crs</th>\n",
       "      <th>전일비_3Y_crs</th>\n",
       "      <th>국고3년대비</th>\n",
       "      <th>국고5년대비</th>\n",
       "      <th>국고10년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>M1_스왑포인트</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_NDF차이</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>0.533389</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>2.032691</td>\n",
       "      <td>-0.205655</td>\n",
       "      <td>0.905480</td>\n",
       "      <td>-0.488709</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>-5.377146</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>-0.495597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-1.136944</td>\n",
       "      <td>-1.011189</td>\n",
       "      <td>1.210863</td>\n",
       "      <td>-0.159384</td>\n",
       "      <td>-0.205655</td>\n",
       "      <td>-0.463176</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.767092</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>-0.415698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>0.790819</td>\n",
       "      <td>3.338756</td>\n",
       "      <td>2.032691</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>-0.691285</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>2.466603</td>\n",
       "      <td>-0.767092</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.423243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.110185</td>\n",
       "      <td>-0.089516</td>\n",
       "      <td>0.936654</td>\n",
       "      <td>0.401191</td>\n",
       "      <td>-0.006957</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>-0.504502</td>\n",
       "      <td>-1.023207</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>0.003773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>1.040201</td>\n",
       "      <td>1.563108</td>\n",
       "      <td>-0.089516</td>\n",
       "      <td>-1.255421</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.006957</td>\n",
       "      <td>-0.324448</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-1.023207</td>\n",
       "      <td>-0.650470</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>-0.092772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.238900</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>0.388635</td>\n",
       "      <td>-1.621628</td>\n",
       "      <td>-1.603722</td>\n",
       "      <td>1.975212</td>\n",
       "      <td>-0.234402</td>\n",
       "      <td>-2.816005</td>\n",
       "      <td>0.541331</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>0.213508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>1.048249</td>\n",
       "      <td>0.265133</td>\n",
       "      <td>-0.342057</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.691285</td>\n",
       "      <td>0.825382</td>\n",
       "      <td>-1.314804</td>\n",
       "      <td>-2.047663</td>\n",
       "      <td>0.107949</td>\n",
       "      <td>-0.987488</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>0.183546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.496329</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>-0.890075</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.235067</td>\n",
       "      <td>0.661120</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>0.257364</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>-0.851696</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>-0.189317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>0.790819</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>0.571308</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.905480</td>\n",
       "      <td>1.153905</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>1.537934</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.942224</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>0.489826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>1.185344</td>\n",
       "      <td>0.790819</td>\n",
       "      <td>0.028701</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>-2.228474</td>\n",
       "      <td>-3.428596</td>\n",
       "      <td>-0.652971</td>\n",
       "      <td>0.305799</td>\n",
       "      <td>0.513478</td>\n",
       "      <td>1.299750</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>0.027076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            전일비_1Y_irs  전일비_2Y_irs  전일비_3Y_irs  전일비_10Y_irs  전일비_1Y_crs  \\\n",
       "DateTime                                                                  \n",
       "2012-08-02    0.604772    0.533389   -0.207732     2.032691   -0.205655   \n",
       "2012-08-03   -1.136944   -1.011189    1.210863    -0.159384   -0.205655   \n",
       "2012-08-06    0.604772    0.790819    3.338756     2.032691   -0.003373   \n",
       "2012-08-07   -0.266086   -0.110185   -0.089516     0.936654    0.401191   \n",
       "2012-08-08    1.040201    1.563108   -0.089516    -1.255421    0.198909   \n",
       "...                ...         ...         ...          ...         ...   \n",
       "2022-07-25   -0.266086   -0.238900   -0.207732     0.388635   -1.621628   \n",
       "2022-07-26    0.604772    1.048249    0.265133    -0.342057    0.198909   \n",
       "2022-07-27   -0.266086   -0.496329   -0.207732    -0.890075    0.198909   \n",
       "2022-07-28    0.604772    0.790819   -0.207732     0.571308    0.805755   \n",
       "2022-07-29    1.185344    0.790819    0.028701     0.023289   -2.228474   \n",
       "\n",
       "            전일비_3Y_crs    국고3년대비    국고5년대비   국고10년대비    통안1년대비  M1_스왑포인트  \\\n",
       "DateTime                                                                   \n",
       "2012-08-02    0.905480 -0.488709 -1.584904 -5.377146 -0.325433  1.909409   \n",
       "2012-08-03   -0.463176  0.004075  0.035699 -0.767092 -0.325433  1.818881   \n",
       "2012-08-06   -0.691285 -0.160187  2.466603 -0.767092 -0.108742  1.818881   \n",
       "2012-08-07   -0.006957 -0.160187 -0.504502 -1.023207 -0.108742  1.909409   \n",
       "2012-08-08   -0.006957 -0.324448  0.035699 -1.023207 -0.650470  1.818881   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "2022-07-25   -1.603722  1.975212 -0.234402 -2.816005  0.541331 -0.896960   \n",
       "2022-07-26   -0.691285  0.825382 -1.314804 -2.047663  0.107949 -0.987488   \n",
       "2022-07-27   -0.235067  0.661120 -1.584904  0.257364 -0.108742 -0.851696   \n",
       "2022-07-28    0.905480  1.153905  0.035699  1.537934 -0.000397 -0.942224   \n",
       "2022-07-29   -3.428596 -0.652971  0.305799  0.513478  1.299750 -0.896960   \n",
       "\n",
       "            전날 종가_ex  종가_NDF차이  \n",
       "DateTime                        \n",
       "2012-08-02 -0.149841 -0.495597  \n",
       "2012-08-03 -0.056232 -0.415698  \n",
       "2012-08-06 -0.000426  0.423243  \n",
       "2012-08-07 -0.104837  0.003773  \n",
       "2012-08-08 -0.108437 -0.092772  \n",
       "...              ...       ...  \n",
       "2022-07-25  3.207485  0.213508  \n",
       "2022-07-26  3.220086  0.183546  \n",
       "2022-07-27  3.110275 -0.189317  \n",
       "2022-07-28  3.212885  0.489826  \n",
       "2022-07-29  2.903255  0.027076  \n",
       "\n",
       "[2459 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 쓸 칼럼만 남기고 feature, target 분리해 각각 x,y 에 저장\n",
    "x = df[[ '전일비_1Y_irs','전일비_2Y_irs', '전일비_3Y_irs', '전일비_10Y_irs', \n",
    "          '전일비_1Y_crs', '전일비_3Y_crs',        \n",
    "           '국고3년대비', '국고5년대비', '국고10년대비', '통안1년대비', 'M1_스왑포인트',\n",
    "            '전날 종가_ex', '종가_NDF차이']]\n",
    "y = df[['종가_ex']]\n",
    "\n",
    "# 이건 이렇게 해야 밑에 코드 8번째 줄 columns에 들어갈 수 있다고 하네요!\n",
    "x.feature = x.columns \n",
    "x.feature\n",
    "\n",
    "# scaling 진행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전일비_1Y_irs</th>\n",
       "      <th>전일비_2Y_irs</th>\n",
       "      <th>전일비_3Y_irs</th>\n",
       "      <th>전일비_10Y_irs</th>\n",
       "      <th>전일비_1Y_crs</th>\n",
       "      <th>전일비_3Y_crs</th>\n",
       "      <th>국고3년대비</th>\n",
       "      <th>국고5년대비</th>\n",
       "      <th>국고10년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>M1_스왑포인트</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_NDF차이</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>0.533389</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>2.032691</td>\n",
       "      <td>-0.205655</td>\n",
       "      <td>0.905480</td>\n",
       "      <td>-0.488709</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>-5.377146</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>-0.495597</td>\n",
       "      <td>1131.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-1.136944</td>\n",
       "      <td>-1.011189</td>\n",
       "      <td>1.210863</td>\n",
       "      <td>-0.159384</td>\n",
       "      <td>-0.205655</td>\n",
       "      <td>-0.463176</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.767092</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>-0.415698</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>0.790819</td>\n",
       "      <td>3.338756</td>\n",
       "      <td>2.032691</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>-0.691285</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>2.466603</td>\n",
       "      <td>-0.767092</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.423243</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.110185</td>\n",
       "      <td>-0.089516</td>\n",
       "      <td>0.936654</td>\n",
       "      <td>0.401191</td>\n",
       "      <td>-0.006957</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>-0.504502</td>\n",
       "      <td>-1.023207</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>1.040201</td>\n",
       "      <td>1.563108</td>\n",
       "      <td>-0.089516</td>\n",
       "      <td>-1.255421</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.006957</td>\n",
       "      <td>-0.324448</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-1.023207</td>\n",
       "      <td>-0.650470</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>-0.092772</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.238900</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>0.388635</td>\n",
       "      <td>-1.621628</td>\n",
       "      <td>-1.603722</td>\n",
       "      <td>1.975212</td>\n",
       "      <td>-0.234402</td>\n",
       "      <td>-2.816005</td>\n",
       "      <td>0.541331</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>0.213508</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>1.048249</td>\n",
       "      <td>0.265133</td>\n",
       "      <td>-0.342057</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.691285</td>\n",
       "      <td>0.825382</td>\n",
       "      <td>-1.314804</td>\n",
       "      <td>-2.047663</td>\n",
       "      <td>0.107949</td>\n",
       "      <td>-0.987488</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>0.183546</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.496329</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>-0.890075</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.235067</td>\n",
       "      <td>0.661120</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>0.257364</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>-0.851696</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>-0.189317</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>0.790819</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>0.571308</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.905480</td>\n",
       "      <td>1.153905</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>1.537934</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.942224</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>0.489826</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>1.185344</td>\n",
       "      <td>0.790819</td>\n",
       "      <td>0.028701</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>-2.228474</td>\n",
       "      <td>-3.428596</td>\n",
       "      <td>-0.652971</td>\n",
       "      <td>0.305799</td>\n",
       "      <td>0.513478</td>\n",
       "      <td>1.299750</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            전일비_1Y_irs  전일비_2Y_irs  전일비_3Y_irs  전일비_10Y_irs  전일비_1Y_crs  \\\n",
       "DateTime                                                                  \n",
       "2012-08-02    0.604772    0.533389   -0.207732     2.032691   -0.205655   \n",
       "2012-08-03   -1.136944   -1.011189    1.210863    -0.159384   -0.205655   \n",
       "2012-08-06    0.604772    0.790819    3.338756     2.032691   -0.003373   \n",
       "2012-08-07   -0.266086   -0.110185   -0.089516     0.936654    0.401191   \n",
       "2012-08-08    1.040201    1.563108   -0.089516    -1.255421    0.198909   \n",
       "...                ...         ...         ...          ...         ...   \n",
       "2022-07-25   -0.266086   -0.238900   -0.207732     0.388635   -1.621628   \n",
       "2022-07-26    0.604772    1.048249    0.265133    -0.342057    0.198909   \n",
       "2022-07-27   -0.266086   -0.496329   -0.207732    -0.890075    0.198909   \n",
       "2022-07-28    0.604772    0.790819   -0.207732     0.571308    0.805755   \n",
       "2022-07-29    1.185344    0.790819    0.028701     0.023289   -2.228474   \n",
       "\n",
       "            전일비_3Y_crs    국고3년대비    국고5년대비   국고10년대비    통안1년대비  M1_스왑포인트  \\\n",
       "DateTime                                                                   \n",
       "2012-08-02    0.905480 -0.488709 -1.584904 -5.377146 -0.325433  1.909409   \n",
       "2012-08-03   -0.463176  0.004075  0.035699 -0.767092 -0.325433  1.818881   \n",
       "2012-08-06   -0.691285 -0.160187  2.466603 -0.767092 -0.108742  1.818881   \n",
       "2012-08-07   -0.006957 -0.160187 -0.504502 -1.023207 -0.108742  1.909409   \n",
       "2012-08-08   -0.006957 -0.324448  0.035699 -1.023207 -0.650470  1.818881   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "2022-07-25   -1.603722  1.975212 -0.234402 -2.816005  0.541331 -0.896960   \n",
       "2022-07-26   -0.691285  0.825382 -1.314804 -2.047663  0.107949 -0.987488   \n",
       "2022-07-27   -0.235067  0.661120 -1.584904  0.257364 -0.108742 -0.851696   \n",
       "2022-07-28    0.905480  1.153905  0.035699  1.537934 -0.000397 -0.942224   \n",
       "2022-07-29   -3.428596 -0.652971  0.305799  0.513478  1.299750 -0.896960   \n",
       "\n",
       "            전날 종가_ex  종가_NDF차이   종가_ex  \n",
       "DateTime                                \n",
       "2012-08-02 -0.149841 -0.495597  1131.7  \n",
       "2012-08-03 -0.056232 -0.415698  1134.8  \n",
       "2012-08-06 -0.000426  0.423243  1129.0  \n",
       "2012-08-07 -0.104837  0.003773  1128.8  \n",
       "2012-08-08 -0.108437 -0.092772  1128.3  \n",
       "...              ...       ...     ...  \n",
       "2022-07-25  3.207485  0.213508  1313.7  \n",
       "2022-07-26  3.220086  0.183546  1307.6  \n",
       "2022-07-27  3.110275 -0.189317  1313.3  \n",
       "2022-07-28  3.212885  0.489826  1296.1  \n",
       "2022-07-29  2.903255  0.027076  1299.1  \n",
       "\n",
       "[2459 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = pd.concat([x_scaled,y], axis=1)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_scaled[0:1945]\n",
    "test = df_scaled[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=1):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['전일비_1Y_irs', '전일비_10Y_irs', '전일비_1Y_crs', '전일비_3Y_crs',        \n",
    "           '국고3년대비', '국고5년대비', 'M1_스왑포인트',\n",
    "            '전날 종가_ex', '종가_NDF차이']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.02420034,  0.20596204, -0.40793665, ..., -1.12328003,\n",
       "         -0.10123622,  0.13360873]],\n",
       "\n",
       "       [[-0.26608569, -0.25072025, -0.61021861, ...,  1.77361651,\n",
       "         -1.39736201, -0.30916602]],\n",
       "\n",
       "       [[-0.26608569,  0.02328913,  0.60347313, ...,  0.50622428,\n",
       "         -0.47927291, -0.19930462]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.02420034,  0.20596204, -0.20565469, ..., -1.30433606,\n",
       "         -1.2983524 , -0.01620228]],\n",
       "\n",
       "       [[-0.55637171,  0.02328913,  0.19890922, ..., -0.76116796,\n",
       "          0.25699855,  0.17688746]],\n",
       "\n",
       "       [[ 0.02420034, -1.43809419, -0.00337274, ..., -1.07801602,\n",
       "          0.50722284, -0.27920382]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((513, 1, 9), (513, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 1)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1269034.2500 - mae: 1125.3386\n",
      "Epoch 1: val_loss improved from inf to 1258494.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 12s 86ms/step - loss: 1268456.0000 - mae: 1125.0884 - val_loss: 1258494.6250 - val_mae: 1120.8126\n",
      "Epoch 2/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1266622.5000 - mae: 1124.2715\n",
      "Epoch 2: val_loss improved from 1258494.62500 to 1256679.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1267146.8750 - mae: 1124.4977 - val_loss: 1256679.3750 - val_mae: 1119.9896\n",
      "Epoch 3/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 1263094.0000 - mae: 1122.6971\n",
      "Epoch 3: val_loss improved from 1256679.37500 to 1253090.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1264444.1250 - mae: 1123.2728 - val_loss: 1253090.8750 - val_mae: 1118.3563\n",
      "Epoch 4/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1261478.5000 - mae: 1121.9242\n",
      "Epoch 4: val_loss improved from 1253090.87500 to 1247177.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1259693.2500 - mae: 1121.1066 - val_loss: 1247177.7500 - val_mae: 1115.6559\n",
      "Epoch 5/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1251806.6250 - mae: 1117.5129\n",
      "Epoch 5: val_loss improved from 1247177.75000 to 1238869.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1252570.2500 - mae: 1117.8512 - val_loss: 1238869.3750 - val_mae: 1111.8412\n",
      "Epoch 6/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1242398.0000 - mae: 1113.1824\n",
      "Epoch 6: val_loss improved from 1238869.37500 to 1228485.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1243180.0000 - mae: 1113.5364 - val_loss: 1228485.8750 - val_mae: 1107.0500\n",
      "Epoch 7/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 1230100.2500 - mae: 1107.5132\n",
      "Epoch 7: val_loss improved from 1228485.87500 to 1216379.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 1231939.1250 - mae: 1108.3401 - val_loss: 1216379.2500 - val_mae: 1101.4283\n",
      "Epoch 8/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1217979.3750 - mae: 1101.8438\n",
      "Epoch 8: val_loss improved from 1216379.25000 to 1202782.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1219079.3750 - mae: 1102.3547 - val_loss: 1202782.5000 - val_mae: 1095.0707\n",
      "Epoch 9/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1204851.5000 - mae: 1095.6471\n",
      "Epoch 9: val_loss improved from 1202782.50000 to 1187927.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 1204838.5000 - mae: 1095.6733 - val_loss: 1187927.8750 - val_mae: 1088.0656\n",
      "Epoch 10/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1189154.2500 - mae: 1088.2482\n",
      "Epoch 10: val_loss improved from 1187927.87500 to 1171744.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1189324.5000 - mae: 1088.3276 - val_loss: 1171744.6250 - val_mae: 1080.3632\n",
      "Epoch 11/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1172191.1250 - mae: 1080.1697\n",
      "Epoch 11: val_loss improved from 1171744.62500 to 1154586.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1172760.3750 - mae: 1080.4071 - val_loss: 1154586.2500 - val_mae: 1072.1145\n",
      "Epoch 12/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1153998.8750 - mae: 1071.3756\n",
      "Epoch 12: val_loss improved from 1154586.25000 to 1136493.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1155119.2500 - mae: 1071.8773 - val_loss: 1136493.6250 - val_mae: 1063.3262\n",
      "Epoch 13/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1136932.8750 - mae: 1063.0074\n",
      "Epoch 13: val_loss improved from 1136493.62500 to 1117538.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1136694.1250 - mae: 1062.8817 - val_loss: 1117538.1250 - val_mae: 1054.0131\n",
      "Epoch 14/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1118018.1250 - mae: 1053.6466\n",
      "Epoch 14: val_loss improved from 1117538.12500 to 1097866.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1117503.2500 - mae: 1053.3804 - val_loss: 1097866.2500 - val_mae: 1044.2323\n",
      "Epoch 15/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1099153.7500 - mae: 1044.1936\n",
      "Epoch 15: val_loss improved from 1097866.25000 to 1077495.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1097558.5000 - mae: 1043.3793 - val_loss: 1077495.0000 - val_mae: 1033.9663\n",
      "Epoch 16/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1075998.2500 - mae: 1032.4867\n",
      "Epoch 16: val_loss improved from 1077495.00000 to 1056663.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1077143.5000 - mae: 1033.0425 - val_loss: 1056663.6250 - val_mae: 1023.3321\n",
      "Epoch 17/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1055635.8750 - mae: 1021.9903\n",
      "Epoch 17: val_loss improved from 1056663.62500 to 1035399.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1056174.6250 - mae: 1022.2333 - val_loss: 1035399.1875 - val_mae: 1012.3304\n",
      "Epoch 18/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1034755.2500 - mae: 1011.0881\n",
      "Epoch 18: val_loss improved from 1035399.18750 to 1013651.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1034765.9375 - mae: 1011.0401 - val_loss: 1013651.9375 - val_mae: 1000.9022\n",
      "Epoch 19/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1013188.8125 - mae: 999.5486 \n",
      "Epoch 19: val_loss improved from 1013651.93750 to 991584.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1012929.0000 - mae: 999.4612 - val_loss: 991584.5000 - val_mae: 989.1281\n",
      "Epoch 20/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 989993.1250 - mae: 987.1199\n",
      "Epoch 20: val_loss improved from 991584.50000 to 969189.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 990915.5625 - mae: 987.5623 - val_loss: 969189.7500 - val_mae: 976.9924\n",
      "Epoch 21/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 973845.3125 - mae: 978.2309\n",
      "Epoch 21: val_loss improved from 969189.75000 to 946746.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 968649.0000 - mae: 975.3829 - val_loss: 946746.3750 - val_mae: 964.6423\n",
      "Epoch 22/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 949417.2500 - mae: 964.5178\n",
      "Epoch 22: val_loss improved from 946746.37500 to 924066.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 946150.8750 - mae: 962.7995 - val_loss: 924066.2500 - val_mae: 951.9308\n",
      "Epoch 23/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 922207.8125 - mae: 949.1298\n",
      "Epoch 23: val_loss improved from 924066.25000 to 901142.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 923560.5625 - mae: 949.9808 - val_loss: 901142.9375 - val_mae: 938.8242\n",
      "Epoch 24/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 905452.8750 - mae: 939.5087\n",
      "Epoch 24: val_loss improved from 901142.93750 to 878408.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 900933.1250 - mae: 936.8715 - val_loss: 878408.7500 - val_mae: 925.6316\n",
      "Epoch 25/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 878258.0000 - mae: 923.5044\n",
      "Epoch 25: val_loss improved from 878408.75000 to 855584.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 878309.5000 - mae: 923.5502 - val_loss: 855584.7500 - val_mae: 912.1176\n",
      "Epoch 26/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 859090.7500 - mae: 912.1133\n",
      "Epoch 26: val_loss improved from 855584.75000 to 832797.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 67ms/step - loss: 855667.8750 - mae: 909.9171 - val_loss: 832797.8750 - val_mae: 898.3621\n",
      "Epoch 27/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 832276.3125 - mae: 895.4073\n",
      "Epoch 27: val_loss improved from 832797.87500 to 810149.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 833207.7500 - mae: 896.1247 - val_loss: 810149.1250 - val_mae: 884.4042\n",
      "Epoch 28/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 815519.3750 - mae: 885.0040\n",
      "Epoch 28: val_loss improved from 810149.12500 to 787719.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 810866.7500 - mae: 882.1430 - val_loss: 787719.0000 - val_mae: 870.3257\n",
      "Epoch 29/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 794953.5000 - mae: 872.1507\n",
      "Epoch 29: val_loss improved from 787719.00000 to 765445.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 788706.6250 - mae: 868.0403 - val_loss: 765445.6250 - val_mae: 856.0394\n",
      "Epoch 30/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 764000.1250 - mae: 852.1434\n",
      "Epoch 30: val_loss improved from 765445.62500 to 743354.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 766809.8750 - mae: 853.9251 - val_loss: 743354.3125 - val_mae: 841.5574\n",
      "Epoch 31/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 743397.3750 - mae: 838.8262\n",
      "Epoch 31: val_loss improved from 743354.31250 to 721610.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 745160.8750 - mae: 839.6392 - val_loss: 721610.1250 - val_mae: 827.0406\n",
      "Epoch 32/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 726618.3750 - mae: 826.9568\n",
      "Epoch 32: val_loss improved from 721610.12500 to 700139.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 723785.3125 - mae: 825.2802 - val_loss: 700139.1250 - val_mae: 812.3595\n",
      "Epoch 33/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 702109.8125 - mae: 810.4662\n",
      "Epoch 33: val_loss improved from 700139.12500 to 679003.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 702737.8750 - mae: 810.8380 - val_loss: 679003.1875 - val_mae: 797.6031\n",
      "Epoch 34/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 685402.6875 - mae: 798.5344\n",
      "Epoch 34: val_loss improved from 679003.18750 to 658234.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 682052.2500 - mae: 796.3341 - val_loss: 658234.3750 - val_mae: 782.7953\n",
      "Epoch 35/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 663054.2500 - mae: 782.9174\n",
      "Epoch 35: val_loss improved from 658234.37500 to 637781.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 661686.1250 - mae: 781.7946 - val_loss: 637781.4375 - val_mae: 767.8314\n",
      "Epoch 36/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 645027.7500 - mae: 769.5693\n",
      "Epoch 36: val_loss improved from 637781.43750 to 617682.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 641729.2500 - mae: 767.0993 - val_loss: 617682.3125 - val_mae: 752.8099\n",
      "Epoch 37/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 625797.5000 - mae: 755.6226\n",
      "Epoch 37: val_loss improved from 617682.31250 to 598070.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 622162.8125 - mae: 752.5089 - val_loss: 598070.4375 - val_mae: 737.8615\n",
      "Epoch 38/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 603614.8750 - mae: 738.2915\n",
      "Epoch 38: val_loss improved from 598070.43750 to 578871.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 603035.5625 - mae: 737.9772 - val_loss: 578871.5000 - val_mae: 722.8478\n",
      "Epoch 39/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 579379.9375 - mae: 719.5287\n",
      "Epoch 39: val_loss improved from 578871.50000 to 560084.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 584274.6250 - mae: 723.5043 - val_loss: 560084.0000 - val_mae: 707.8311\n",
      "Epoch 40/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 565323.3125 - mae: 708.4501\n",
      "Epoch 40: val_loss improved from 560084.00000 to 541323.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 565857.3125 - mae: 709.0063 - val_loss: 541323.1250 - val_mae: 692.6314\n",
      "Epoch 41/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 551746.7500 - mae: 697.9186\n",
      "Epoch 41: val_loss improved from 541323.12500 to 522507.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 547328.1875 - mae: 694.2672 - val_loss: 522507.7500 - val_mae: 677.2413\n",
      "Epoch 42/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 529259.7500 - mae: 679.8122\n",
      "Epoch 42: val_loss improved from 522507.75000 to 503737.90625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 528566.3125 - mae: 679.3377 - val_loss: 503737.9062 - val_mae: 661.7946\n",
      "Epoch 43/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 510731.5000 - mae: 665.5430\n",
      "Epoch 43: val_loss improved from 503737.90625 to 485324.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 510082.0312 - mae: 664.3667 - val_loss: 485324.5000 - val_mae: 646.4900\n",
      "Epoch 44/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 491106.3125 - mae: 648.8929\n",
      "Epoch 44: val_loss improved from 485324.50000 to 467447.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 492096.5312 - mae: 649.6281 - val_loss: 467447.0000 - val_mae: 631.5299\n",
      "Epoch 45/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 474768.4062 - mae: 635.1525\n",
      "Epoch 45: val_loss improved from 467447.00000 to 449976.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 57ms/step - loss: 474504.4375 - mae: 634.9239 - val_loss: 449976.1875 - val_mae: 616.7281\n",
      "Epoch 46/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 459894.5000 - mae: 622.5827\n",
      "Epoch 46: val_loss improved from 449976.18750 to 433013.03125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 457340.9062 - mae: 620.2333 - val_loss: 433013.0312 - val_mae: 602.3129\n",
      "Epoch 47/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 441877.5938 - mae: 608.1284\n",
      "Epoch 47: val_loss improved from 433013.03125 to 416455.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 440664.1875 - mae: 606.0377 - val_loss: 416455.4375 - val_mae: 587.9953\n",
      "Epoch 48/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 425644.4062 - mae: 593.3780\n",
      "Epoch 48: val_loss improved from 416455.43750 to 400375.28125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 424428.5625 - mae: 592.0168 - val_loss: 400375.2812 - val_mae: 573.9837\n",
      "Epoch 49/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 409571.8750 - mae: 579.3805\n",
      "Epoch 49: val_loss improved from 400375.28125 to 384814.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 408667.8750 - mae: 578.5336 - val_loss: 384814.4688 - val_mae: 560.4265\n",
      "Epoch 50/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 395508.6250 - mae: 567.2406\n",
      "Epoch 50: val_loss improved from 384814.46875 to 369524.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 393295.6250 - mae: 565.2634 - val_loss: 369524.8125 - val_mae: 546.9556\n",
      "Epoch 51/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 374077.6250 - mae: 547.9188\n",
      "Epoch 51: val_loss improved from 369524.81250 to 354722.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 378290.9062 - mae: 552.3228 - val_loss: 354722.7188 - val_mae: 533.6217\n",
      "Epoch 52/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 360751.3125 - mae: 536.9581\n",
      "Epoch 52: val_loss improved from 354722.71875 to 340292.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 363719.3125 - mae: 539.7758 - val_loss: 340292.8125 - val_mae: 520.7229\n",
      "Epoch 53/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 349707.6250 - mae: 527.6218\n",
      "Epoch 53: val_loss improved from 340292.81250 to 326344.28125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 349518.3125 - mae: 527.4268 - val_loss: 326344.2812 - val_mae: 508.1552\n",
      "Epoch 54/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 334453.8438 - mae: 514.4720\n",
      "Epoch 54: val_loss improved from 326344.28125 to 312837.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 335722.4375 - mae: 515.5942 - val_loss: 312837.5000 - val_mae: 496.1697\n",
      "Epoch 55/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 322679.2188 - mae: 502.9205\n",
      "Epoch 55: val_loss improved from 312837.50000 to 299748.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 322285.0000 - mae: 503.8449 - val_loss: 299748.3438 - val_mae: 484.6762\n",
      "Epoch 56/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 311567.4375 - mae: 494.2863\n",
      "Epoch 56: val_loss improved from 299748.34375 to 286744.28125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 309183.1250 - mae: 492.4090 - val_loss: 286744.2812 - val_mae: 473.2657\n",
      "Epoch 57/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 296404.3750 - mae: 481.0797\n",
      "Epoch 57: val_loss improved from 286744.28125 to 274201.84375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 296404.3750 - mae: 481.0797 - val_loss: 274201.8438 - val_mae: 462.0744\n",
      "Epoch 58/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 284317.5312 - mae: 470.1632\n",
      "Epoch 58: val_loss improved from 274201.84375 to 262042.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 284022.0312 - mae: 469.9451 - val_loss: 262042.5312 - val_mae: 451.0679\n",
      "Epoch 59/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 271951.5000 - mae: 458.6394\n",
      "Epoch 59: val_loss improved from 262042.53125 to 250222.35938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 271913.8125 - mae: 459.0607 - val_loss: 250222.3594 - val_mae: 440.4330\n",
      "Epoch 60/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 261483.1250 - mae: 448.5776\n",
      "Epoch 60: val_loss improved from 250222.35938 to 238587.07812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 260062.6875 - mae: 448.1638 - val_loss: 238587.0781 - val_mae: 429.8682\n",
      "Epoch 61/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 247521.4688 - mae: 438.0382\n",
      "Epoch 61: val_loss improved from 238587.07812 to 227275.32812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 248564.4219 - mae: 437.3599 - val_loss: 227275.3281 - val_mae: 419.4695\n",
      "Epoch 62/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 237403.5000 - mae: 426.7683\n",
      "Epoch 62: val_loss improved from 227275.32812 to 216576.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 237403.5000 - mae: 426.7683 - val_loss: 216576.3750 - val_mae: 409.4357\n",
      "Epoch 63/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 226743.0625 - mae: 416.3975\n",
      "Epoch 63: val_loss improved from 216576.37500 to 205973.60938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 226479.3125 - mae: 416.2085 - val_loss: 205973.6094 - val_mae: 399.2389\n",
      "Epoch 64/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 216363.2344 - mae: 406.9472\n",
      "Epoch 64: val_loss improved from 205973.60938 to 195711.42188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 216006.8594 - mae: 405.4399 - val_loss: 195711.4219 - val_mae: 389.0126\n",
      "Epoch 65/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 206281.8281 - mae: 397.4395\n",
      "Epoch 65: val_loss improved from 195711.42188 to 185978.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 205746.3125 - mae: 394.7888 - val_loss: 185978.8750 - val_mae: 379.0329\n",
      "Epoch 66/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 196438.3438 - mae: 383.5269\n",
      "Epoch 66: val_loss improved from 185978.87500 to 176629.60938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 195869.0000 - mae: 384.5154 - val_loss: 176629.6094 - val_mae: 369.2463\n",
      "Epoch 67/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 187265.7188 - mae: 374.6226\n",
      "Epoch 67: val_loss improved from 176629.60938 to 167364.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 186238.8281 - mae: 373.8082 - val_loss: 167364.0938 - val_mae: 359.2162\n",
      "Epoch 68/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 176955.8281 - mae: 363.4276\n",
      "Epoch 68: val_loss improved from 167364.09375 to 158415.89062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 176955.8281 - mae: 363.4276 - val_loss: 158415.8906 - val_mae: 349.2829\n",
      "Epoch 69/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 167948.0469 - mae: 352.3590\n",
      "Epoch 69: val_loss improved from 158415.89062 to 150082.59375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 167981.6406 - mae: 353.0363 - val_loss: 150082.5938 - val_mae: 339.6426\n",
      "Epoch 70/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 161368.8594 - mae: 345.1043\n",
      "Epoch 70: val_loss improved from 150082.59375 to 141939.89062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 159347.3594 - mae: 342.7646 - val_loss: 141939.8906 - val_mae: 329.8613\n",
      "Epoch 71/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 147485.3438 - mae: 331.1614\n",
      "Epoch 71: val_loss improved from 141939.89062 to 134108.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 151091.5781 - mae: 332.3286 - val_loss: 134108.9375 - val_mae: 320.0768\n",
      "Epoch 72/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 143739.2812 - mae: 323.1709\n",
      "Epoch 72: val_loss improved from 134108.93750 to 127500.41406, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 143627.4688 - mae: 323.0183 - val_loss: 127500.4141 - val_mae: 311.4922\n",
      "Epoch 73/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 139570.9062 - mae: 316.0627\n",
      "Epoch 73: val_loss improved from 127500.41406 to 120602.73438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 136341.0781 - mae: 313.6361 - val_loss: 120602.7344 - val_mae: 302.2233\n",
      "Epoch 74/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 131116.4688 - mae: 305.7092\n",
      "Epoch 74: val_loss improved from 120602.73438 to 113978.28125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 67ms/step - loss: 129257.1484 - mae: 303.8080 - val_loss: 113978.2812 - val_mae: 292.9424\n",
      "Epoch 75/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 122559.7734 - mae: 294.3460\n",
      "Epoch 75: val_loss improved from 113978.28125 to 107722.59375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 122559.7734 - mae: 294.3460 - val_loss: 107722.5938 - val_mae: 283.8085\n",
      "Epoch 76/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 116060.1562 - mae: 283.8251\n",
      "Epoch 76: val_loss improved from 107722.59375 to 101869.21094, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 116143.8281 - mae: 284.9685 - val_loss: 101869.2109 - val_mae: 274.9907\n",
      "Epoch 77/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 110168.8984 - mae: 276.0501\n",
      "Epoch 77: val_loss improved from 101869.21094 to 96176.14062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 110002.4609 - mae: 275.7957 - val_loss: 96176.1406 - val_mae: 266.0630\n",
      "Epoch 78/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 106923.1641 - mae: 268.4877\n",
      "Epoch 78: val_loss improved from 96176.14062 to 90849.89844, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 104201.0625 - mae: 266.5142 - val_loss: 90849.8984 - val_mae: 257.2785\n",
      "Epoch 79/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 94530.9844 - mae: 256.8136\n",
      "Epoch 79: val_loss improved from 90849.89844 to 85892.23438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 98717.3750 - mae: 257.7386 - val_loss: 85892.2344 - val_mae: 248.7420\n",
      "Epoch 80/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 89172.4297 - mae: 246.0893\n",
      "Epoch 80: val_loss improved from 85892.23438 to 81162.21094, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 93522.1094 - mae: 248.6957 - val_loss: 81162.2109 - val_mae: 240.2308\n",
      "Epoch 81/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 89099.5000 - mae: 239.7978\n",
      "Epoch 81: val_loss improved from 81162.21094 to 76745.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 88544.1719 - mae: 240.0292 - val_loss: 76745.4688 - val_mae: 232.0798\n",
      "Epoch 82/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 83751.1875 - mae: 231.5506\n",
      "Epoch 82: val_loss improved from 76745.46875 to 72504.75781, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 83878.8516 - mae: 231.4274 - val_loss: 72504.7578 - val_mae: 223.8209\n",
      "Epoch 83/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 76349.4062 - mae: 222.9264\n",
      "Epoch 83: val_loss improved from 72504.75781 to 68659.70312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 79490.8203 - mae: 223.0769 - val_loss: 68659.7031 - val_mae: 215.7706\n",
      "Epoch 84/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 78008.7266 - mae: 216.5289\n",
      "Epoch 84: val_loss improved from 68659.70312 to 65047.57422, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 75387.8828 - mae: 214.7539 - val_loss: 65047.5742 - val_mae: 208.3753\n",
      "Epoch 85/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 63708.6641 - mae: 203.8585\n",
      "Epoch 85: val_loss improved from 65047.57422 to 61548.53906, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 71577.8906 - mae: 206.7563 - val_loss: 61548.5391 - val_mae: 200.5866\n",
      "Epoch 86/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 67916.6797 - mae: 198.6460\n",
      "Epoch 86: val_loss improved from 61548.53906 to 58446.16016, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 67916.6797 - mae: 198.6460 - val_loss: 58446.1602 - val_mae: 193.2555\n",
      "Epoch 87/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 65383.9297 - mae: 192.7253\n",
      "Epoch 87: val_loss improved from 58446.16016 to 55807.45312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 64591.8594 - mae: 191.9864 - val_loss: 55807.4531 - val_mae: 187.0697\n",
      "Epoch 88/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 64710.2109 - mae: 187.5333\n",
      "Epoch 88: val_loss improved from 55807.45312 to 53025.12109, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 61443.5820 - mae: 184.6664 - val_loss: 53025.1211 - val_mae: 180.0665\n",
      "Epoch 89/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 59846.8516 - mae: 178.7098\n",
      "Epoch 89: val_loss improved from 53025.12109 to 50446.63281, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 58489.6172 - mae: 177.5175 - val_loss: 50446.6328 - val_mae: 173.2777\n",
      "Epoch 90/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 56247.4375 - mae: 170.7057\n",
      "Epoch 90: val_loss improved from 50446.63281 to 48062.36328, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 55718.4102 - mae: 170.4055 - val_loss: 48062.3633 - val_mae: 166.5788\n",
      "Epoch 91/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 56466.8828 - mae: 164.7094\n",
      "Epoch 91: val_loss improved from 48062.36328 to 45803.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 53075.2266 - mae: 163.4729 - val_loss: 45803.2188 - val_mae: 160.2695\n",
      "Epoch 92/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 53008.6289 - mae: 158.9912\n",
      "Epoch 92: val_loss improved from 45803.21875 to 43719.23828, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 50667.4258 - mae: 156.9005 - val_loss: 43719.2383 - val_mae: 154.0597\n",
      "Epoch 93/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 48692.5508 - mae: 150.7898\n",
      "Epoch 93: val_loss improved from 43719.23828 to 41827.71094, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 48347.7969 - mae: 150.5249 - val_loss: 41827.7109 - val_mae: 148.1406\n",
      "Epoch 94/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 46238.9609 - mae: 144.5179\n",
      "Epoch 94: val_loss improved from 41827.71094 to 40011.03516, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 46238.9609 - mae: 144.5179 - val_loss: 40011.0352 - val_mae: 142.5919\n",
      "Epoch 95/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 43395.4023 - mae: 138.4833\n",
      "Epoch 95: val_loss improved from 40011.03516 to 38470.89062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 44230.7617 - mae: 138.8242 - val_loss: 38470.8906 - val_mae: 137.1506\n",
      "Epoch 96/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 42742.5977 - mae: 134.0531\n",
      "Epoch 96: val_loss improved from 38470.89062 to 36826.19141, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 42343.1523 - mae: 133.4550 - val_loss: 36826.1914 - val_mae: 132.1567\n",
      "Epoch 97/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 40726.3711 - mae: 127.9523\n",
      "Epoch 97: val_loss improved from 36826.19141 to 35395.86328, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 40633.4766 - mae: 128.2191 - val_loss: 35395.8633 - val_mae: 127.1735\n",
      "Epoch 98/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 40761.2812 - mae: 124.0916\n",
      "Epoch 98: val_loss improved from 35395.86328 to 33991.94141, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 38988.6602 - mae: 123.2463 - val_loss: 33991.9414 - val_mae: 122.4758\n",
      "Epoch 99/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 38057.6562 - mae: 118.8872\n",
      "Epoch 99: val_loss improved from 33991.94141 to 32685.64258, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 37438.6094 - mae: 118.2346 - val_loss: 32685.6426 - val_mae: 117.6299\n",
      "Epoch 100/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 35972.6016 - mae: 113.3255\n",
      "Epoch 100: val_loss improved from 32685.64258 to 31429.23047, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 35972.6016 - mae: 113.3255 - val_loss: 31429.2305 - val_mae: 113.1194\n",
      "Epoch 101/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 30874.4004 - mae: 106.7377\n",
      "Epoch 101: val_loss improved from 31429.23047 to 30345.27734, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 34588.5977 - mae: 108.5873 - val_loss: 30345.2773 - val_mae: 108.7285\n",
      "Epoch 102/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 29835.8691 - mae: 102.9786\n",
      "Epoch 102: val_loss improved from 30345.27734 to 29190.99805, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 33276.5625 - mae: 104.0977 - val_loss: 29190.9980 - val_mae: 104.4672\n",
      "Epoch 103/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 28474.7637 - mae: 99.1344 \n",
      "Epoch 103: val_loss improved from 29190.99805 to 28187.11328, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 32040.3965 - mae: 100.1124 - val_loss: 28187.1133 - val_mae: 100.7039\n",
      "Epoch 104/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 32287.3789 - mae: 97.2574\n",
      "Epoch 104: val_loss improved from 28187.11328 to 27134.52148, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 30880.6484 - mae: 96.0132 - val_loss: 27134.5215 - val_mae: 96.8600\n",
      "Epoch 105/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 32499.4316 - mae: 94.3692\n",
      "Epoch 105: val_loss improved from 27134.52148 to 26213.63672, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 29739.1875 - mae: 92.6666 - val_loss: 26213.6367 - val_mae: 93.6417\n",
      "Epoch 106/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 27020.9941 - mae: 89.7698\n",
      "Epoch 106: val_loss improved from 26213.63672 to 25326.71484, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 28663.2402 - mae: 88.9232 - val_loss: 25326.7148 - val_mae: 90.1686\n",
      "Epoch 107/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 27905.2266 - mae: 85.8833\n",
      "Epoch 107: val_loss improved from 25326.71484 to 24465.25781, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 27660.2363 - mae: 85.6587 - val_loss: 24465.2578 - val_mae: 87.1634\n",
      "Epoch 108/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 27201.6699 - mae: 82.4127\n",
      "Epoch 108: val_loss improved from 24465.25781 to 23731.69922, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 26669.5879 - mae: 82.5021 - val_loss: 23731.6992 - val_mae: 84.3127\n",
      "Epoch 109/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 23037.5586 - mae: 78.5355\n",
      "Epoch 109: val_loss improved from 23731.69922 to 22951.52148, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 25721.1504 - mae: 79.6192 - val_loss: 22951.5215 - val_mae: 81.3145\n",
      "Epoch 110/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 24867.5938 - mae: 77.1573\n",
      "Epoch 110: val_loss improved from 22951.52148 to 22246.53516, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 24838.1582 - mae: 77.1753 - val_loss: 22246.5352 - val_mae: 78.8905\n",
      "Epoch 111/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 24037.6641 - mae: 74.1798\n",
      "Epoch 111: val_loss improved from 22246.53516 to 21568.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 24014.8379 - mae: 74.2341 - val_loss: 21568.2188 - val_mae: 76.2564\n",
      "Epoch 112/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 23263.5742 - mae: 71.6145\n",
      "Epoch 112: val_loss improved from 21568.21875 to 20876.70117, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 23219.3770 - mae: 71.5025 - val_loss: 20876.7012 - val_mae: 73.7005\n",
      "Epoch 113/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 24458.9688 - mae: 70.3329\n",
      "Epoch 113: val_loss improved from 20876.70117 to 20268.15234, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 22446.3750 - mae: 69.5114 - val_loss: 20268.1523 - val_mae: 71.5540\n",
      "Epoch 114/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 22064.0098 - mae: 67.4852\n",
      "Epoch 114: val_loss improved from 20268.15234 to 19655.07617, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 21697.7539 - mae: 67.1198 - val_loss: 19655.0762 - val_mae: 69.1685\n",
      "Epoch 115/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 20981.9355 - mae: 65.0467\n",
      "Epoch 115: val_loss improved from 19655.07617 to 19042.65820, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 20981.9355 - mae: 65.0467 - val_loss: 19042.6582 - val_mae: 67.1755\n",
      "Epoch 116/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 21101.8008 - mae: 63.5325\n",
      "Epoch 116: val_loss improved from 19042.65820 to 18451.07812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 20274.0664 - mae: 62.9665 - val_loss: 18451.0781 - val_mae: 65.1360\n",
      "Epoch 117/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 19859.7402 - mae: 61.4906\n",
      "Epoch 117: val_loss improved from 18451.07812 to 17911.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 19624.8906 - mae: 60.9899 - val_loss: 17911.6875 - val_mae: 63.0652\n",
      "Epoch 118/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 19619.9805 - mae: 59.1597\n",
      "Epoch 118: val_loss improved from 17911.68750 to 17305.73633, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 18902.7773 - mae: 59.0138 - val_loss: 17305.7363 - val_mae: 61.1780\n",
      "Epoch 119/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 20202.2598 - mae: 58.8573\n",
      "Epoch 119: val_loss improved from 17305.73633 to 16662.24023, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 18087.5801 - mae: 57.0900 - val_loss: 16662.2402 - val_mae: 59.2586\n",
      "Epoch 120/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 17418.4824 - mae: 55.6073\n",
      "Epoch 120: val_loss improved from 16662.24023 to 16008.64160, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 17080.9277 - mae: 55.1797 - val_loss: 16008.6416 - val_mae: 57.3070\n",
      "Epoch 121/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 16003.1748 - mae: 52.9966\n",
      "Epoch 121: val_loss improved from 16008.64160 to 15387.56641, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 15978.1660 - mae: 52.9881 - val_loss: 15387.5664 - val_mae: 55.4938\n",
      "Epoch 122/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 15566.8389 - mae: 51.3402\n",
      "Epoch 122: val_loss improved from 15387.56641 to 14865.01953, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 15160.2236 - mae: 50.9724 - val_loss: 14865.0195 - val_mae: 53.8160\n",
      "Epoch 123/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 14490.2549 - mae: 49.2570\n",
      "Epoch 123: val_loss improved from 14865.01953 to 14344.96875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 14419.4648 - mae: 49.4094 - val_loss: 14344.9688 - val_mae: 52.1494\n",
      "Epoch 124/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 14526.2451 - mae: 48.4371\n",
      "Epoch 124: val_loss improved from 14344.96875 to 13912.25684, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 13553.4092 - mae: 47.6055 - val_loss: 13912.2568 - val_mae: 50.5550\n",
      "Epoch 125/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 12830.7383 - mae: 45.7813\n",
      "Epoch 125: val_loss improved from 13912.25684 to 13455.95801, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 12807.3154 - mae: 45.7417 - val_loss: 13455.9580 - val_mae: 49.1237\n",
      "Epoch 126/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 12075.5303 - mae: 44.0423\n",
      "Epoch 126: val_loss improved from 13455.95801 to 13061.03516, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 12129.0381 - mae: 44.3611 - val_loss: 13061.0352 - val_mae: 47.6180\n",
      "Epoch 127/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 11989.7812 - mae: 43.1973\n",
      "Epoch 127: val_loss improved from 13061.03516 to 12617.25684, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 11428.0137 - mae: 42.9814 - val_loss: 12617.2568 - val_mae: 46.3289\n",
      "Epoch 128/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 11890.9531 - mae: 42.3894\n",
      "Epoch 128: val_loss improved from 12617.25684 to 12206.46777, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 10828.0645 - mae: 41.3098 - val_loss: 12206.4678 - val_mae: 44.9802\n",
      "Epoch 129/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 10511.8760 - mae: 40.1302\n",
      "Epoch 129: val_loss improved from 12206.46777 to 11837.47461, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 10396.2520 - mae: 40.2091 - val_loss: 11837.4746 - val_mae: 43.7238\n",
      "Epoch 130/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 10328.9980 - mae: 38.9568\n",
      "Epoch 130: val_loss improved from 11837.47461 to 11482.25293, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 9983.2549 - mae: 38.7396 - val_loss: 11482.2529 - val_mae: 42.5296\n",
      "Epoch 131/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 9789.3477 - mae: 37.6088\n",
      "Epoch 131: val_loss improved from 11482.25293 to 11134.86719, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 9618.5938 - mae: 37.5645 - val_loss: 11134.8672 - val_mae: 41.2815\n",
      "Epoch 132/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 9338.7471 - mae: 36.5418\n",
      "Epoch 132: val_loss improved from 11134.86719 to 10801.68652, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 9280.7197 - mae: 36.5768 - val_loss: 10801.6865 - val_mae: 40.2070\n",
      "Epoch 133/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 9290.6797 - mae: 36.1108\n",
      "Epoch 133: val_loss improved from 10801.68652 to 10407.46191, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 8948.4062 - mae: 35.7081 - val_loss: 10407.4619 - val_mae: 39.1232\n",
      "Epoch 134/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 8600.3125 - mae: 34.2513\n",
      "Epoch 134: val_loss improved from 10407.46191 to 10073.76758, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 8638.9951 - mae: 34.7541 - val_loss: 10073.7676 - val_mae: 38.0587\n",
      "Epoch 135/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 7025.8550 - mae: 33.4842\n",
      "Epoch 135: val_loss improved from 10073.76758 to 9674.99707, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 8330.5508 - mae: 33.7300 - val_loss: 9674.9971 - val_mae: 36.9973\n",
      "Epoch 136/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 8043.5786 - mae: 33.0721\n",
      "Epoch 136: val_loss improved from 9674.99707 to 9329.87012, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 8043.5786 - mae: 33.0721 - val_loss: 9329.8701 - val_mae: 35.9711\n",
      "Epoch 137/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 8001.6104 - mae: 32.2610\n",
      "Epoch 137: val_loss improved from 9329.87012 to 8973.94629, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 7763.8857 - mae: 31.9285 - val_loss: 8973.9463 - val_mae: 35.0075\n",
      "Epoch 138/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 6421.7490 - mae: 29.3909\n",
      "Epoch 138: val_loss improved from 8973.94629 to 8682.18359, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 7479.5981 - mae: 31.2268 - val_loss: 8682.1836 - val_mae: 34.0283\n",
      "Epoch 139/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 7219.5425 - mae: 30.5162\n",
      "Epoch 139: val_loss improved from 8682.18359 to 8329.97852, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 7205.8784 - mae: 30.4754 - val_loss: 8329.9785 - val_mae: 33.1543\n",
      "Epoch 140/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 7295.4507 - mae: 29.6989\n",
      "Epoch 140: val_loss improved from 8329.97852 to 8038.69727, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 6957.9839 - mae: 29.7403 - val_loss: 8038.6973 - val_mae: 32.3171\n",
      "Epoch 141/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 6692.4468 - mae: 29.1800\n",
      "Epoch 141: val_loss improved from 8038.69727 to 7753.78711, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 6692.4468 - mae: 29.1800 - val_loss: 7753.7871 - val_mae: 31.4788\n",
      "Epoch 142/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 6534.1392 - mae: 28.2791\n",
      "Epoch 142: val_loss improved from 7753.78711 to 7509.70459, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 6431.0879 - mae: 28.2558 - val_loss: 7509.7046 - val_mae: 30.5934\n",
      "Epoch 143/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 6317.7593 - mae: 27.8844\n",
      "Epoch 143: val_loss improved from 7509.70459 to 7266.49414, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 6194.5869 - mae: 27.6837 - val_loss: 7266.4941 - val_mae: 29.8920\n",
      "Epoch 144/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 4789.0420 - mae: 25.9379\n",
      "Epoch 144: val_loss improved from 7266.49414 to 7027.27246, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 5954.1519 - mae: 26.7890 - val_loss: 7027.2725 - val_mae: 29.0066\n",
      "Epoch 145/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 5699.3286 - mae: 26.4162\n",
      "Epoch 145: val_loss improved from 7027.27246 to 6855.41064, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 5671.7319 - mae: 26.7270 - val_loss: 6855.4106 - val_mae: 28.6743\n",
      "Epoch 146/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 5480.1909 - mae: 25.6829\n",
      "Epoch 146: val_loss improved from 6855.41064 to 6626.04248, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 5420.7837 - mae: 25.5876 - val_loss: 6626.0425 - val_mae: 27.8711\n",
      "Epoch 147/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 5496.6240 - mae: 25.4841\n",
      "Epoch 147: val_loss improved from 6626.04248 to 6401.85498, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 5229.8154 - mae: 25.0562 - val_loss: 6401.8550 - val_mae: 27.2048\n",
      "Epoch 148/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 4363.0269 - mae: 22.7963\n",
      "Epoch 148: val_loss improved from 6401.85498 to 6203.99854, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 5030.3281 - mae: 24.1246 - val_loss: 6203.9985 - val_mae: 26.5057\n",
      "Epoch 149/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 4932.7412 - mae: 23.4090\n",
      "Epoch 149: val_loss improved from 6203.99854 to 5992.06689, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 4840.7280 - mae: 23.5173 - val_loss: 5992.0669 - val_mae: 25.9933\n",
      "Epoch 150/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 2420.8398 - mae: 21.2106\n",
      "Epoch 150: val_loss improved from 5992.06689 to 5773.84717, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 4664.3223 - mae: 22.8708 - val_loss: 5773.8472 - val_mae: 25.2915\n",
      "Epoch 151/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 4521.6646 - mae: 22.5279\n",
      "Epoch 151: val_loss improved from 5773.84717 to 5540.72217, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 4472.3472 - mae: 22.4639 - val_loss: 5540.7222 - val_mae: 24.7557\n",
      "Epoch 152/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 4292.2637 - mae: 21.8202\n",
      "Epoch 152: val_loss improved from 5540.72217 to 5358.19629, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 4285.1001 - mae: 21.8179 - val_loss: 5358.1963 - val_mae: 24.1384\n",
      "Epoch 153/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 4147.9341 - mae: 21.2761\n",
      "Epoch 153: val_loss improved from 5358.19629 to 5139.24756, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 4105.0405 - mae: 21.2372 - val_loss: 5139.2476 - val_mae: 23.6737\n",
      "Epoch 154/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1997.9784 - mae: 19.1671\n",
      "Epoch 154: val_loss improved from 5139.24756 to 4915.49805, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 3923.6438 - mae: 20.5948 - val_loss: 4915.4980 - val_mae: 23.0586\n",
      "Epoch 155/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 3717.4578 - mae: 20.2634\n",
      "Epoch 155: val_loss improved from 4915.49805 to 4685.47314, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 67ms/step - loss: 3717.4578 - mae: 20.2634 - val_loss: 4685.4731 - val_mae: 22.6180\n",
      "Epoch 156/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 3508.9539 - mae: 19.5549\n",
      "Epoch 156: val_loss improved from 4685.47314 to 4464.55811, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 3446.6338 - mae: 19.4395 - val_loss: 4464.5581 - val_mae: 22.0417\n",
      "Epoch 157/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 2651.2612 - mae: 18.5105\n",
      "Epoch 157: val_loss improved from 4464.55811 to 4204.89160, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 3202.0002 - mae: 18.8830 - val_loss: 4204.8916 - val_mae: 21.5211\n",
      "Epoch 158/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 3013.7219 - mae: 18.2971\n",
      "Epoch 158: val_loss improved from 4204.89160 to 3993.79199, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 3022.9907 - mae: 18.3952 - val_loss: 3993.7920 - val_mae: 21.0362\n",
      "Epoch 159/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 3032.4011 - mae: 18.1246\n",
      "Epoch 159: val_loss improved from 3993.79199 to 3856.18018, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 2885.2700 - mae: 17.8559 - val_loss: 3856.1802 - val_mae: 20.6112\n",
      "Epoch 160/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 2849.1064 - mae: 17.6395\n",
      "Epoch 160: val_loss improved from 3856.18018 to 3713.20239, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 2763.5669 - mae: 17.4451 - val_loss: 3713.2024 - val_mae: 20.2246\n",
      "Epoch 161/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 2804.4287 - mae: 17.1785\n",
      "Epoch 161: val_loss improved from 3713.20239 to 3573.30566, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 2650.0791 - mae: 17.1080 - val_loss: 3573.3057 - val_mae: 19.9433\n",
      "Epoch 162/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 2713.1211 - mae: 17.0243\n",
      "Epoch 162: val_loss improved from 3573.30566 to 3454.17896, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 2540.7034 - mae: 16.7610 - val_loss: 3454.1790 - val_mae: 19.5407\n",
      "Epoch 163/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 2662.2070 - mae: 16.5818\n",
      "Epoch 163: val_loss improved from 3454.17896 to 3338.16260, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 2439.0752 - mae: 16.5962 - val_loss: 3338.1626 - val_mae: 19.2615\n",
      "Epoch 164/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 2380.1235 - mae: 16.2401\n",
      "Epoch 164: val_loss improved from 3338.16260 to 3195.69702, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 2331.5002 - mae: 16.2855 - val_loss: 3195.6970 - val_mae: 18.9227\n",
      "Epoch 165/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 2255.0527 - mae: 15.9372\n",
      "Epoch 165: val_loss improved from 3195.69702 to 3089.63940, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 2212.1902 - mae: 15.8797 - val_loss: 3089.6394 - val_mae: 18.6362\n",
      "Epoch 166/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 2129.4861 - mae: 15.6098\n",
      "Epoch 166: val_loss improved from 3089.63940 to 2980.66284, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 2125.6980 - mae: 15.5997 - val_loss: 2980.6628 - val_mae: 18.3681\n",
      "Epoch 167/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 2109.7002 - mae: 15.5672\n",
      "Epoch 167: val_loss improved from 2980.66284 to 2886.77563, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 2028.6619 - mae: 15.3301 - val_loss: 2886.7756 - val_mae: 18.0647\n",
      "Epoch 168/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1942.4420 - mae: 15.1720\n",
      "Epoch 168: val_loss improved from 2886.77563 to 2792.61157, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1939.4170 - mae: 15.1756 - val_loss: 2792.6116 - val_mae: 17.8059\n",
      "Epoch 169/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1818.2382 - mae: 14.6881\n",
      "Epoch 169: val_loss improved from 2792.61157 to 2694.77930, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1850.3257 - mae: 14.8169 - val_loss: 2694.7793 - val_mae: 17.4738\n",
      "Epoch 170/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1801.5841 - mae: 14.6222\n",
      "Epoch 170: val_loss improved from 2694.77930 to 2610.33765, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1767.1910 - mae: 14.5917 - val_loss: 2610.3376 - val_mae: 17.2810\n",
      "Epoch 171/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1692.6626 - mae: 14.1367\n",
      "Epoch 171: val_loss improved from 2610.33765 to 2535.68726, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 1687.0668 - mae: 14.3286 - val_loss: 2535.6873 - val_mae: 16.9992\n",
      "Epoch 172/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1661.6782 - mae: 14.3226\n",
      "Epoch 172: val_loss improved from 2535.68726 to 2453.46021, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1612.3947 - mae: 14.1632 - val_loss: 2453.4602 - val_mae: 16.7830\n",
      "Epoch 173/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1621.6158 - mae: 14.0480\n",
      "Epoch 173: val_loss improved from 2453.46021 to 2376.57251, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1536.1626 - mae: 13.8845 - val_loss: 2376.5725 - val_mae: 16.5479\n",
      "Epoch 174/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1527.2203 - mae: 13.7044\n",
      "Epoch 174: val_loss improved from 2376.57251 to 2311.98120, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 1462.9863 - mae: 13.6855 - val_loss: 2311.9812 - val_mae: 16.3271\n",
      "Epoch 175/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1411.5389 - mae: 13.4659\n",
      "Epoch 175: val_loss improved from 2311.98120 to 2244.48730, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 1397.8295 - mae: 13.4716 - val_loss: 2244.4873 - val_mae: 16.0814\n",
      "Epoch 176/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 858.3297 - mae: 12.5721\n",
      "Epoch 176: val_loss improved from 2244.48730 to 2178.09521, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1324.1149 - mae: 13.1061 - val_loss: 2178.0952 - val_mae: 15.8112\n",
      "Epoch 177/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1007.9546 - mae: 12.6456\n",
      "Epoch 177: val_loss improved from 2178.09521 to 2103.18921, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1257.6605 - mae: 12.9206 - val_loss: 2103.1892 - val_mae: 15.6867\n",
      "Epoch 178/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1266.9764 - mae: 12.8868\n",
      "Epoch 178: val_loss improved from 2103.18921 to 2041.08789, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1199.1781 - mae: 12.7628 - val_loss: 2041.0879 - val_mae: 15.4426\n",
      "Epoch 179/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1231.9395 - mae: 12.5824\n",
      "Epoch 179: val_loss improved from 2041.08789 to 1966.82507, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1139.5144 - mae: 12.4382 - val_loss: 1966.8251 - val_mae: 15.2607\n",
      "Epoch 180/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1113.3600 - mae: 12.3749\n",
      "Epoch 180: val_loss improved from 1966.82507 to 1901.18286, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 1083.5087 - mae: 12.2965 - val_loss: 1901.1829 - val_mae: 15.0671\n",
      "Epoch 181/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1028.7621 - mae: 12.0493\n",
      "Epoch 181: val_loss improved from 1901.18286 to 1845.60779, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1030.7892 - mae: 12.1178 - val_loss: 1845.6078 - val_mae: 14.8618\n",
      "Epoch 182/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1080.6689 - mae: 12.2536\n",
      "Epoch 182: val_loss improved from 1845.60779 to 1796.65466, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 984.2204 - mae: 11.9668 - val_loss: 1796.6547 - val_mae: 14.6420\n",
      "Epoch 183/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 982.3447 - mae: 11.8023\n",
      "Epoch 183: val_loss improved from 1796.65466 to 1744.22729, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 934.1108 - mae: 11.7045 - val_loss: 1744.2273 - val_mae: 14.4777\n",
      "Epoch 184/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 926.6615 - mae: 11.5553\n",
      "Epoch 184: val_loss improved from 1744.22729 to 1635.28320, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 890.2231 - mae: 11.5442 - val_loss: 1635.2832 - val_mae: 14.2741\n",
      "Epoch 185/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 887.0360 - mae: 11.5060 \n",
      "Epoch 185: val_loss improved from 1635.28320 to 1591.79333, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 849.7198 - mae: 11.4161 - val_loss: 1591.7933 - val_mae: 14.1351\n",
      "Epoch 186/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 859.7400 - mae: 11.3709\n",
      "Epoch 186: val_loss improved from 1591.79333 to 1552.96375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 807.2867 - mae: 11.2654 - val_loss: 1552.9637 - val_mae: 14.0283\n",
      "Epoch 187/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 804.8811 - mae: 11.1383\n",
      "Epoch 187: val_loss improved from 1552.96375 to 1473.85779, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 771.4866 - mae: 11.0585 - val_loss: 1473.8578 - val_mae: 13.8561\n",
      "Epoch 188/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 752.1263 - mae: 11.0250\n",
      "Epoch 188: val_loss improved from 1473.85779 to 1436.15247, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 731.9761 - mae: 10.9416 - val_loss: 1436.1525 - val_mae: 13.7224\n",
      "Epoch 189/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 770.7791 - mae: 10.7541\n",
      "Epoch 189: val_loss improved from 1436.15247 to 1372.75342, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 697.3590 - mae: 10.8210 - val_loss: 1372.7534 - val_mae: 13.6142\n",
      "Epoch 190/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 666.3725 - mae: 10.6052\n",
      "Epoch 190: val_loss improved from 1372.75342 to 1319.29602, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 666.3725 - mae: 10.6052 - val_loss: 1319.2960 - val_mae: 13.4668\n",
      "Epoch 191/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 644.5997 - mae: 10.4954\n",
      "Epoch 191: val_loss improved from 1319.29602 to 1256.12109, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 638.3906 - mae: 10.5387 - val_loss: 1256.1211 - val_mae: 13.3369\n",
      "Epoch 192/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 660.9761 - mae: 10.4778\n",
      "Epoch 192: val_loss improved from 1256.12109 to 1231.33850, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 602.9549 - mae: 10.4027 - val_loss: 1231.3385 - val_mae: 13.2086\n",
      "Epoch 193/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 583.9671 - mae: 10.3167\n",
      "Epoch 193: val_loss improved from 1231.33850 to 1180.84082, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 578.6278 - mae: 10.3056 - val_loss: 1180.8408 - val_mae: 13.1513\n",
      "Epoch 194/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 556.2745 - mae: 10.2114\n",
      "Epoch 194: val_loss improved from 1180.84082 to 1136.47070, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 551.0791 - mae: 10.1942 - val_loss: 1136.4707 - val_mae: 12.9596\n",
      "Epoch 195/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 471.8598 - mae: 9.9987 \n",
      "Epoch 195: val_loss improved from 1136.47070 to 1091.58496, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 525.2856 - mae: 10.1026 - val_loss: 1091.5850 - val_mae: 12.8742\n",
      "Epoch 196/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 501.6995 - mae: 9.8041\n",
      "Epoch 196: val_loss improved from 1091.58496 to 1061.09180, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 498.2982 - mae: 9.9087 - val_loss: 1061.0918 - val_mae: 12.7649\n",
      "Epoch 197/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 505.3423 - mae: 9.9169 \n",
      "Epoch 197: val_loss improved from 1061.09180 to 1020.45471, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 479.7841 - mae: 9.8655 - val_loss: 1020.4547 - val_mae: 12.6831\n",
      "Epoch 198/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 454.0785 - mae: 9.7141\n",
      "Epoch 198: val_loss improved from 1020.45471 to 982.62152, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 454.2155 - mae: 9.7505 - val_loss: 982.6215 - val_mae: 12.5092\n",
      "Epoch 199/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 444.8675 - mae: 9.6321\n",
      "Epoch 199: val_loss improved from 982.62152 to 951.63483, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 431.3219 - mae: 9.5933 - val_loss: 951.6348 - val_mae: 12.4309\n",
      "Epoch 200/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 443.6640 - mae: 9.6932\n",
      "Epoch 200: val_loss improved from 951.63483 to 933.16193, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 414.2992 - mae: 9.5710 - val_loss: 933.1619 - val_mae: 12.3051\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일: 모델학습을 위한 학습과정 설정단계\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "# print(\"\\n Accuracy: %.4f\" % model.evaluate(x_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f8f95d7460>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_feature)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(test_label, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.grid\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7adfe9f3775c954d01fc005199cfce03b4193bbc6ff21451e4d1eaf7785b4e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
