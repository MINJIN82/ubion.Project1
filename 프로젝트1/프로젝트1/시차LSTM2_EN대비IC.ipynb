{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"./xlsx/시차상관분석6Data.xlsx\",index_col=0)\n",
    "df = df.set_index(\"DateTime\")\n",
    "\n",
    "# 대비 계산\n",
    "df['대비_irs_1Y'] = df['1Y_Mid_irs'] - df['1Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_2Y'] = df['2Y_Mid_irs'] - df['2Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_3Y'] = df['3Y_Mid_irs'] - df['3Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_5Y'] = df['5Y_Mid_irs'] - df['5Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_10Y'] = df['10Y_Mid_irs'] - df['10Y_Mid_irs'].shift(1) \n",
    "\n",
    "df['대비_crs_1Y'] = df['1Y_Mid_crs'] - df['1Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_2Y'] = df['2Y_Mid_crs'] - df['2Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_3Y'] = df['3Y_Mid_crs'] - df['3Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_5Y'] = df['5Y_Mid_crs'] - df['5Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_10Y'] = df['10Y_Mid_crs'] - df['10Y_Mid_crs'].shift(1)\n",
    "\n",
    "df['대비_swapbasis_1Y'] = df['1Y_베이시스']-df['1Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_2Y'] = df['2Y_베이시스']-df['2Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_3Y'] = df['3Y_베이시스']-df['3Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_5Y'] = df['5Y_베이시스']-df['5Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_10Y'] = df['10Y_베이시스']-df['10Y_베이시스'].shift(1)\n",
    "\n",
    "df['대비_국고_1Y'] = df['국고1년']-df['국고1년'].shift(1)\n",
    "df['대비_국고_3Y'] = df['국고3년']-df['국고3년'].shift(1)\n",
    "df['대비_국고_5Y'] = df['국고5년']-df['국고5년'].shift(1)\n",
    "df['대비_국고_10Y'] = df['국고10년']-df['국고10년'].shift(1)\n",
    "\n",
    "df['대비_통안_1Y'] = df['통안364일']-df['통안364일'].shift(1)\n",
    "df['대비_통안_2Y'] = df['통안2년']-df['통안2년'].shift(1)\n",
    "\n",
    "df['대비_ndf'] = df['Mid_ndf']-df['Mid_ndf'].shift(1)\n",
    "df['스왑포인트_1M'] = df[\"M1_스왑포인트\"]/100 \n",
    "df['전일종가_ex'] = df['종가_ex'].shift(1)\n",
    "df['종가_NDF_차이'] = df['전일종가_ex'] - df['Mid_ndf']\n",
    "\n",
    "# 필요한 칼럼만 추출\n",
    "df_1 = df[['대비_irs_1Y', '대비_irs_2Y', '대비_irs_3Y', '대비_irs_5Y', '대비_irs_10Y',\n",
    "           '대비_crs_1Y', '대비_crs_2Y', '대비_crs_3Y', '대비_crs_5Y', '대비_crs_10Y', \n",
    "           '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y', '대비_swapbasis_10Y',\n",
    "           '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y', \n",
    "           '대비_통안_1Y', '대비_통안_2Y', '대비_ndf', '스왑포인트_1M', '전일종가_ex', \n",
    "           '종가_ex', '종가_NDF_차이' ]]   \n",
    "\n",
    "# 결측치 제거\n",
    "df_1 = df_1.dropna()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_24164\\1960586329.py:12: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  x.feature = x.columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_irs_1Y</th>\n",
       "      <th>대비_irs_2Y</th>\n",
       "      <th>대비_irs_3Y</th>\n",
       "      <th>대비_irs_5Y</th>\n",
       "      <th>대비_irs_10Y</th>\n",
       "      <th>대비_crs_1Y</th>\n",
       "      <th>대비_crs_2Y</th>\n",
       "      <th>대비_crs_3Y</th>\n",
       "      <th>대비_crs_5Y</th>\n",
       "      <th>대비_crs_10Y</th>\n",
       "      <th>대비_국고_1Y</th>\n",
       "      <th>대비_국고_3Y</th>\n",
       "      <th>대비_국고_5Y</th>\n",
       "      <th>대비_국고_10Y</th>\n",
       "      <th>대비_통안_1Y</th>\n",
       "      <th>대비_통안_2Y</th>\n",
       "      <th>스왑포인트_1M</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_NDF_차이</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-0.848159</td>\n",
       "      <td>-0.732099</td>\n",
       "      <td>-0.562745</td>\n",
       "      <td>-0.628439</td>\n",
       "      <td>-0.403644</td>\n",
       "      <td>-0.205698</td>\n",
       "      <td>-0.364180</td>\n",
       "      <td>-0.462791</td>\n",
       "      <td>-1.872418</td>\n",
       "      <td>-3.115253</td>\n",
       "      <td>-1.133777</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>-1.798842</td>\n",
       "      <td>-0.217667</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.056282</td>\n",
       "      <td>-1.367171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>0.559997</td>\n",
       "      <td>0.481892</td>\n",
       "      <td>0.370922</td>\n",
       "      <td>0.415773</td>\n",
       "      <td>0.202253</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>-0.602348</td>\n",
       "      <td>-0.690892</td>\n",
       "      <td>-0.939341</td>\n",
       "      <td>-0.833148</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>0.159979</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>0.123726</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>1.602437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.278366</td>\n",
       "      <td>-0.003704</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>0.206931</td>\n",
       "      <td>0.404219</td>\n",
       "      <td>0.401029</td>\n",
       "      <td>0.350324</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-1.639149</td>\n",
       "      <td>-1.663004</td>\n",
       "      <td>-0.568154</td>\n",
       "      <td>-0.001379</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>1.911215</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>0.117633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.481892</td>\n",
       "      <td>0.370922</td>\n",
       "      <td>0.415773</td>\n",
       "      <td>0.606184</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.006928</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-0.472802</td>\n",
       "      <td>-0.833148</td>\n",
       "      <td>-0.568154</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>-0.514104</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.108476</td>\n",
       "      <td>-0.224108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-09</th>\n",
       "      <td>1.404890</td>\n",
       "      <td>1.453085</td>\n",
       "      <td>1.117855</td>\n",
       "      <td>1.042301</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>1.816727</td>\n",
       "      <td>0.945744</td>\n",
       "      <td>0.905815</td>\n",
       "      <td>0.926814</td>\n",
       "      <td>0.826565</td>\n",
       "      <td>2.825583</td>\n",
       "      <td>0.966767</td>\n",
       "      <td>1.351270</td>\n",
       "      <td>1.284529</td>\n",
       "      <td>0.432293</td>\n",
       "      <td>0.747946</td>\n",
       "      <td>1.775350</td>\n",
       "      <td>-0.117475</td>\n",
       "      <td>-0.872236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-1.129790</td>\n",
       "      <td>-1.946090</td>\n",
       "      <td>-1.309678</td>\n",
       "      <td>-1.881494</td>\n",
       "      <td>-2.221333</td>\n",
       "      <td>-1.621396</td>\n",
       "      <td>-2.150439</td>\n",
       "      <td>-1.603295</td>\n",
       "      <td>-2.572226</td>\n",
       "      <td>-2.285397</td>\n",
       "      <td>-1.133777</td>\n",
       "      <td>-0.969524</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>-2.312737</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.625337</td>\n",
       "      <td>-0.896666</td>\n",
       "      <td>3.206786</td>\n",
       "      <td>0.860035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>-0.284897</td>\n",
       "      <td>-0.489301</td>\n",
       "      <td>-0.562745</td>\n",
       "      <td>-0.628439</td>\n",
       "      <td>-0.605609</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.721432</td>\n",
       "      <td>-0.690892</td>\n",
       "      <td>-0.472802</td>\n",
       "      <td>-0.418220</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>-0.771052</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>-0.987243</td>\n",
       "      <td>3.219385</td>\n",
       "      <td>0.753978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>-0.284897</td>\n",
       "      <td>-1.217695</td>\n",
       "      <td>-1.122944</td>\n",
       "      <td>-1.254966</td>\n",
       "      <td>-1.211506</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.245096</td>\n",
       "      <td>-0.234690</td>\n",
       "      <td>-0.239533</td>\n",
       "      <td>-0.210756</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-1.350390</td>\n",
       "      <td>-1.541894</td>\n",
       "      <td>0.215640</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>-0.851378</td>\n",
       "      <td>3.109596</td>\n",
       "      <td>-0.565848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.841628</td>\n",
       "      <td>0.967489</td>\n",
       "      <td>0.931121</td>\n",
       "      <td>1.042301</td>\n",
       "      <td>1.414046</td>\n",
       "      <td>0.805515</td>\n",
       "      <td>0.945744</td>\n",
       "      <td>0.905815</td>\n",
       "      <td>0.693544</td>\n",
       "      <td>0.411636</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>0.644052</td>\n",
       "      <td>0.810938</td>\n",
       "      <td>2.055371</td>\n",
       "      <td>0.215640</td>\n",
       "      <td>0.373414</td>\n",
       "      <td>-0.941955</td>\n",
       "      <td>3.212186</td>\n",
       "      <td>1.838120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-1.974684</td>\n",
       "      <td>-3.402879</td>\n",
       "      <td>-2.616811</td>\n",
       "      <td>-2.925706</td>\n",
       "      <td>-2.423299</td>\n",
       "      <td>-2.228124</td>\n",
       "      <td>-4.055782</td>\n",
       "      <td>-3.428103</td>\n",
       "      <td>-2.105687</td>\n",
       "      <td>-1.870468</td>\n",
       "      <td>-2.265023</td>\n",
       "      <td>-2.099027</td>\n",
       "      <td>-3.241553</td>\n",
       "      <td>-2.055789</td>\n",
       "      <td>-0.325994</td>\n",
       "      <td>-1.374400</td>\n",
       "      <td>-0.896666</td>\n",
       "      <td>2.902617</td>\n",
       "      <td>0.200122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2458 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_irs_1Y  대비_irs_2Y  대비_irs_3Y  대비_irs_5Y  대비_irs_10Y  대비_crs_1Y  \\\n",
       "DateTime                                                                        \n",
       "2012-08-03  -0.848159  -0.732099  -0.562745  -0.628439   -0.403644  -0.205698   \n",
       "2012-08-06   0.559997   0.481892   0.370922   0.415773    0.202253  -0.003456   \n",
       "2012-08-07   0.278366  -0.003704  -0.002545   0.206931    0.404219   0.401029   \n",
       "2012-08-08   0.278366   0.481892   0.370922   0.415773    0.606184   0.198787   \n",
       "2012-08-09   1.404890   1.453085   1.117855   1.042301    0.808150   1.816727   \n",
       "...               ...        ...        ...        ...         ...        ...   \n",
       "2022-07-25  -1.129790  -1.946090  -1.309678  -1.881494   -2.221333  -1.621396   \n",
       "2022-07-26  -0.284897  -0.489301  -0.562745  -0.628439   -0.605609   0.198787   \n",
       "2022-07-27  -0.284897  -1.217695  -1.122944  -1.254966   -1.211506   0.198787   \n",
       "2022-07-28   0.841628   0.967489   0.931121   1.042301    1.414046   0.805515   \n",
       "2022-07-29  -1.974684  -3.402879  -2.616811  -2.925706   -2.423299  -2.228124   \n",
       "\n",
       "            대비_crs_2Y  대비_crs_3Y  대비_crs_5Y  대비_crs_10Y  대비_국고_1Y  대비_국고_3Y  \\\n",
       "DateTime                                                                      \n",
       "2012-08-03  -0.364180  -0.462791  -1.872418   -3.115253 -1.133777 -0.324094   \n",
       "2012-08-06  -0.602348  -0.690892  -0.939341   -0.833148  0.563092  0.159979   \n",
       "2012-08-07   0.350324  -0.006589  -1.639149   -1.663004 -0.568154 -0.001379   \n",
       "2012-08-08  -0.006928  -0.006589  -0.472802   -0.833148 -0.568154 -0.324094   \n",
       "2012-08-09   0.945744   0.905815   0.926814    0.826565  2.825583  0.966767   \n",
       "...               ...        ...        ...         ...       ...       ...   \n",
       "2022-07-25  -2.150439  -1.603295  -2.572226   -2.285397 -1.133777 -0.969524   \n",
       "2022-07-26  -0.721432  -0.690892  -0.472802   -0.418220  0.563092 -0.485451   \n",
       "2022-07-27  -0.245096  -0.234690  -0.239533   -0.210756 -0.002531 -0.485451   \n",
       "2022-07-28   0.945744   0.905815   0.693544    0.411636  0.563092  0.644052   \n",
       "2022-07-29  -4.055782  -3.428103  -2.105687   -1.870468 -2.265023 -2.099027   \n",
       "\n",
       "            대비_국고_5Y  대비_국고_10Y  대비_통안_1Y  대비_통안_2Y  스왑포인트_1M   전일종가_ex  \\\n",
       "DateTime                                                                  \n",
       "2012-08-03 -1.890723  -1.798842 -0.217667 -0.125961  1.820638 -0.056282   \n",
       "2012-08-06  0.000440  -0.000209  0.107313  0.123726  1.820638 -0.000487   \n",
       "2012-08-07  0.000440  -0.000209 -0.109340 -0.125961  1.911215 -0.104877   \n",
       "2012-08-08 -0.539892  -0.514104 -0.109340 -0.125961  1.820638 -0.108476   \n",
       "2012-08-09  1.351270   1.284529  0.432293  0.747946  1.775350 -0.117475   \n",
       "...              ...        ...       ...       ...       ...       ...   \n",
       "2022-07-25 -1.890723  -2.312737 -0.109340 -0.625337 -0.896666  3.206786   \n",
       "2022-07-26 -0.539892  -0.771052  0.107313 -0.001117 -0.987243  3.219385   \n",
       "2022-07-27 -1.350390  -1.541894  0.215640 -0.125961 -0.851378  3.109596   \n",
       "2022-07-28  0.810938   2.055371  0.215640  0.373414 -0.941955  3.212186   \n",
       "2022-07-29 -3.241553  -2.055789 -0.325994 -1.374400 -0.896666  2.902617   \n",
       "\n",
       "            종가_NDF_차이  \n",
       "DateTime               \n",
       "2012-08-03  -1.367171  \n",
       "2012-08-06   1.602437  \n",
       "2012-08-07   0.117633  \n",
       "2012-08-08  -0.224108  \n",
       "2012-08-09  -0.872236  \n",
       "...               ...  \n",
       "2022-07-25   0.860035  \n",
       "2022-07-26   0.753978  \n",
       "2022-07-27  -0.565848  \n",
       "2022-07-28   1.838120  \n",
       "2022-07-29   0.200122  \n",
       "\n",
       "[2458 rows x 19 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 쓸 칼럼만 남기고 feature, target 분리해 각각 x,y 에 저장\n",
    "x = df_1[[ '대비_irs_1Y', '대비_irs_2Y', '대비_irs_3Y', '대비_irs_5Y', '대비_irs_10Y',\n",
    "           '대비_crs_1Y', '대비_crs_2Y', '대비_crs_3Y', '대비_crs_5Y', '대비_crs_10Y', \n",
    "           '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y', \n",
    "           '대비_통안_1Y', '대비_통안_2Y', '스왑포인트_1M', '전일종가_ex', '종가_NDF_차이']]\n",
    "y = df_1[['종가_ex']]\n",
    "\n",
    "# 이건 이렇게 해야 밑에 코드 8번째 줄 columns에 들어갈 수 있다고 하네요!\n",
    "x.feature = x.columns \n",
    "x.feature\n",
    "\n",
    "# scaling 진행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor     Feature\n",
      "0     9.960690   대비_irs_1Y\n",
      "1    19.497712   대비_irs_2Y\n",
      "2     2.661563   대비_irs_3Y\n",
      "3     6.471270   대비_irs_5Y\n",
      "4     5.878681  대비_irs_10Y\n",
      "5     2.665512   대비_crs_1Y\n",
      "6     5.271707   대비_crs_2Y\n",
      "7     3.831097   대비_crs_3Y\n",
      "8     6.696835   대비_crs_5Y\n",
      "9     3.715722  대비_crs_10Y\n",
      "10    1.925701    대비_국고_1Y\n",
      "11    1.254068    대비_국고_3Y\n",
      "12    6.296340    대비_국고_5Y\n",
      "13    5.056358   대비_국고_10Y\n",
      "14    1.025199    대비_통안_1Y\n",
      "15    1.115797    대비_통안_2Y\n",
      "16    1.247545    스왑포인트_1M\n",
      "17    1.192971     전일종가_ex\n",
      "18    1.162083   종가_NDF_차이\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.452e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:17:06</td>     <th>  Log-Likelihood:    </th> <td> -6904.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.385e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2438</td>      <th>  BIC:               </th> <td>1.397e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.081</td> <td>  1.4e+04</td> <td> 0.000</td> <td> 1134.736</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.0303</td> <td>    0.257</td> <td>    0.118</td> <td> 0.906</td> <td>   -0.473</td> <td>    0.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_2Y</th>  <td>    0.4393</td> <td>    0.359</td> <td>    1.223</td> <td> 0.221</td> <td>   -0.265</td> <td>    1.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_3Y</th>  <td>    0.0617</td> <td>    0.133</td> <td>    0.465</td> <td> 0.642</td> <td>   -0.199</td> <td>    0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_5Y</th>  <td>   -0.0228</td> <td>    0.207</td> <td>   -0.110</td> <td> 0.912</td> <td>   -0.429</td> <td>    0.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.5283</td> <td>    0.197</td> <td>   -2.679</td> <td> 0.007</td> <td>   -0.915</td> <td>   -0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -0.6870</td> <td>    0.133</td> <td>   -5.174</td> <td> 0.000</td> <td>   -0.947</td> <td>   -0.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_2Y</th>  <td>    0.0520</td> <td>    0.187</td> <td>    0.279</td> <td> 0.781</td> <td>   -0.314</td> <td>    0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.2963</td> <td>    0.159</td> <td>   -1.862</td> <td> 0.063</td> <td>   -0.608</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_5Y</th>  <td>   -0.3179</td> <td>    0.210</td> <td>   -1.511</td> <td> 0.131</td> <td>   -0.731</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_10Y</th> <td>    0.1187</td> <td>    0.157</td> <td>    0.757</td> <td> 0.449</td> <td>   -0.189</td> <td>    0.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>   <td>   -0.0140</td> <td>    0.113</td> <td>   -0.124</td> <td> 0.901</td> <td>   -0.235</td> <td>    0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.1733</td> <td>    0.091</td> <td>   -1.903</td> <td> 0.057</td> <td>   -0.352</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.2574</td> <td>    0.204</td> <td>    1.261</td> <td> 0.207</td> <td>   -0.143</td> <td>    0.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>  <td>   -0.0070</td> <td>    0.183</td> <td>   -0.038</td> <td> 0.970</td> <td>   -0.366</td> <td>    0.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_1Y</th>   <td>   -0.0047</td> <td>    0.082</td> <td>   -0.057</td> <td> 0.954</td> <td>   -0.166</td> <td>    0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>   <td>    0.0295</td> <td>    0.086</td> <td>    0.343</td> <td> 0.732</td> <td>   -0.139</td> <td>    0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -1.1733</td> <td>    0.091</td> <td>  -12.916</td> <td> 0.000</td> <td>   -1.351</td> <td>   -0.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.7489</td> <td>    0.089</td> <td>  627.597</td> <td> 0.000</td> <td>   55.575</td> <td>   55.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>  <td>   -3.9505</td> <td>    0.088</td> <td>  -45.060</td> <td> 0.000</td> <td>   -4.122</td> <td>   -3.779</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>208.763</td> <th>  Durbin-Watson:     </th> <td>   2.115</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 893.947</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.306</td>  <th>  Prob(JB):          </th> <td>7.62e-195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.891</td>  <th>  Cond. No.          </th> <td>    13.2</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 2.452e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        21:17:06   Log-Likelihood:                -6904.9\n",
       "No. Observations:                2458   AIC:                         1.385e+04\n",
       "Df Residuals:                    2438   BIC:                         1.397e+04\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.081    1.4e+04      0.000    1134.736    1135.055\n",
       "대비_irs_1Y      0.0303      0.257      0.118      0.906      -0.473       0.534\n",
       "대비_irs_2Y      0.4393      0.359      1.223      0.221      -0.265       1.143\n",
       "대비_irs_3Y      0.0617      0.133      0.465      0.642      -0.199       0.322\n",
       "대비_irs_5Y     -0.0228      0.207     -0.110      0.912      -0.429       0.383\n",
       "대비_irs_10Y    -0.5283      0.197     -2.679      0.007      -0.915      -0.142\n",
       "대비_crs_1Y     -0.6870      0.133     -5.174      0.000      -0.947      -0.427\n",
       "대비_crs_2Y      0.0520      0.187      0.279      0.781      -0.314       0.418\n",
       "대비_crs_3Y     -0.2963      0.159     -1.862      0.063      -0.608       0.016\n",
       "대비_crs_5Y     -0.3179      0.210     -1.511      0.131      -0.731       0.095\n",
       "대비_crs_10Y     0.1187      0.157      0.757      0.449      -0.189       0.426\n",
       "대비_국고_1Y      -0.0140      0.113     -0.124      0.901      -0.235       0.207\n",
       "대비_국고_3Y      -0.1733      0.091     -1.903      0.057      -0.352       0.005\n",
       "대비_국고_5Y       0.2574      0.204      1.261      0.207      -0.143       0.658\n",
       "대비_국고_10Y     -0.0070      0.183     -0.038      0.970      -0.366       0.352\n",
       "대비_통안_1Y      -0.0047      0.082     -0.057      0.954      -0.166       0.157\n",
       "대비_통안_2Y       0.0295      0.086      0.343      0.732      -0.139       0.198\n",
       "스왑포인트_1M      -1.1733      0.091    -12.916      0.000      -1.351      -0.995\n",
       "전일종가_ex       55.7489      0.089    627.597      0.000      55.575      55.923\n",
       "종가_NDF_차이     -3.9505      0.088    -45.060      0.000      -4.122      -3.779\n",
       "==============================================================================\n",
       "Omnibus:                      208.763   Durbin-Watson:                   2.115\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              893.947\n",
       "Skew:                           0.306   Prob(JB):                    7.62e-195\n",
       "Kurtosis:                       5.891   Cond. No.                         13.2\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor     Feature\n",
      "0     3.931532   대비_irs_1Y\n",
      "1     2.551145   대비_irs_3Y\n",
      "2     5.804756   대비_irs_5Y\n",
      "3     5.223022  대비_irs_10Y\n",
      "4     2.662664   대비_crs_1Y\n",
      "5     5.260265   대비_crs_2Y\n",
      "6     3.828657   대비_crs_3Y\n",
      "7     6.691565   대비_crs_5Y\n",
      "8     3.715552  대비_crs_10Y\n",
      "9     1.914954    대비_국고_1Y\n",
      "10    1.248022    대비_국고_3Y\n",
      "11    6.209873    대비_국고_5Y\n",
      "12    5.042455   대비_국고_10Y\n",
      "13    1.024839    대비_통안_1Y\n",
      "14    1.115108    대비_통안_2Y\n",
      "15    1.247525    스왑포인트_1M\n",
      "16    1.192279     전일종가_ex\n",
      "17    1.161921   종가_NDF_차이\n"
     ]
    }
   ],
   "source": [
    "x_scaled.drop(['대비_irs_2Y'], axis=1, inplace=True)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.588e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:17:07</td>     <th>  Log-Likelihood:    </th> <td> -6905.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.385e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2439</td>      <th>  BIC:               </th> <td>1.396e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    18</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.081</td> <td>  1.4e+04</td> <td> 0.000</td> <td> 1134.736</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.2746</td> <td>    0.161</td> <td>    1.703</td> <td> 0.089</td> <td>   -0.042</td> <td>    0.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_3Y</th>  <td>    0.0947</td> <td>    0.130</td> <td>    0.729</td> <td> 0.466</td> <td>   -0.160</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_5Y</th>  <td>    0.0584</td> <td>    0.196</td> <td>    0.298</td> <td> 0.766</td> <td>   -0.326</td> <td>    0.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.4477</td> <td>    0.186</td> <td>   -2.409</td> <td> 0.016</td> <td>   -0.812</td> <td>   -0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -0.6923</td> <td>    0.133</td> <td>   -5.216</td> <td> 0.000</td> <td>   -0.953</td> <td>   -0.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_2Y</th>  <td>    0.0627</td> <td>    0.187</td> <td>    0.336</td> <td> 0.737</td> <td>   -0.303</td> <td>    0.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.3012</td> <td>    0.159</td> <td>   -1.893</td> <td> 0.058</td> <td>   -0.613</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_5Y</th>  <td>   -0.3107</td> <td>    0.210</td> <td>   -1.477</td> <td> 0.140</td> <td>   -0.723</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_10Y</th> <td>    0.1200</td> <td>    0.157</td> <td>    0.765</td> <td> 0.444</td> <td>   -0.187</td> <td>    0.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>   <td>   -0.0037</td> <td>    0.113</td> <td>   -0.033</td> <td> 0.974</td> <td>   -0.224</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.1655</td> <td>    0.091</td> <td>   -1.822</td> <td> 0.069</td> <td>   -0.344</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.2866</td> <td>    0.203</td> <td>    1.414</td> <td> 0.157</td> <td>   -0.111</td> <td>    0.684</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>  <td>   -0.0187</td> <td>    0.183</td> <td>   -0.103</td> <td> 0.918</td> <td>   -0.377</td> <td>    0.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_1Y</th>   <td>   -0.0066</td> <td>    0.082</td> <td>   -0.080</td> <td> 0.936</td> <td>   -0.168</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>   <td>    0.0321</td> <td>    0.086</td> <td>    0.374</td> <td> 0.709</td> <td>   -0.136</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -1.1728</td> <td>    0.091</td> <td>  -12.910</td> <td> 0.000</td> <td>   -1.351</td> <td>   -0.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.7463</td> <td>    0.089</td> <td>  627.686</td> <td> 0.000</td> <td>   55.572</td> <td>   55.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>  <td>   -3.9518</td> <td>    0.088</td> <td>  -45.073</td> <td> 0.000</td> <td>   -4.124</td> <td>   -3.780</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>211.657</td> <th>  Durbin-Watson:     </th> <td>   2.115</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 912.031</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.311</td>  <th>  Prob(JB):          </th> <td>9.01e-199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.919</td>  <th>  Cond. No.          </th> <td>    7.56</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 2.588e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        21:17:07   Log-Likelihood:                -6905.6\n",
       "No. Observations:                2458   AIC:                         1.385e+04\n",
       "Df Residuals:                    2439   BIC:                         1.396e+04\n",
       "Df Model:                          18                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.081    1.4e+04      0.000    1134.736    1135.055\n",
       "대비_irs_1Y      0.2746      0.161      1.703      0.089      -0.042       0.591\n",
       "대비_irs_3Y      0.0947      0.130      0.729      0.466      -0.160       0.349\n",
       "대비_irs_5Y      0.0584      0.196      0.298      0.766      -0.326       0.443\n",
       "대비_irs_10Y    -0.4477      0.186     -2.409      0.016      -0.812      -0.083\n",
       "대비_crs_1Y     -0.6923      0.133     -5.216      0.000      -0.953      -0.432\n",
       "대비_crs_2Y      0.0627      0.187      0.336      0.737      -0.303       0.428\n",
       "대비_crs_3Y     -0.3012      0.159     -1.893      0.058      -0.613       0.011\n",
       "대비_crs_5Y     -0.3107      0.210     -1.477      0.140      -0.723       0.102\n",
       "대비_crs_10Y     0.1200      0.157      0.765      0.444      -0.187       0.427\n",
       "대비_국고_1Y      -0.0037      0.113     -0.033      0.974      -0.224       0.217\n",
       "대비_국고_3Y      -0.1655      0.091     -1.822      0.069      -0.344       0.013\n",
       "대비_국고_5Y       0.2866      0.203      1.414      0.157      -0.111       0.684\n",
       "대비_국고_10Y     -0.0187      0.183     -0.103      0.918      -0.377       0.339\n",
       "대비_통안_1Y      -0.0066      0.082     -0.080      0.936      -0.168       0.155\n",
       "대비_통안_2Y       0.0321      0.086      0.374      0.709      -0.136       0.201\n",
       "스왑포인트_1M      -1.1728      0.091    -12.910      0.000      -1.351      -0.995\n",
       "전일종가_ex       55.7463      0.089    627.686      0.000      55.572      55.920\n",
       "종가_NDF_차이     -3.9518      0.088    -45.073      0.000      -4.124      -3.780\n",
       "==============================================================================\n",
       "Omnibus:                      211.657   Durbin-Watson:                   2.115\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              912.031\n",
       "Skew:                           0.311   Prob(JB):                    9.01e-199\n",
       "Kurtosis:                       5.919   Cond. No.                         7.56\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor     Feature\n",
      "0     3.844748   대비_irs_1Y\n",
      "1     2.551052   대비_irs_3Y\n",
      "2     5.803866   대비_irs_5Y\n",
      "3     5.108473  대비_irs_10Y\n",
      "4     2.662361   대비_crs_1Y\n",
      "5     5.258649   대비_crs_2Y\n",
      "6     3.827684   대비_crs_3Y\n",
      "7     6.691009   대비_crs_5Y\n",
      "8     3.714625  대비_crs_10Y\n",
      "9     1.901653    대비_국고_1Y\n",
      "10    1.247796    대비_국고_3Y\n",
      "11    2.697168    대비_국고_5Y\n",
      "12    1.114999    대비_통안_2Y\n",
      "13    1.247511    스왑포인트_1M\n",
      "14    1.192176     전일종가_ex\n",
      "15    1.160739   종가_NDF_차이\n"
     ]
    }
   ],
   "source": [
    "x_scaled.drop(['대비_국고_10Y', '대비_통안_1Y'], axis=1, inplace=True)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.914e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:17:09</td>     <th>  Log-Likelihood:    </th> <td> -6905.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.385e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2441</td>      <th>  BIC:               </th> <td>1.394e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    16</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.081</td> <td>  1.4e+04</td> <td> 0.000</td> <td> 1134.736</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.2769</td> <td>    0.159</td> <td>    1.737</td> <td> 0.083</td> <td>   -0.036</td> <td>    0.589</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_3Y</th>  <td>    0.0948</td> <td>    0.130</td> <td>    0.730</td> <td> 0.465</td> <td>   -0.160</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_5Y</th>  <td>    0.0581</td> <td>    0.196</td> <td>    0.297</td> <td> 0.767</td> <td>   -0.326</td> <td>    0.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.4504</td> <td>    0.184</td> <td>   -2.451</td> <td> 0.014</td> <td>   -0.811</td> <td>   -0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -0.6922</td> <td>    0.133</td> <td>   -5.218</td> <td> 0.000</td> <td>   -0.952</td> <td>   -0.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_2Y</th>  <td>    0.0630</td> <td>    0.186</td> <td>    0.338</td> <td> 0.736</td> <td>   -0.303</td> <td>    0.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.3015</td> <td>    0.159</td> <td>   -1.895</td> <td> 0.058</td> <td>   -0.613</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_5Y</th>  <td>   -0.3105</td> <td>    0.210</td> <td>   -1.476</td> <td> 0.140</td> <td>   -0.723</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_10Y</th> <td>    0.1197</td> <td>    0.157</td> <td>    0.764</td> <td> 0.445</td> <td>   -0.188</td> <td>    0.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>   <td>   -0.0048</td> <td>    0.112</td> <td>   -0.042</td> <td> 0.966</td> <td>   -0.225</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.1655</td> <td>    0.091</td> <td>   -1.822</td> <td> 0.069</td> <td>   -0.344</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.2706</td> <td>    0.134</td> <td>    2.027</td> <td> 0.043</td> <td>    0.009</td> <td>    0.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>   <td>    0.0320</td> <td>    0.086</td> <td>    0.373</td> <td> 0.709</td> <td>   -0.136</td> <td>    0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -1.1728</td> <td>    0.091</td> <td>  -12.916</td> <td> 0.000</td> <td>   -1.351</td> <td>   -0.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.7462</td> <td>    0.089</td> <td>  627.967</td> <td> 0.000</td> <td>   55.572</td> <td>   55.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>  <td>   -3.9520</td> <td>    0.088</td> <td>  -45.118</td> <td> 0.000</td> <td>   -4.124</td> <td>   -3.780</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>211.784</td> <th>  Durbin-Watson:     </th> <td>   2.115</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 913.230</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.311</td>  <th>  Prob(JB):          </th> <td>4.95e-199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.921</td>  <th>  Cond. No.          </th> <td>    7.09</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 2.914e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        21:17:09   Log-Likelihood:                -6905.6\n",
       "No. Observations:                2458   AIC:                         1.385e+04\n",
       "Df Residuals:                    2441   BIC:                         1.394e+04\n",
       "Df Model:                          16                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.081    1.4e+04      0.000    1134.736    1135.055\n",
       "대비_irs_1Y      0.2769      0.159      1.737      0.083      -0.036       0.589\n",
       "대비_irs_3Y      0.0948      0.130      0.730      0.465      -0.160       0.349\n",
       "대비_irs_5Y      0.0581      0.196      0.297      0.767      -0.326       0.442\n",
       "대비_irs_10Y    -0.4504      0.184     -2.451      0.014      -0.811      -0.090\n",
       "대비_crs_1Y     -0.6922      0.133     -5.218      0.000      -0.952      -0.432\n",
       "대비_crs_2Y      0.0630      0.186      0.338      0.736      -0.303       0.429\n",
       "대비_crs_3Y     -0.3015      0.159     -1.895      0.058      -0.613       0.010\n",
       "대비_crs_5Y     -0.3105      0.210     -1.476      0.140      -0.723       0.102\n",
       "대비_crs_10Y     0.1197      0.157      0.764      0.445      -0.188       0.427\n",
       "대비_국고_1Y      -0.0048      0.112     -0.042      0.966      -0.225       0.215\n",
       "대비_국고_3Y      -0.1655      0.091     -1.822      0.069      -0.344       0.013\n",
       "대비_국고_5Y       0.2706      0.134      2.027      0.043       0.009       0.532\n",
       "대비_통안_2Y       0.0320      0.086      0.373      0.709      -0.136       0.200\n",
       "스왑포인트_1M      -1.1728      0.091    -12.916      0.000      -1.351      -0.995\n",
       "전일종가_ex       55.7462      0.089    627.967      0.000      55.572      55.920\n",
       "종가_NDF_차이     -3.9520      0.088    -45.118      0.000      -4.124      -3.780\n",
       "==============================================================================\n",
       "Omnibus:                      211.784   Durbin-Watson:                   2.115\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              913.230\n",
       "Skew:                           0.311   Prob(JB):                    4.95e-199\n",
       "Kurtosis:                       5.921   Cond. No.                         7.09\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.333e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:17:10</td>     <th>  Log-Likelihood:    </th> <td> -6905.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.384e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2443</td>      <th>  BIC:               </th> <td>1.393e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    14</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.081</td> <td>  1.4e+04</td> <td> 0.000</td> <td> 1134.736</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.2778</td> <td>    0.158</td> <td>    1.762</td> <td> 0.078</td> <td>   -0.031</td> <td>    0.587</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_3Y</th>  <td>    0.0953</td> <td>    0.130</td> <td>    0.735</td> <td> 0.462</td> <td>   -0.159</td> <td>    0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_5Y</th>  <td>    0.0573</td> <td>    0.196</td> <td>    0.293</td> <td> 0.770</td> <td>   -0.327</td> <td>    0.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.4511</td> <td>    0.183</td> <td>   -2.471</td> <td> 0.014</td> <td>   -0.809</td> <td>   -0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -0.6717</td> <td>    0.118</td> <td>   -5.704</td> <td> 0.000</td> <td>   -0.903</td> <td>   -0.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.2863</td> <td>    0.152</td> <td>   -1.879</td> <td> 0.060</td> <td>   -0.585</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_5Y</th>  <td>   -0.2848</td> <td>    0.196</td> <td>   -1.454</td> <td> 0.146</td> <td>   -0.669</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_10Y</th> <td>    0.1204</td> <td>    0.157</td> <td>    0.769</td> <td> 0.442</td> <td>   -0.186</td> <td>    0.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.1658</td> <td>    0.090</td> <td>   -1.841</td> <td> 0.066</td> <td>   -0.342</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.2688</td> <td>    0.117</td> <td>    2.304</td> <td> 0.021</td> <td>    0.040</td> <td>    0.498</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>   <td>    0.0316</td> <td>    0.085</td> <td>    0.371</td> <td> 0.711</td> <td>   -0.135</td> <td>    0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -1.1724</td> <td>    0.091</td> <td>  -12.917</td> <td> 0.000</td> <td>   -1.350</td> <td>   -0.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.7461</td> <td>    0.088</td> <td>  630.082</td> <td> 0.000</td> <td>   55.573</td> <td>   55.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>  <td>   -3.9509</td> <td>    0.087</td> <td>  -45.162</td> <td> 0.000</td> <td>   -4.122</td> <td>   -3.779</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>211.349</td> <th>  Durbin-Watson:     </th> <td>   2.115</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 910.694</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.310</td>  <th>  Prob(JB):          </th> <td>1.76e-198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.917</td>  <th>  Cond. No.          </th> <td>    6.20</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 3.333e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        21:17:10   Log-Likelihood:                -6905.7\n",
       "No. Observations:                2458   AIC:                         1.384e+04\n",
       "Df Residuals:                    2443   BIC:                         1.393e+04\n",
       "Df Model:                          14                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.081    1.4e+04      0.000    1134.736    1135.055\n",
       "대비_irs_1Y      0.2778      0.158      1.762      0.078      -0.031       0.587\n",
       "대비_irs_3Y      0.0953      0.130      0.735      0.462      -0.159       0.350\n",
       "대비_irs_5Y      0.0573      0.196      0.293      0.770      -0.327       0.441\n",
       "대비_irs_10Y    -0.4511      0.183     -2.471      0.014      -0.809      -0.093\n",
       "대비_crs_1Y     -0.6717      0.118     -5.704      0.000      -0.903      -0.441\n",
       "대비_crs_3Y     -0.2863      0.152     -1.879      0.060      -0.585       0.012\n",
       "대비_crs_5Y     -0.2848      0.196     -1.454      0.146      -0.669       0.099\n",
       "대비_crs_10Y     0.1204      0.157      0.769      0.442      -0.186       0.427\n",
       "대비_국고_3Y      -0.1658      0.090     -1.841      0.066      -0.342       0.011\n",
       "대비_국고_5Y       0.2688      0.117      2.304      0.021       0.040       0.498\n",
       "대비_통안_2Y       0.0316      0.085      0.371      0.711      -0.135       0.198\n",
       "스왑포인트_1M      -1.1724      0.091    -12.917      0.000      -1.350      -0.994\n",
       "전일종가_ex       55.7461      0.088    630.082      0.000      55.573      55.920\n",
       "종가_NDF_차이     -3.9509      0.087    -45.162      0.000      -4.122      -3.779\n",
       "==============================================================================\n",
       "Omnibus:                      211.349   Durbin-Watson:                   2.115\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              910.694\n",
       "Skew:                           0.310   Prob(JB):                    1.76e-198\n",
       "Kurtosis:                       5.917   Cond. No.                         6.20\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_국고_1Y',\t'대비_crs_2Y'\t], axis=1, inplace=True)\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.891e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:17:11</td>     <th>  Log-Likelihood:    </th> <td> -6905.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.384e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2445</td>      <th>  BIC:               </th> <td>1.391e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.081</td> <td>  1.4e+04</td> <td> 0.000</td> <td> 1134.736</td> <td> 1135.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.2998</td> <td>    0.143</td> <td>    2.093</td> <td> 0.036</td> <td>    0.019</td> <td>    0.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_3Y</th>  <td>    0.1017</td> <td>    0.128</td> <td>    0.793</td> <td> 0.428</td> <td>   -0.150</td> <td>    0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.4269</td> <td>    0.157</td> <td>   -2.727</td> <td> 0.006</td> <td>   -0.734</td> <td>   -0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -0.6728</td> <td>    0.118</td> <td>   -5.721</td> <td> 0.000</td> <td>   -0.903</td> <td>   -0.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.2868</td> <td>    0.152</td> <td>   -1.884</td> <td> 0.060</td> <td>   -0.585</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_5Y</th>  <td>   -0.2828</td> <td>    0.196</td> <td>   -1.445</td> <td> 0.149</td> <td>   -0.667</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_10Y</th> <td>    0.1212</td> <td>    0.156</td> <td>    0.775</td> <td> 0.439</td> <td>   -0.186</td> <td>    0.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.1638</td> <td>    0.090</td> <td>   -1.821</td> <td> 0.069</td> <td>   -0.340</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.2837</td> <td>    0.112</td> <td>    2.525</td> <td> 0.012</td> <td>    0.063</td> <td>    0.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -1.1722</td> <td>    0.091</td> <td>  -12.921</td> <td> 0.000</td> <td>   -1.350</td> <td>   -0.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.7459</td> <td>    0.088</td> <td>  630.402</td> <td> 0.000</td> <td>   55.573</td> <td>   55.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>  <td>   -3.9509</td> <td>    0.087</td> <td>  -45.200</td> <td> 0.000</td> <td>   -4.122</td> <td>   -3.779</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>211.135</td> <th>  Durbin-Watson:     </th> <td>   2.115</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 911.718</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.309</td>  <th>  Prob(JB):          </th> <td>1.05e-198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.919</td>  <th>  Cond. No.          </th> <td>    5.76</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 3.891e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        21:17:11   Log-Likelihood:                -6905.8\n",
       "No. Observations:                2458   AIC:                         1.384e+04\n",
       "Df Residuals:                    2445   BIC:                         1.391e+04\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.081    1.4e+04      0.000    1134.736    1135.054\n",
       "대비_irs_1Y      0.2998      0.143      2.093      0.036       0.019       0.581\n",
       "대비_irs_3Y      0.1017      0.128      0.793      0.428      -0.150       0.353\n",
       "대비_irs_10Y    -0.4269      0.157     -2.727      0.006      -0.734      -0.120\n",
       "대비_crs_1Y     -0.6728      0.118     -5.721      0.000      -0.903      -0.442\n",
       "대비_crs_3Y     -0.2868      0.152     -1.884      0.060      -0.585       0.012\n",
       "대비_crs_5Y     -0.2828      0.196     -1.445      0.149      -0.667       0.101\n",
       "대비_crs_10Y     0.1212      0.156      0.775      0.439      -0.186       0.428\n",
       "대비_국고_3Y      -0.1638      0.090     -1.821      0.069      -0.340       0.013\n",
       "대비_국고_5Y       0.2837      0.112      2.525      0.012       0.063       0.504\n",
       "스왑포인트_1M      -1.1722      0.091    -12.921      0.000      -1.350      -0.994\n",
       "전일종가_ex       55.7459      0.088    630.402      0.000      55.573      55.919\n",
       "종가_NDF_차이     -3.9509      0.087    -45.200      0.000      -4.122      -3.779\n",
       "==============================================================================\n",
       "Omnibus:                      211.135   Durbin-Watson:                   2.115\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              911.718\n",
       "Skew:                           0.309   Prob(JB):                    1.05e-198\n",
       "Kurtosis:                       5.919   Cond. No.                         5.76\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_irs_5Y', '대비_통안_2Y'], axis=1, inplace=True)\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>4.671e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:17:11</td>     <th>  Log-Likelihood:    </th> <td> -6906.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.383e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2447</td>      <th>  BIC:               </th> <td>1.390e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.081</td> <td>  1.4e+04</td> <td> 0.000</td> <td> 1134.736</td> <td> 1135.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.3420</td> <td>    0.131</td> <td>    2.602</td> <td> 0.009</td> <td>    0.084</td> <td>    0.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.3911</td> <td>    0.151</td> <td>   -2.588</td> <td> 0.010</td> <td>   -0.687</td> <td>   -0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -0.6736</td> <td>    0.118</td> <td>   -5.731</td> <td> 0.000</td> <td>   -0.904</td> <td>   -0.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.2821</td> <td>    0.152</td> <td>   -1.855</td> <td> 0.064</td> <td>   -0.580</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_5Y</th>  <td>   -0.1797</td> <td>    0.151</td> <td>   -1.190</td> <td> 0.234</td> <td>   -0.476</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.1623</td> <td>    0.090</td> <td>   -1.806</td> <td> 0.071</td> <td>   -0.339</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.2946</td> <td>    0.112</td> <td>    2.634</td> <td> 0.008</td> <td>    0.075</td> <td>    0.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -1.1731</td> <td>    0.091</td> <td>  -12.934</td> <td> 0.000</td> <td>   -1.351</td> <td>   -0.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.7443</td> <td>    0.088</td> <td>  630.567</td> <td> 0.000</td> <td>   55.571</td> <td>   55.918</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>  <td>   -3.9508</td> <td>    0.087</td> <td>  -45.211</td> <td> 0.000</td> <td>   -4.122</td> <td>   -3.779</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>211.797</td> <th>  Durbin-Watson:     </th> <td>   2.114</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 914.570</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.311</td>  <th>  Prob(JB):          </th> <td>2.53e-199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.923</td>  <th>  Cond. No.          </th> <td>    4.28</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 4.671e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        21:17:11   Log-Likelihood:                -6906.4\n",
       "No. Observations:                2458   AIC:                         1.383e+04\n",
       "Df Residuals:                    2447   BIC:                         1.390e+04\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.081    1.4e+04      0.000    1134.736    1135.054\n",
       "대비_irs_1Y      0.3420      0.131      2.602      0.009       0.084       0.600\n",
       "대비_irs_10Y    -0.3911      0.151     -2.588      0.010      -0.687      -0.095\n",
       "대비_crs_1Y     -0.6736      0.118     -5.731      0.000      -0.904      -0.443\n",
       "대비_crs_3Y     -0.2821      0.152     -1.855      0.064      -0.580       0.016\n",
       "대비_crs_5Y     -0.1797      0.151     -1.190      0.234      -0.476       0.117\n",
       "대비_국고_3Y      -0.1623      0.090     -1.806      0.071      -0.339       0.014\n",
       "대비_국고_5Y       0.2946      0.112      2.634      0.008       0.075       0.514\n",
       "스왑포인트_1M      -1.1731      0.091    -12.934      0.000      -1.351      -0.995\n",
       "전일종가_ex       55.7443      0.088    630.567      0.000      55.571      55.918\n",
       "종가_NDF_차이     -3.9508      0.087    -45.211      0.000      -4.122      -3.779\n",
       "==============================================================================\n",
       "Omnibus:                      211.797   Durbin-Watson:                   2.114\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              914.570\n",
       "Skew:                           0.311   Prob(JB):                    2.53e-199\n",
       "Kurtosis:                       5.923   Cond. No.                         4.28\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_irs_3Y', '대비_crs_10Y'], axis=1, inplace=True)\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>5.189e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:17:12</td>     <th>  Log-Likelihood:    </th> <td> -6907.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.383e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2448</td>      <th>  BIC:               </th> <td>1.389e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.081</td> <td>  1.4e+04</td> <td> 0.000</td> <td> 1134.736</td> <td> 1135.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.3496</td> <td>    0.131</td> <td>    2.663</td> <td> 0.008</td> <td>    0.092</td> <td>    0.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.4054</td> <td>    0.151</td> <td>   -2.690</td> <td> 0.007</td> <td>   -0.701</td> <td>   -0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -0.7098</td> <td>    0.114</td> <td>   -6.252</td> <td> 0.000</td> <td>   -0.932</td> <td>   -0.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.3991</td> <td>    0.116</td> <td>   -3.444</td> <td> 0.001</td> <td>   -0.626</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.1642</td> <td>    0.090</td> <td>   -1.828</td> <td> 0.068</td> <td>   -0.340</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.2788</td> <td>    0.111</td> <td>    2.511</td> <td> 0.012</td> <td>    0.061</td> <td>    0.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -1.1723</td> <td>    0.091</td> <td>  -12.925</td> <td> 0.000</td> <td>   -1.350</td> <td>   -0.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.7464</td> <td>    0.088</td> <td>  630.661</td> <td> 0.000</td> <td>   55.573</td> <td>   55.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>  <td>   -3.9501</td> <td>    0.087</td> <td>  -45.200</td> <td> 0.000</td> <td>   -4.122</td> <td>   -3.779</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>212.629</td> <th>  Durbin-Watson:     </th> <td>   2.112</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 922.539</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.311</td>  <th>  Prob(JB):          </th> <td>4.71e-201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.936</td>  <th>  Cond. No.          </th> <td>    3.84</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 5.189e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        21:17:12   Log-Likelihood:                -6907.1\n",
       "No. Observations:                2458   AIC:                         1.383e+04\n",
       "Df Residuals:                    2448   BIC:                         1.389e+04\n",
       "Df Model:                           9                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.081    1.4e+04      0.000    1134.736    1135.054\n",
       "대비_irs_1Y      0.3496      0.131      2.663      0.008       0.092       0.607\n",
       "대비_irs_10Y    -0.4054      0.151     -2.690      0.007      -0.701      -0.110\n",
       "대비_crs_1Y     -0.7098      0.114     -6.252      0.000      -0.932      -0.487\n",
       "대비_crs_3Y     -0.3991      0.116     -3.444      0.001      -0.626      -0.172\n",
       "대비_국고_3Y      -0.1642      0.090     -1.828      0.068      -0.340       0.012\n",
       "대비_국고_5Y       0.2788      0.111      2.511      0.012       0.061       0.496\n",
       "스왑포인트_1M      -1.1723      0.091    -12.925      0.000      -1.350      -0.994\n",
       "전일종가_ex       55.7464      0.088    630.661      0.000      55.573      55.920\n",
       "종가_NDF_차이     -3.9501      0.087    -45.200      0.000      -4.122      -3.779\n",
       "==============================================================================\n",
       "Omnibus:                      212.629   Durbin-Watson:                   2.112\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              922.539\n",
       "Skew:                           0.311   Prob(JB):                    4.71e-201\n",
       "Kurtosis:                       5.936   Cond. No.                         3.84\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_crs_5Y'], axis=1, inplace=True)\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>5.832e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:17:13</td>     <th>  Log-Likelihood:    </th> <td> -6908.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.384e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2449</td>      <th>  BIC:               </th> <td>1.389e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.081</td> <td>  1.4e+04</td> <td> 0.000</td> <td> 1134.736</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.3316</td> <td>    0.131</td> <td>    2.531</td> <td> 0.011</td> <td>    0.075</td> <td>    0.588</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.3941</td> <td>    0.151</td> <td>   -2.616</td> <td> 0.009</td> <td>   -0.689</td> <td>   -0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -0.7114</td> <td>    0.114</td> <td>   -6.263</td> <td> 0.000</td> <td>   -0.934</td> <td>   -0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.4014</td> <td>    0.116</td> <td>   -3.462</td> <td> 0.001</td> <td>   -0.629</td> <td>   -0.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.2098</td> <td>    0.104</td> <td>    2.008</td> <td> 0.045</td> <td>    0.005</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -1.1737</td> <td>    0.091</td> <td>  -12.934</td> <td> 0.000</td> <td>   -1.352</td> <td>   -0.996</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.7460</td> <td>    0.088</td> <td>  630.358</td> <td> 0.000</td> <td>   55.573</td> <td>   55.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>  <td>   -3.9535</td> <td>    0.087</td> <td>  -45.228</td> <td> 0.000</td> <td>   -4.125</td> <td>   -3.782</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>211.569</td> <th>  Durbin-Watson:     </th> <td>   2.112</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 914.663</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.309</td>  <th>  Prob(JB):          </th> <td>2.42e-199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.924</td>  <th>  Cond. No.          </th> <td>    3.68</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 5.832e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        21:17:13   Log-Likelihood:                -6908.8\n",
       "No. Observations:                2458   AIC:                         1.384e+04\n",
       "Df Residuals:                    2449   BIC:                         1.389e+04\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.081    1.4e+04      0.000    1134.736    1135.055\n",
       "대비_irs_1Y      0.3316      0.131      2.531      0.011       0.075       0.588\n",
       "대비_irs_10Y    -0.3941      0.151     -2.616      0.009      -0.689      -0.099\n",
       "대비_crs_1Y     -0.7114      0.114     -6.263      0.000      -0.934      -0.489\n",
       "대비_crs_3Y     -0.4014      0.116     -3.462      0.001      -0.629      -0.174\n",
       "대비_국고_5Y       0.2098      0.104      2.008      0.045       0.005       0.415\n",
       "스왑포인트_1M      -1.1737      0.091    -12.934      0.000      -1.352      -0.996\n",
       "전일종가_ex       55.7460      0.088    630.358      0.000      55.573      55.919\n",
       "종가_NDF_차이     -3.9535      0.087    -45.228      0.000      -4.125      -3.782\n",
       "==============================================================================\n",
       "Omnibus:                      211.569   Durbin-Watson:                   2.112\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              914.663\n",
       "Skew:                           0.309   Prob(JB):                    2.42e-199\n",
       "Kurtosis:                       5.924   Cond. No.                         3.68\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_국고_3Y'], axis=1, inplace=True)\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['대비_irs_1Y', '대비_irs_10Y', '대비_crs_1Y', '대비_crs_3Y', '대비_국고_5Y',\n",
       "       '스왑포인트_1M', '전일종가_ex', '종가_NDF_차이'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_irs_1Y</th>\n",
       "      <th>대비_irs_10Y</th>\n",
       "      <th>대비_crs_1Y</th>\n",
       "      <th>대비_crs_3Y</th>\n",
       "      <th>대비_국고_5Y</th>\n",
       "      <th>스왑포인트_1M</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_NDF_차이</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-0.848159</td>\n",
       "      <td>-0.403644</td>\n",
       "      <td>-0.205698</td>\n",
       "      <td>-0.462791</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.056282</td>\n",
       "      <td>-1.367171</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>0.559997</td>\n",
       "      <td>0.202253</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>-0.690892</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>1.602437</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.404219</td>\n",
       "      <td>0.401029</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1.911215</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>0.117633</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.606184</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.108476</td>\n",
       "      <td>-0.224108</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-09</th>\n",
       "      <td>1.404890</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>1.816727</td>\n",
       "      <td>0.905815</td>\n",
       "      <td>1.351270</td>\n",
       "      <td>1.775350</td>\n",
       "      <td>-0.117475</td>\n",
       "      <td>-0.872236</td>\n",
       "      <td>1125.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-1.129790</td>\n",
       "      <td>-2.221333</td>\n",
       "      <td>-1.621396</td>\n",
       "      <td>-1.603295</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>-0.896666</td>\n",
       "      <td>3.206786</td>\n",
       "      <td>0.860035</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>-0.284897</td>\n",
       "      <td>-0.605609</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.690892</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>-0.987243</td>\n",
       "      <td>3.219385</td>\n",
       "      <td>0.753978</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>-0.284897</td>\n",
       "      <td>-1.211506</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.234690</td>\n",
       "      <td>-1.350390</td>\n",
       "      <td>-0.851378</td>\n",
       "      <td>3.109596</td>\n",
       "      <td>-0.565848</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.841628</td>\n",
       "      <td>1.414046</td>\n",
       "      <td>0.805515</td>\n",
       "      <td>0.905815</td>\n",
       "      <td>0.810938</td>\n",
       "      <td>-0.941955</td>\n",
       "      <td>3.212186</td>\n",
       "      <td>1.838120</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-1.974684</td>\n",
       "      <td>-2.423299</td>\n",
       "      <td>-2.228124</td>\n",
       "      <td>-3.428103</td>\n",
       "      <td>-3.241553</td>\n",
       "      <td>-0.896666</td>\n",
       "      <td>2.902617</td>\n",
       "      <td>0.200122</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2458 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_irs_1Y  대비_irs_10Y  대비_crs_1Y  대비_crs_3Y  대비_국고_5Y  스왑포인트_1M  \\\n",
       "DateTime                                                                      \n",
       "2012-08-03  -0.848159   -0.403644  -0.205698  -0.462791 -1.890723  1.820638   \n",
       "2012-08-06   0.559997    0.202253  -0.003456  -0.690892  0.000440  1.820638   \n",
       "2012-08-07   0.278366    0.404219   0.401029  -0.006589  0.000440  1.911215   \n",
       "2012-08-08   0.278366    0.606184   0.198787  -0.006589 -0.539892  1.820638   \n",
       "2012-08-09   1.404890    0.808150   1.816727   0.905815  1.351270  1.775350   \n",
       "...               ...         ...        ...        ...       ...       ...   \n",
       "2022-07-25  -1.129790   -2.221333  -1.621396  -1.603295 -1.890723 -0.896666   \n",
       "2022-07-26  -0.284897   -0.605609   0.198787  -0.690892 -0.539892 -0.987243   \n",
       "2022-07-27  -0.284897   -1.211506   0.198787  -0.234690 -1.350390 -0.851378   \n",
       "2022-07-28   0.841628    1.414046   0.805515   0.905815  0.810938 -0.941955   \n",
       "2022-07-29  -1.974684   -2.423299  -2.228124  -3.428103 -3.241553 -0.896666   \n",
       "\n",
       "             전일종가_ex  종가_NDF_차이   종가_ex  \n",
       "DateTime                                 \n",
       "2012-08-03 -0.056282  -1.367171  1134.8  \n",
       "2012-08-06 -0.000487   1.602437  1129.0  \n",
       "2012-08-07 -0.104877   0.117633  1128.8  \n",
       "2012-08-08 -0.108476  -0.224108  1128.3  \n",
       "2012-08-09 -0.117475  -0.872236  1125.5  \n",
       "...              ...        ...     ...  \n",
       "2022-07-25  3.206786   0.860035  1313.7  \n",
       "2022-07-26  3.219385   0.753978  1307.6  \n",
       "2022-07-27  3.109596  -0.565848  1313.3  \n",
       "2022-07-28  3.212186   1.838120  1296.1  \n",
       "2022-07-29  2.902617   0.200122  1299.1  \n",
       "\n",
       "[2458 rows x 9 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = pd.concat([x_scaled,y], axis=1)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_scaled[0:1945]\n",
    "test = df_scaled[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=5):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['대비_irs_1Y', '대비_irs_10Y', '대비_crs_1Y', '대비_crs_3Y', '대비_국고_5Y',\n",
    "       '스왑포인트_1M', '전일종가_ex', '종가_NDF_차이']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 5)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5.59996879e-01,  1.41404643e+00,  4.01029391e-01, ...,\n",
       "         -8.14749063e-02, -4.86437850e-01, -2.00351545e+00],\n",
       "        [ 2.78365712e-01,  6.06184230e-01, -3.45573131e-03, ...,\n",
       "         -1.26763310e-01, -3.80248656e-01, -1.88755079e-01],\n",
       "        [-2.84896622e-01,  2.87583166e-04, -2.05698292e-01, ...,\n",
       "         -8.14749063e-02, -4.07245908e-01, -6.60121408e-01],\n",
       "        [ 2.78365712e-01,  6.06184230e-01, -2.05698292e-01, ...,\n",
       "         -1.72051713e-01, -3.47851952e-01,  5.87122431e-02],\n",
       "        [ 2.78365712e-01,  1.01011533e+00,  4.01029391e-01, ...,\n",
       "         -1.26763310e-01, -4.75638949e-01, -4.73451808e-02]],\n",
       "\n",
       "       [[-3.26545495e-03,  6.06184230e-01, -2.05698292e-01, ...,\n",
       "          1.27717719e+00, -1.12717265e+00, -6.36553092e-01],\n",
       "        [-3.26545495e-03, -4.03643515e-01, -2.05698292e-01, ...,\n",
       "          1.27717719e+00, -1.12897247e+00, -1.50858080e+00],\n",
       "        [-3.26545495e-03, -2.01677966e-01,  8.05514512e-01, ...,\n",
       "          1.23188879e+00, -1.08577687e+00,  6.47920154e-01],\n",
       "        [-3.26545495e-03,  4.04218681e-01,  8.05514512e-01, ...,\n",
       "          1.23188879e+00, -1.11457393e+00, -6.12984775e-01],\n",
       "        [-2.84896622e-01,  2.87583166e-04, -4.07940853e-01, ...,\n",
       "          1.36775400e+00, -1.24956020e+00, -1.43787585e+00]],\n",
       "\n",
       "       [[-3.26545495e-03, -2.01677966e-01, -3.45573131e-03, ...,\n",
       "          4.61985934e-01,  1.56096769e-01, -5.54063984e-01],\n",
       "        [-2.84896622e-01, -2.01677966e-01, -3.45573131e-03, ...,\n",
       "          5.52562741e-01,  3.48677173e-01,  1.39032212e+00],\n",
       "        [-3.26545495e-03,  2.02253132e-01,  1.98786830e-01, ...,\n",
       "          5.07274337e-01,  2.02892007e-01, -1.50858080e+00],\n",
       "        [ 2.78365712e-01,  2.87583166e-04, -2.05698292e-01, ...,\n",
       "          5.52562741e-01,  3.64875525e-01, -1.27289764e+00],\n",
       "        [ 2.78365712e-01,  4.04218681e-01,  4.01029391e-01, ...,\n",
       "          5.52562741e-01,  3.82873694e-01,  1.41201351e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.84896622e-01, -6.05609064e-01, -2.05698292e-01, ...,\n",
       "         -1.16839659e+00, -1.93067702e-01,  2.00122142e-01],\n",
       "        [-2.84896622e-01, -4.03643515e-01, -3.45573131e-03, ...,\n",
       "         -1.12310818e+00, -1.96667336e-01,  3.41532040e-01],\n",
       "        [ 2.78365712e-01,  6.06184230e-01,  1.98786830e-01, ...,\n",
       "         -1.07781978e+00, -1.82268801e-01,  4.69280849e-02],\n",
       "        [-3.26545495e-03,  2.02253132e-01, -4.07940853e-01, ...,\n",
       "         -1.07781978e+00, -1.96667336e-01,  8.83603318e-01],\n",
       "        [-3.26545495e-03, -2.01677966e-01, -3.45573131e-03, ...,\n",
       "         -1.12310818e+00, -2.36263307e-01, -7.07258041e-01]],\n",
       "\n",
       "       [[ 2.78365712e-01,  4.04218681e-01,  8.05514512e-01, ...,\n",
       "         -1.07781978e+00,  9.71413807e-01,  1.50816370e+00],\n",
       "        [ 2.78365712e-01,  2.02253132e-01, -2.05698292e-01, ...,\n",
       "         -1.12310818e+00,  9.01220950e-01,  5.65431047e-01],\n",
       "        [-1.12979012e+00,  6.06184230e-01,  6.03271952e-01, ...,\n",
       "         -1.12310818e+00,  9.06620400e-01,  3.76884515e-01],\n",
       "        [ 8.41628046e-01,  1.01011533e+00,  6.03271952e-01, ...,\n",
       "         -1.12310818e+00,  9.53415639e-01,  6.24351838e-01],\n",
       "        [-5.66527789e-01,  2.02253132e-01, -3.45573131e-03, ...,\n",
       "         -1.12310818e+00,  9.39017104e-01,  2.17986072e+00]],\n",
       "\n",
       "       [[-2.84896622e-01, -1.00954016e+00, -5.46400488e+00, ...,\n",
       "          1.09602358e+00, -5.44818037e-02, -1.46144417e+00],\n",
       "        [ 5.59996879e-01,  4.04218681e-01,  6.67054878e+00, ...,\n",
       "          1.18660039e+00,  9.49029958e-02, -8.84020414e-01],\n",
       "        [-2.84896622e-01,  6.06184230e-01, -1.01466854e+00, ...,\n",
       "          1.32246560e+00,  8.23042778e-02,  3.29747882e-01],\n",
       "        [ 2.53141505e+00,  2.22190862e+00,  2.01896988e+00, ...,\n",
       "          1.18660039e+00,  1.57110539e-02,  1.22534391e+00],\n",
       "        [ 5.59996879e-01,  6.06184230e-01, -4.07940853e-01, ...,\n",
       "          1.18660039e+00, -1.03076859e-01, -4.48006560e-01]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((508, 5, 8), (508, 1))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 5)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "97/97 [==============================] - ETA: 0s - loss: 1260284.2500 - mae: 1121.5541\n",
      "Epoch 1: val_loss improved from inf to 1252077.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 10s 77ms/step - loss: 1260284.2500 - mae: 1121.5541 - val_loss: 1252077.0000 - val_mae: 1118.0861\n",
      "Epoch 2/200\n",
      "95/97 [============================>.] - ETA: 0s - loss: 1007281.1250 - mae: 964.0506\n",
      "Epoch 2: val_loss improved from 1252077.00000 to 619882.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 81ms/step - loss: 999526.1875 - mae: 957.5783 - val_loss: 619882.1250 - val_mae: 713.9823\n",
      "Epoch 3/200\n",
      "84/97 [========================>.....] - ETA: 0s - loss: 492179.2812 - mae: 609.5801\n",
      "Epoch 3: val_loss improved from 619882.12500 to 191553.85938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 83ms/step - loss: 460206.0625 - mae: 576.3331 - val_loss: 191553.8594 - val_mae: 345.6505\n",
      "Epoch 4/200\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 171231.0938 - mae: 308.9061\n",
      "Epoch 4: val_loss improved from 191553.85938 to 97590.57812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 84ms/step - loss: 168495.3750 - mae: 305.0760 - val_loss: 97590.5781 - val_mae: 242.3146\n",
      "Epoch 5/200\n",
      "94/97 [============================>.] - ETA: 0s - loss: 100218.2266 - mae: 235.0756\n",
      "Epoch 5: val_loss improved from 97590.57812 to 62257.13281, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 73ms/step - loss: 100388.0625 - mae: 234.8243 - val_loss: 62257.1328 - val_mae: 197.7608\n",
      "Epoch 6/200\n",
      "87/97 [=========================>....] - ETA: 0s - loss: 66551.6250 - mae: 191.3235\n",
      "Epoch 6: val_loss improved from 62257.13281 to 40088.48828, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 78ms/step - loss: 64616.1016 - mae: 189.2491 - val_loss: 40088.4883 - val_mae: 160.1849\n",
      "Epoch 7/200\n",
      "92/97 [===========================>..] - ETA: 0s - loss: 42843.7773 - mae: 150.6003\n",
      "Epoch 7: val_loss improved from 40088.48828 to 24593.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 71ms/step - loss: 42585.1367 - mae: 149.3651 - val_loss: 24593.8125 - val_mae: 123.4325\n",
      "Epoch 8/200\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 29660.4785 - mae: 124.0771\n",
      "Epoch 8: val_loss improved from 24593.81250 to 16728.54297, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 80ms/step - loss: 28916.5566 - mae: 122.7788 - val_loss: 16728.5430 - val_mae: 103.4249\n",
      "Epoch 9/200\n",
      "84/97 [========================>.....] - ETA: 0s - loss: 20441.1719 - mae: 101.6337\n",
      "Epoch 9: val_loss improved from 16728.54297 to 11271.87109, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 90ms/step - loss: 19969.5352 - mae: 100.7790 - val_loss: 11271.8711 - val_mae: 82.3933\n",
      "Epoch 10/200\n",
      "95/97 [============================>.] - ETA: 0s - loss: 13455.5645 - mae: 83.8841\n",
      "Epoch 10: val_loss improved from 11271.87109 to 8152.97168, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 95ms/step - loss: 14306.9072 - mae: 84.7056 - val_loss: 8152.9717 - val_mae: 69.9233\n",
      "Epoch 11/200\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 10781.3828 - mae: 73.8679\n",
      "Epoch 11: val_loss improved from 8152.97168 to 5977.97363, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 10s 100ms/step - loss: 10611.2031 - mae: 73.2517 - val_loss: 5977.9736 - val_mae: 61.2676\n",
      "Epoch 12/200\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 7910.8755 - mae: 63.1907\n",
      "Epoch 12: val_loss improved from 5977.97363 to 4448.55518, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 95ms/step - loss: 7908.7764 - mae: 63.0239 - val_loss: 4448.5552 - val_mae: 51.6410\n",
      "Epoch 13/200\n",
      "97/97 [==============================] - ETA: 0s - loss: 6230.3950 - mae: 55.9587\n",
      "Epoch 13: val_loss improved from 4448.55518 to 3512.85547, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 10s 101ms/step - loss: 6230.3950 - mae: 55.9587 - val_loss: 3512.8555 - val_mae: 46.1418\n",
      "Epoch 14/200\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 5096.4976 - mae: 49.9381\n",
      "Epoch 14: val_loss improved from 3512.85547 to 2994.88330, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 92ms/step - loss: 4971.8447 - mae: 49.4593 - val_loss: 2994.8833 - val_mae: 42.5943\n",
      "Epoch 15/200\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 4099.1069 - mae: 44.4667\n",
      "Epoch 15: val_loss improved from 2994.88330 to 2493.19434, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 10s 100ms/step - loss: 4060.8064 - mae: 44.6177 - val_loss: 2493.1943 - val_mae: 38.3353\n",
      "Epoch 16/200\n",
      "95/97 [============================>.] - ETA: 0s - loss: 3402.0950 - mae: 40.8191\n",
      "Epoch 16: val_loss improved from 2493.19434 to 2121.55151, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 95ms/step - loss: 3362.6646 - mae: 40.6308 - val_loss: 2121.5515 - val_mae: 35.2344\n",
      "Epoch 17/200\n",
      "87/97 [=========================>....] - ETA: 0s - loss: 2921.4514 - mae: 37.5927\n",
      "Epoch 17: val_loss improved from 2121.55151 to 2102.97119, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 94ms/step - loss: 2907.3008 - mae: 37.7614 - val_loss: 2102.9712 - val_mae: 35.7970\n",
      "Epoch 18/200\n",
      "97/97 [==============================] - ETA: 0s - loss: 2466.4719 - mae: 34.5891\n",
      "Epoch 18: val_loss improved from 2102.97119 to 1804.64600, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 98ms/step - loss: 2466.4719 - mae: 34.5891 - val_loss: 1804.6460 - val_mae: 31.9746\n",
      "Epoch 19/200\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 2123.8008 - mae: 32.4106\n",
      "Epoch 19: val_loss improved from 1804.64600 to 1541.61328, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 94ms/step - loss: 2107.5813 - mae: 32.2137 - val_loss: 1541.6133 - val_mae: 29.7566\n",
      "Epoch 20/200\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 1838.2802 - mae: 29.9267\n",
      "Epoch 20: val_loss improved from 1541.61328 to 1349.50708, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 77ms/step - loss: 1798.8262 - mae: 29.6303 - val_loss: 1349.5071 - val_mae: 27.3824\n",
      "Epoch 21/200\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 1612.7496 - mae: 27.6589\n",
      "Epoch 21: val_loss improved from 1349.50708 to 1231.53992, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 76ms/step - loss: 1577.3790 - mae: 27.4117 - val_loss: 1231.5399 - val_mae: 26.0915\n",
      "Epoch 22/200\n",
      "96/97 [============================>.] - ETA: 0s - loss: 1436.7010 - mae: 26.3499\n",
      "Epoch 22: val_loss improved from 1231.53992 to 1185.14905, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 72ms/step - loss: 1442.8206 - mae: 26.4151 - val_loss: 1185.1490 - val_mae: 26.0267\n",
      "Epoch 23/200\n",
      "92/97 [===========================>..] - ETA: 0s - loss: 1314.0569 - mae: 24.9068\n",
      "Epoch 23: val_loss improved from 1185.14905 to 1073.24719, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 80ms/step - loss: 1288.3774 - mae: 24.7106 - val_loss: 1073.2472 - val_mae: 24.4059\n",
      "Epoch 24/200\n",
      "92/97 [===========================>..] - ETA: 0s - loss: 1165.0411 - mae: 23.3763\n",
      "Epoch 24: val_loss improved from 1073.24719 to 1008.65997, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 73ms/step - loss: 1161.3738 - mae: 23.3816 - val_loss: 1008.6600 - val_mae: 23.4490\n",
      "Epoch 25/200\n",
      "95/97 [============================>.] - ETA: 0s - loss: 1055.1978 - mae: 22.0535\n",
      "Epoch 25: val_loss improved from 1008.65997 to 972.96326, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 75ms/step - loss: 1052.1604 - mae: 22.0066 - val_loss: 972.9633 - val_mae: 23.1218\n",
      "Epoch 26/200\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 969.4123 - mae: 21.5491\n",
      "Epoch 26: val_loss improved from 972.96326 to 880.74426, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 78ms/step - loss: 976.4042 - mae: 21.6812 - val_loss: 880.7443 - val_mae: 21.6365\n",
      "Epoch 27/200\n",
      "87/97 [=========================>....] - ETA: 0s - loss: 883.5432 - mae: 20.3614\n",
      "Epoch 27: val_loss improved from 880.74426 to 824.08295, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 10s 106ms/step - loss: 875.1003 - mae: 20.3713 - val_loss: 824.0829 - val_mae: 21.1247\n",
      "Epoch 28/200\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 810.8985 - mae: 19.0935\n",
      "Epoch 28: val_loss improved from 824.08295 to 805.96173, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 90ms/step - loss: 794.5285 - mae: 19.0763 - val_loss: 805.9617 - val_mae: 20.6815\n",
      "Epoch 29/200\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 695.9125 - mae: 18.7888\n",
      "Epoch 29: val_loss improved from 805.96173 to 762.23236, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 92ms/step - loss: 750.2656 - mae: 18.8075 - val_loss: 762.2324 - val_mae: 20.3786\n",
      "Epoch 30/200\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 692.4280 - mae: 17.8802\n",
      "Epoch 30: val_loss improved from 762.23236 to 711.72565, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 80ms/step - loss: 677.3006 - mae: 17.7800 - val_loss: 711.7256 - val_mae: 19.3658\n",
      "Epoch 31/200\n",
      "88/97 [==========================>...] - ETA: 0s - loss: 632.9262 - mae: 17.0941\n",
      "Epoch 31: val_loss improved from 711.72565 to 676.71552, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 82ms/step - loss: 618.5417 - mae: 16.9661 - val_loss: 676.7155 - val_mae: 18.5355\n",
      "Epoch 32/200\n",
      "96/97 [============================>.] - ETA: 0s - loss: 574.0252 - mae: 16.4501\n",
      "Epoch 32: val_loss improved from 676.71552 to 633.36627, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 84ms/step - loss: 572.5408 - mae: 16.4433 - val_loss: 633.3663 - val_mae: 17.9190\n",
      "Epoch 33/200\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 551.3912 - mae: 16.3835\n",
      "Epoch 33: val_loss did not improve from 633.36627\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 545.3378 - mae: 16.3800 - val_loss: 713.8621 - val_mae: 18.9250\n",
      "Epoch 34/200\n",
      "86/97 [=========================>....] - ETA: 0s - loss: 498.2776 - mae: 15.5781\n",
      "Epoch 34: val_loss improved from 633.36627 to 596.00623, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 77ms/step - loss: 499.4478 - mae: 15.5536 - val_loss: 596.0062 - val_mae: 16.9890\n",
      "Epoch 35/200\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 458.3689 - mae: 15.2033\n",
      "Epoch 35: val_loss improved from 596.00623 to 543.70404, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 84ms/step - loss: 468.2403 - mae: 15.2544 - val_loss: 543.7040 - val_mae: 16.1370\n",
      "Epoch 36/200\n",
      "86/97 [=========================>....] - ETA: 0s - loss: 424.8602 - mae: 14.4792\n",
      "Epoch 36: val_loss did not improve from 543.70404\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 450.8818 - mae: 14.8298 - val_loss: 548.0853 - val_mae: 16.6570\n",
      "Epoch 37/200\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 385.8944 - mae: 14.3647\n",
      "Epoch 37: val_loss improved from 543.70404 to 515.54254, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 69ms/step - loss: 410.0420 - mae: 14.4473 - val_loss: 515.5425 - val_mae: 15.5738\n",
      "Epoch 38/200\n",
      "95/97 [============================>.] - ETA: 0s - loss: 384.3126 - mae: 13.8914\n",
      "Epoch 38: val_loss improved from 515.54254 to 502.49866, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 80ms/step - loss: 382.8391 - mae: 13.8858 - val_loss: 502.4987 - val_mae: 15.4499\n",
      "Epoch 39/200\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 365.5816 - mae: 13.4548\n",
      "Epoch 39: val_loss improved from 502.49866 to 486.10764, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 75ms/step - loss: 357.7840 - mae: 13.3693 - val_loss: 486.1076 - val_mae: 15.0432\n",
      "Epoch 40/200\n",
      "96/97 [============================>.] - ETA: 0s - loss: 346.0614 - mae: 13.3641\n",
      "Epoch 40: val_loss improved from 486.10764 to 473.27771, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 80ms/step - loss: 348.7540 - mae: 13.4025 - val_loss: 473.2777 - val_mae: 14.8437\n",
      "Epoch 41/200\n",
      "88/97 [==========================>...] - ETA: 0s - loss: 326.5634 - mae: 12.8584\n",
      "Epoch 41: val_loss did not improve from 473.27771\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 322.7274 - mae: 12.8663 - val_loss: 479.5488 - val_mae: 15.0651\n",
      "Epoch 42/200\n",
      "87/97 [=========================>....] - ETA: 0s - loss: 293.8301 - mae: 12.2407\n",
      "Epoch 42: val_loss improved from 473.27771 to 451.35962, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 79ms/step - loss: 302.7822 - mae: 12.4771 - val_loss: 451.3596 - val_mae: 14.6578\n",
      "Epoch 43/200\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 294.6969 - mae: 12.3187\n",
      "Epoch 43: val_loss improved from 451.35962 to 420.70212, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 75ms/step - loss: 292.9651 - mae: 12.2769 - val_loss: 420.7021 - val_mae: 13.7741\n",
      "Epoch 44/200\n",
      "97/97 [==============================] - ETA: 0s - loss: 288.7387 - mae: 12.3402\n",
      "Epoch 44: val_loss improved from 420.70212 to 406.09323, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 9s 90ms/step - loss: 288.7387 - mae: 12.3402 - val_loss: 406.0932 - val_mae: 13.7124\n",
      "Epoch 45/200\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 259.8154 - mae: 11.8606\n",
      "Epoch 45: val_loss did not improve from 406.09323\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 268.2484 - mae: 11.9769 - val_loss: 419.5417 - val_mae: 13.9554\n",
      "Epoch 46/200\n",
      "92/97 [===========================>..] - ETA: 0s - loss: 259.3480 - mae: 11.9122\n",
      "Epoch 46: val_loss improved from 406.09323 to 378.47250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 75ms/step - loss: 269.0417 - mae: 12.0055 - val_loss: 378.4725 - val_mae: 13.2459\n",
      "Epoch 47/200\n",
      "96/97 [============================>.] - ETA: 0s - loss: 244.9614 - mae: 11.3777\n",
      "Epoch 47: val_loss did not improve from 378.47250\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 244.3730 - mae: 11.3715 - val_loss: 378.6769 - val_mae: 13.2728\n",
      "Epoch 48/200\n",
      "88/97 [==========================>...] - ETA: 0s - loss: 248.8218 - mae: 11.6851\n",
      "Epoch 48: val_loss improved from 378.47250 to 358.66641, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 83ms/step - loss: 246.2150 - mae: 11.6124 - val_loss: 358.6664 - val_mae: 12.8702\n",
      "Epoch 49/200\n",
      "87/97 [=========================>....] - ETA: 0s - loss: 216.3719 - mae: 10.8607\n",
      "Epoch 49: val_loss improved from 358.66641 to 347.67096, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 71ms/step - loss: 227.4985 - mae: 11.1525 - val_loss: 347.6710 - val_mae: 12.6454\n",
      "Epoch 50/200\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 216.7315 - mae: 10.9667\n",
      "Epoch 50: val_loss did not improve from 347.67096\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 217.8206 - mae: 10.9500 - val_loss: 378.1143 - val_mae: 13.4748\n",
      "Epoch 51/200\n",
      "97/97 [==============================] - ETA: 0s - loss: 228.1563 - mae: 11.3557\n",
      "Epoch 51: val_loss improved from 347.67096 to 340.24738, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 80ms/step - loss: 228.1563 - mae: 11.3557 - val_loss: 340.2474 - val_mae: 12.7646\n",
      "Epoch 52/200\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 212.3206 - mae: 10.9714\n",
      "Epoch 52: val_loss did not improve from 340.24738\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 210.2863 - mae: 10.9227 - val_loss: 349.7563 - val_mae: 12.9943\n",
      "Epoch 53/200\n",
      "97/97 [==============================] - ETA: 0s - loss: 194.6077 - mae: 10.5678\n",
      "Epoch 53: val_loss improved from 340.24738 to 319.55081, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 76ms/step - loss: 194.6077 - mae: 10.5678 - val_loss: 319.5508 - val_mae: 12.2415\n",
      "Epoch 54/200\n",
      "86/97 [=========================>....] - ETA: 0s - loss: 195.8790 - mae: 10.8096\n",
      "Epoch 54: val_loss did not improve from 319.55081\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 200.4421 - mae: 10.8437 - val_loss: 363.8818 - val_mae: 13.2685\n",
      "Epoch 55/200\n",
      "88/97 [==========================>...] - ETA: 0s - loss: 189.0567 - mae: 10.3154\n",
      "Epoch 55: val_loss did not improve from 319.55081\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 188.1631 - mae: 10.2704 - val_loss: 324.2545 - val_mae: 12.4814\n",
      "Epoch 56/200\n",
      "92/97 [===========================>..] - ETA: 0s - loss: 183.1221 - mae: 10.2902\n",
      "Epoch 56: val_loss did not improve from 319.55081\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 190.8492 - mae: 10.4744 - val_loss: 475.0146 - val_mae: 17.3206\n",
      "Epoch 57/200\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 195.9102 - mae: 10.9182\n",
      "Epoch 57: val_loss did not improve from 319.55081\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 193.6413 - mae: 10.8251 - val_loss: 341.3601 - val_mae: 13.5190\n",
      "Epoch 58/200\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 171.6765 - mae: 10.0533\n",
      "Epoch 58: val_loss improved from 319.55081 to 297.88000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 76ms/step - loss: 174.1469 - mae: 10.1533 - val_loss: 297.8800 - val_mae: 12.1334\n",
      "Epoch 59/200\n",
      "94/97 [============================>.] - ETA: 0s - loss: 171.6336 - mae: 10.0549\n",
      "Epoch 59: val_loss did not improve from 297.88000\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 172.2250 - mae: 10.0653 - val_loss: 313.7878 - val_mae: 12.5961\n",
      "Epoch 60/200\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 174.7830 - mae: 10.2584\n",
      "Epoch 60: val_loss improved from 297.88000 to 276.73453, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 8s 81ms/step - loss: 176.3201 - mae: 10.3115 - val_loss: 276.7345 - val_mae: 11.5693\n",
      "Epoch 61/200\n",
      "87/97 [=========================>....] - ETA: 0s - loss: 160.2479 - mae: 9.8096\n",
      "Epoch 61: val_loss did not improve from 276.73453\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 160.8107 - mae: 9.8144 - val_loss: 289.4609 - val_mae: 11.8998\n",
      "Epoch 62/200\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 179.8317 - mae: 10.4361\n",
      "Epoch 62: val_loss did not improve from 276.73453\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 178.0013 - mae: 10.3651 - val_loss: 297.8211 - val_mae: 11.9467\n",
      "Epoch 63/200\n",
      "95/97 [============================>.] - ETA: 0s - loss: 162.5853 - mae: 9.9133 \n",
      "Epoch 63: val_loss did not improve from 276.73453\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 162.5452 - mae: 9.9045 - val_loss: 280.7104 - val_mae: 11.8922\n",
      "Epoch 64/200\n",
      "95/97 [============================>.] - ETA: 0s - loss: 151.8900 - mae: 9.6258\n",
      "Epoch 64: val_loss improved from 276.73453 to 275.10324, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 72ms/step - loss: 153.2522 - mae: 9.6842 - val_loss: 275.1032 - val_mae: 11.8173\n",
      "Epoch 65/200\n",
      "95/97 [============================>.] - ETA: 0s - loss: 148.3693 - mae: 9.4665\n",
      "Epoch 65: val_loss improved from 275.10324 to 273.71429, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 73ms/step - loss: 147.7495 - mae: 9.4471 - val_loss: 273.7143 - val_mae: 11.5291\n",
      "Epoch 66/200\n",
      "85/97 [=========================>....] - ETA: 0s - loss: 156.2145 - mae: 9.7063\n",
      "Epoch 66: val_loss improved from 273.71429 to 263.03812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 68ms/step - loss: 153.7556 - mae: 9.6243 - val_loss: 263.0381 - val_mae: 11.2910\n",
      "Epoch 67/200\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 158.9794 - mae: 9.9122\n",
      "Epoch 67: val_loss did not improve from 263.03812\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 159.4758 - mae: 9.9075 - val_loss: 283.7436 - val_mae: 11.9633\n",
      "Epoch 68/200\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 150.1053 - mae: 9.5159\n",
      "Epoch 68: val_loss improved from 263.03812 to 250.78673, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 73ms/step - loss: 148.0878 - mae: 9.4247 - val_loss: 250.7867 - val_mae: 11.1258\n",
      "Epoch 69/200\n",
      "94/97 [============================>.] - ETA: 0s - loss: 148.7207 - mae: 9.4951\n",
      "Epoch 69: val_loss did not improve from 250.78673\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 148.1816 - mae: 9.4898 - val_loss: 266.1229 - val_mae: 11.2864\n",
      "Epoch 70/200\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 142.4805 - mae: 9.2837\n",
      "Epoch 70: val_loss did not improve from 250.78673\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 142.2147 - mae: 9.2613 - val_loss: 262.5384 - val_mae: 11.2356\n",
      "Epoch 71/200\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 142.9020 - mae: 9.3004\n",
      "Epoch 71: val_loss did not improve from 250.78673\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 143.7351 - mae: 9.3115 - val_loss: 270.5552 - val_mae: 11.7408\n",
      "Epoch 72/200\n",
      "87/97 [=========================>....] - ETA: 0s - loss: 141.9623 - mae: 9.3216\n",
      "Epoch 72: val_loss improved from 250.78673 to 247.32477, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "97/97 [==============================] - 7s 70ms/step - loss: 143.3965 - mae: 9.3615 - val_loss: 247.3248 - val_mae: 10.8200\n",
      "Epoch 73/200\n",
      "96/97 [============================>.] - ETA: 0s - loss: 144.0916 - mae: 9.4272\n",
      "Epoch 73: val_loss did not improve from 247.32477\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 145.7671 - mae: 9.4856 - val_loss: 278.7387 - val_mae: 11.9130\n",
      "Epoch 74/200\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 166.2461 - mae: 10.2211\n",
      "Epoch 74: val_loss did not improve from 247.32477\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 163.4752 - mae: 10.1401 - val_loss: 253.6989 - val_mae: 11.0904\n",
      "Epoch 75/200\n",
      "96/97 [============================>.] - ETA: 0s - loss: 158.7455 - mae: 9.7777\n",
      "Epoch 75: val_loss did not improve from 247.32477\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 159.2352 - mae: 9.7853 - val_loss: 255.5134 - val_mae: 11.1357\n",
      "Epoch 76/200\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 136.3305 - mae: 8.9362\n",
      "Epoch 76: val_loss did not improve from 247.32477\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 137.9259 - mae: 8.9867 - val_loss: 288.6332 - val_mae: 11.8727\n",
      "Epoch 77/200\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 149.7943 - mae: 9.3677\n",
      "Epoch 77: val_loss did not improve from 247.32477\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 151.8577 - mae: 9.3864 - val_loss: 273.1342 - val_mae: 11.9290\n",
      "Epoch 78/200\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 146.2993 - mae: 9.3792\n",
      "Epoch 78: val_loss did not improve from 247.32477\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 144.5433 - mae: 9.3443 - val_loss: 381.8129 - val_mae: 14.7138\n",
      "Epoch 79/200\n",
      "95/97 [============================>.] - ETA: 0s - loss: 143.0435 - mae: 9.4296\n",
      "Epoch 79: val_loss did not improve from 247.32477\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 143.2556 - mae: 9.4370 - val_loss: 280.6997 - val_mae: 11.7517\n",
      "Epoch 80/200\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 133.3791 - mae: 9.0961\n",
      "Epoch 80: val_loss did not improve from 247.32477\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 131.8195 - mae: 9.0199 - val_loss: 263.0675 - val_mae: 11.3261\n",
      "Epoch 81/200\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 130.9275 - mae: 8.8416\n",
      "Epoch 81: val_loss did not improve from 247.32477\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 131.3540 - mae: 8.8711 - val_loss: 254.6857 - val_mae: 11.1964\n",
      "Epoch 82/200\n",
      "92/97 [===========================>..] - ETA: 0s - loss: 139.1522 - mae: 9.3549\n",
      "Epoch 82: val_loss did not improve from 247.32477\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 139.7675 - mae: 9.3635 - val_loss: 272.0596 - val_mae: 11.9566\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일: 모델학습을 위한 학습과정 설정단계\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "# print(\"\\n Accuracy: %.4f\" % model.evaluate(x_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2699a0c43a0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_feature)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(test_label, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7adfe9f3775c954d01fc005199cfce03b4193bbc6ff21451e4d1eaf7785b4e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
