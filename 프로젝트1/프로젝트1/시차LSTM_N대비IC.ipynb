{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>종가_ex</th>\n",
       "      <th>1Y_Mid_irs</th>\n",
       "      <th>2Y_Mid_irs</th>\n",
       "      <th>3Y_Mid_irs</th>\n",
       "      <th>5Y_Mid_irs</th>\n",
       "      <th>10Y_Mid_irs</th>\n",
       "      <th>1Y_Mid_crs</th>\n",
       "      <th>2Y_Mid_crs</th>\n",
       "      <th>3Y_Mid_crs</th>\n",
       "      <th>...</th>\n",
       "      <th>국고10년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>통안2년대비</th>\n",
       "      <th>전일비_1Y_베이시스</th>\n",
       "      <th>전일비_2Y_베이시스</th>\n",
       "      <th>전일비_3Y_베이시스</th>\n",
       "      <th>전일비_5Y_베이시스</th>\n",
       "      <th>전일비_10Y_베이시스</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_NDF차이</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>2.820</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.860</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.845</td>\n",
       "      <td>1.85</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1126.5</td>\n",
       "      <td>-7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>2</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>2.790</td>\n",
       "      <td>2.660</td>\n",
       "      <td>2.660</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.840</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.830</td>\n",
       "      <td>1.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>-6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>3</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>2.810</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.710</td>\n",
       "      <td>2.850</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.805</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>4</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>2.820</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.870</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.820</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>5</td>\n",
       "      <td>1128.3</td>\n",
       "      <td>2.830</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.740</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.820</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>-1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>2455</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.235</td>\n",
       "      <td>3.205</td>\n",
       "      <td>3.125</td>\n",
       "      <td>2.965</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.730</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>2456</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>3.155</td>\n",
       "      <td>3.215</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.095</td>\n",
       "      <td>2.935</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.68</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>2457</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>3.145</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.115</td>\n",
       "      <td>3.035</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>-2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>2458</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.205</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.085</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.730</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>7.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>2459</td>\n",
       "      <td>1299.1</td>\n",
       "      <td>3.105</td>\n",
       "      <td>3.065</td>\n",
       "      <td>3.025</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.825</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.560</td>\n",
       "      <td>2.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0   종가_ex  1Y_Mid_irs  2Y_Mid_irs  3Y_Mid_irs  \\\n",
       "DateTime                                                             \n",
       "2012-08-02           1  1131.7       2.820       2.690       2.690   \n",
       "2012-08-03           2  1134.8       2.790       2.660       2.660   \n",
       "2012-08-06           3  1129.0       2.810       2.680       2.680   \n",
       "2012-08-07           4  1128.8       2.820       2.680       2.680   \n",
       "2012-08-08           5  1128.3       2.830       2.700       2.700   \n",
       "...                ...     ...         ...         ...         ...   \n",
       "2022-07-25        2455  1313.7       3.165       3.235       3.205   \n",
       "2022-07-26        2456  1307.6       3.155       3.215       3.175   \n",
       "2022-07-27        2457  1313.3       3.145       3.165       3.115   \n",
       "2022-07-28        2458  1296.1       3.175       3.205       3.165   \n",
       "2022-07-29        2459  1299.1       3.105       3.065       3.025   \n",
       "\n",
       "            5Y_Mid_irs  10Y_Mid_irs  1Y_Mid_crs  2Y_Mid_crs  3Y_Mid_crs  ...  \\\n",
       "DateTime                                                                 ...   \n",
       "2012-08-02       2.720        2.860        2.08       1.845        1.85  ...   \n",
       "2012-08-03       2.690        2.840        2.07       1.830        1.83  ...   \n",
       "2012-08-06       2.710        2.850        2.07       1.805        1.80  ...   \n",
       "2012-08-07       2.720        2.870        2.09       1.820        1.80  ...   \n",
       "2012-08-08       2.740        2.900        2.10       1.820        1.80  ...   \n",
       "...                ...          ...         ...         ...         ...  ...   \n",
       "2022-07-25       3.125        2.965        2.55       2.730        2.71  ...   \n",
       "2022-07-26       3.095        2.935        2.56       2.700        2.68  ...   \n",
       "2022-07-27       3.035        2.875        2.57       2.690        2.67  ...   \n",
       "2022-07-28       3.085        2.945        2.61       2.730        2.71  ...   \n",
       "2022-07-29       2.945        2.825        2.50       2.560        2.56  ...   \n",
       "\n",
       "            국고10년대비  통안1년대비  통안2년대비  전일비_1Y_베이시스  전일비_2Y_베이시스  전일비_3Y_베이시스  \\\n",
       "DateTime                                                                     \n",
       "2012-08-02    -0.21   -0.03    0.02          2.0          8.0          9.0   \n",
       "2012-08-03    -0.03   -0.03    0.00          2.0          1.5          1.0   \n",
       "2012-08-06    -0.03   -0.01    0.02         -2.0         -4.5         -5.0   \n",
       "2012-08-07    -0.04   -0.01   -0.01          1.0          1.5          0.0   \n",
       "2012-08-08    -0.04   -0.06   -0.03          0.0         -2.0         -2.0   \n",
       "...             ...     ...     ...          ...          ...          ...   \n",
       "2022-07-25    -0.11    0.05    0.00         -4.0         -1.0          0.0   \n",
       "2022-07-26    -0.08    0.01   -0.05          2.0         -1.0          0.0   \n",
       "2022-07-27     0.01   -0.01   -0.02          2.0          4.0          5.0   \n",
       "2022-07-28     0.06    0.00    0.00          1.0          0.0         -1.0   \n",
       "2022-07-29     0.02    0.12    0.01         -4.0         -3.0         -1.0   \n",
       "\n",
       "            전일비_5Y_베이시스  전일비_10Y_베이시스  전날 종가_ex  종가_NDF차이  \n",
       "DateTime                                                   \n",
       "2012-08-02          9.0           9.0    1126.5     -7.50  \n",
       "2012-08-03         -5.0         -13.0    1131.7     -6.30  \n",
       "2012-08-06         -6.0          -5.0    1134.8      6.30  \n",
       "2012-08-07         -8.0         -10.0    1129.0      0.00  \n",
       "2012-08-08         -4.0          -7.0    1128.8     -1.45  \n",
       "...                 ...           ...       ...       ...  \n",
       "2022-07-25         -2.0           0.0    1313.0      3.15  \n",
       "2022-07-26          1.0           1.0    1313.7      2.70  \n",
       "2022-07-27          5.0           5.0    1307.6     -2.90  \n",
       "2022-07-28         -2.0          -5.0    1313.3      7.30  \n",
       "2022-07-29          5.0           3.0    1296.1      0.35  \n",
       "\n",
       "[2459 rows x 51 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 파일 불러오기\n",
    "df = pd.read_excel(\"./xlsx/시차상관분석4Data.xlsx\", index_col = 0)    \n",
    "\n",
    "\n",
    "df = df.set_index(\"DateTime\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', '종가_ex', '1Y_Mid_irs', '2Y_Mid_irs', '3Y_Mid_irs',\n",
       "       '5Y_Mid_irs', '10Y_Mid_irs', '1Y_Mid_crs', '2Y_Mid_crs', '3Y_Mid_crs',\n",
       "       '5Y_Mid_crs', '10Y_Mid_crs', '국고1년', '국고3년', '국고5년', '국고10년', '통안364일',\n",
       "       '통안2년', 'Mid_ndf', '전일비_ndf', '1Y_베이시스', '2Y_베이시스', '3Y_베이시스',\n",
       "       '5Y_베이시스', '10Y_베이시스', 'M1_스왑포인트', '전일대비_종가_ex', '등락률_종가_ex',\n",
       "       '전일비_1Y_irs', '전일비_2Y_irs', '전일비_3Y_irs', '전일비_5Y_irs', '전일비_10Y_irs',\n",
       "       '전일비_1Y_crs', '전일비_2Y_crs', '전일비_3Y_crs', '전일비_5Y_crs', '전일비_10Y_crs',\n",
       "       '국고1년대비', '국고3년대비', '국고5년대비', '국고10년대비', '통안1년대비', '통안2년대비',\n",
       "       '전일비_1Y_베이시스', '전일비_2Y_베이시스', '전일비_3Y_베이시스', '전일비_5Y_베이시스',\n",
       "       '전일비_10Y_베이시스', '전날 종가_ex', '종가_NDF차이'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['종가_NDF차이'] = df['전날 종가_ex'] - df['Mid_ndf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전일비_1Y_irs</th>\n",
       "      <th>전일비_3Y_irs</th>\n",
       "      <th>전일비_10Y_irs</th>\n",
       "      <th>전일비_1Y_crs</th>\n",
       "      <th>전일비_3Y_crs</th>\n",
       "      <th>국고3년대비</th>\n",
       "      <th>국고5년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>통안2년대비</th>\n",
       "      <th>전일비_ndf</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>2.032691</td>\n",
       "      <td>-0.205655</td>\n",
       "      <td>0.905480</td>\n",
       "      <td>-0.488709</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>0.267777</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>-0.149841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-1.136944</td>\n",
       "      <td>1.210863</td>\n",
       "      <td>-0.159384</td>\n",
       "      <td>-0.205655</td>\n",
       "      <td>-0.463176</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.184972</td>\n",
       "      <td>-0.056232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>3.338756</td>\n",
       "      <td>2.032691</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>-0.691285</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>2.466603</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>0.267777</td>\n",
       "      <td>-0.449862</td>\n",
       "      <td>-0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.089516</td>\n",
       "      <td>0.936654</td>\n",
       "      <td>0.401191</td>\n",
       "      <td>-0.006957</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>-0.504502</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>-0.111668</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>-0.104837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>1.040201</td>\n",
       "      <td>-0.089516</td>\n",
       "      <td>-1.255421</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.006957</td>\n",
       "      <td>-0.324448</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.650470</td>\n",
       "      <td>-0.364632</td>\n",
       "      <td>0.055654</td>\n",
       "      <td>-0.108437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>0.388635</td>\n",
       "      <td>-1.621628</td>\n",
       "      <td>-1.603722</td>\n",
       "      <td>1.975212</td>\n",
       "      <td>-0.234402</td>\n",
       "      <td>0.541331</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.154406</td>\n",
       "      <td>3.207485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>0.265133</td>\n",
       "      <td>-0.342057</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.691285</td>\n",
       "      <td>0.825382</td>\n",
       "      <td>-1.314804</td>\n",
       "      <td>0.107949</td>\n",
       "      <td>-0.617595</td>\n",
       "      <td>0.050952</td>\n",
       "      <td>3.220086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>-0.890075</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.235067</td>\n",
       "      <td>0.661120</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>-0.238150</td>\n",
       "      <td>-0.026639</td>\n",
       "      <td>3.110275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>0.571308</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.905480</td>\n",
       "      <td>1.153905</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>-0.214738</td>\n",
       "      <td>3.212885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>1.185344</td>\n",
       "      <td>0.028701</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>-2.228474</td>\n",
       "      <td>-3.428596</td>\n",
       "      <td>-0.652971</td>\n",
       "      <td>0.305799</td>\n",
       "      <td>1.299750</td>\n",
       "      <td>0.141295</td>\n",
       "      <td>-0.485131</td>\n",
       "      <td>2.903255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            전일비_1Y_irs  전일비_3Y_irs  전일비_10Y_irs  전일비_1Y_crs  전일비_3Y_crs  \\\n",
       "DateTime                                                                  \n",
       "2012-08-02    0.604772   -0.207732     2.032691   -0.205655    0.905480   \n",
       "2012-08-03   -1.136944    1.210863    -0.159384   -0.205655   -0.463176   \n",
       "2012-08-06    0.604772    3.338756     2.032691   -0.003373   -0.691285   \n",
       "2012-08-07   -0.266086   -0.089516     0.936654    0.401191   -0.006957   \n",
       "2012-08-08    1.040201   -0.089516    -1.255421    0.198909   -0.006957   \n",
       "...                ...         ...          ...         ...         ...   \n",
       "2022-07-25   -0.266086   -0.207732     0.388635   -1.621628   -1.603722   \n",
       "2022-07-26    0.604772    0.265133    -0.342057    0.198909   -0.691285   \n",
       "2022-07-27   -0.266086   -0.207732    -0.890075    0.198909   -0.235067   \n",
       "2022-07-28    0.604772   -0.207732     0.571308    0.805755    0.905480   \n",
       "2022-07-29    1.185344    0.028701     0.023289   -2.228474   -3.428596   \n",
       "\n",
       "              국고3년대비    국고5년대비    통안1년대비    통안2년대비   전일비_ndf  전날 종가_ex  \n",
       "DateTime                                                                \n",
       "2012-08-02 -0.488709 -1.584904 -0.325433  0.267777  0.079167 -0.149841  \n",
       "2012-08-03  0.004075  0.035699 -0.325433  0.014814  0.184972 -0.056232  \n",
       "2012-08-06 -0.160187  2.466603 -0.108742  0.267777 -0.449862 -0.000426  \n",
       "2012-08-07 -0.160187 -0.504502 -0.108742 -0.111668  0.020386 -0.104837  \n",
       "2012-08-08 -0.324448  0.035699 -0.650470 -0.364632  0.055654 -0.108437  \n",
       "...              ...       ...       ...       ...       ...       ...  \n",
       "2022-07-25  1.975212 -0.234402  0.541331  0.014814  0.154406  3.207485  \n",
       "2022-07-26  0.825382 -1.314804  0.107949 -0.617595  0.050952  3.220086  \n",
       "2022-07-27  0.661120 -1.584904 -0.108742 -0.238150 -0.026639  3.110275  \n",
       "2022-07-28  1.153905  0.035699 -0.000397  0.014814 -0.214738  3.212885  \n",
       "2022-07-29 -0.652971  0.305799  1.299750  0.141295 -0.485131  2.903255  \n",
       "\n",
       "[2459 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 쓸 칼럼만 남기고 feature, target 분리해 각각 x,y 에 저장\n",
    "x = df[[ '전일비_1Y_irs','전일비_3Y_irs', '전일비_10Y_irs', \n",
    "          '전일비_1Y_crs', '전일비_3Y_crs',        \n",
    "           '국고3년대비', '국고5년대비', '통안1년대비', '통안2년대비',\n",
    "           '전일비_ndf', '전날 종가_ex']]\n",
    "y = df[['종가_ex']]\n",
    "\n",
    "# 이건 이렇게 해야 밑에 코드 8번째 줄 columns에 들어갈 수 있다고 하네요!\n",
    "x.feature = x.columns \n",
    "x.feature\n",
    "\n",
    "# scaling 진행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전일비_1Y_irs</th>\n",
       "      <th>전일비_3Y_irs</th>\n",
       "      <th>전일비_10Y_irs</th>\n",
       "      <th>전일비_1Y_crs</th>\n",
       "      <th>전일비_3Y_crs</th>\n",
       "      <th>국고3년대비</th>\n",
       "      <th>국고5년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>통안2년대비</th>\n",
       "      <th>전일비_ndf</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>2.032691</td>\n",
       "      <td>-0.205655</td>\n",
       "      <td>0.905480</td>\n",
       "      <td>-0.488709</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>0.267777</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>1131.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-1.136944</td>\n",
       "      <td>1.210863</td>\n",
       "      <td>-0.159384</td>\n",
       "      <td>-0.205655</td>\n",
       "      <td>-0.463176</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.184972</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>3.338756</td>\n",
       "      <td>2.032691</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>-0.691285</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>2.466603</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>0.267777</td>\n",
       "      <td>-0.449862</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.089516</td>\n",
       "      <td>0.936654</td>\n",
       "      <td>0.401191</td>\n",
       "      <td>-0.006957</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>-0.504502</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>-0.111668</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>1.040201</td>\n",
       "      <td>-0.089516</td>\n",
       "      <td>-1.255421</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.006957</td>\n",
       "      <td>-0.324448</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.650470</td>\n",
       "      <td>-0.364632</td>\n",
       "      <td>0.055654</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>0.388635</td>\n",
       "      <td>-1.621628</td>\n",
       "      <td>-1.603722</td>\n",
       "      <td>1.975212</td>\n",
       "      <td>-0.234402</td>\n",
       "      <td>0.541331</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.154406</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>0.265133</td>\n",
       "      <td>-0.342057</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.691285</td>\n",
       "      <td>0.825382</td>\n",
       "      <td>-1.314804</td>\n",
       "      <td>0.107949</td>\n",
       "      <td>-0.617595</td>\n",
       "      <td>0.050952</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>-0.266086</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>-0.890075</td>\n",
       "      <td>0.198909</td>\n",
       "      <td>-0.235067</td>\n",
       "      <td>0.661120</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>-0.238150</td>\n",
       "      <td>-0.026639</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.604772</td>\n",
       "      <td>-0.207732</td>\n",
       "      <td>0.571308</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.905480</td>\n",
       "      <td>1.153905</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>-0.214738</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>1.185344</td>\n",
       "      <td>0.028701</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>-2.228474</td>\n",
       "      <td>-3.428596</td>\n",
       "      <td>-0.652971</td>\n",
       "      <td>0.305799</td>\n",
       "      <td>1.299750</td>\n",
       "      <td>0.141295</td>\n",
       "      <td>-0.485131</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            전일비_1Y_irs  전일비_3Y_irs  전일비_10Y_irs  전일비_1Y_crs  전일비_3Y_crs  \\\n",
       "DateTime                                                                  \n",
       "2012-08-02    0.604772   -0.207732     2.032691   -0.205655    0.905480   \n",
       "2012-08-03   -1.136944    1.210863    -0.159384   -0.205655   -0.463176   \n",
       "2012-08-06    0.604772    3.338756     2.032691   -0.003373   -0.691285   \n",
       "2012-08-07   -0.266086   -0.089516     0.936654    0.401191   -0.006957   \n",
       "2012-08-08    1.040201   -0.089516    -1.255421    0.198909   -0.006957   \n",
       "...                ...         ...          ...         ...         ...   \n",
       "2022-07-25   -0.266086   -0.207732     0.388635   -1.621628   -1.603722   \n",
       "2022-07-26    0.604772    0.265133    -0.342057    0.198909   -0.691285   \n",
       "2022-07-27   -0.266086   -0.207732    -0.890075    0.198909   -0.235067   \n",
       "2022-07-28    0.604772   -0.207732     0.571308    0.805755    0.905480   \n",
       "2022-07-29    1.185344    0.028701     0.023289   -2.228474   -3.428596   \n",
       "\n",
       "              국고3년대비    국고5년대비    통안1년대비    통안2년대비   전일비_ndf  전날 종가_ex   종가_ex  \n",
       "DateTime                                                                        \n",
       "2012-08-02 -0.488709 -1.584904 -0.325433  0.267777  0.079167 -0.149841  1131.7  \n",
       "2012-08-03  0.004075  0.035699 -0.325433  0.014814  0.184972 -0.056232  1134.8  \n",
       "2012-08-06 -0.160187  2.466603 -0.108742  0.267777 -0.449862 -0.000426  1129.0  \n",
       "2012-08-07 -0.160187 -0.504502 -0.108742 -0.111668  0.020386 -0.104837  1128.8  \n",
       "2012-08-08 -0.324448  0.035699 -0.650470 -0.364632  0.055654 -0.108437  1128.3  \n",
       "...              ...       ...       ...       ...       ...       ...     ...  \n",
       "2022-07-25  1.975212 -0.234402  0.541331  0.014814  0.154406  3.207485  1313.7  \n",
       "2022-07-26  0.825382 -1.314804  0.107949 -0.617595  0.050952  3.220086  1307.6  \n",
       "2022-07-27  0.661120 -1.584904 -0.108742 -0.238150 -0.026639  3.110275  1313.3  \n",
       "2022-07-28  1.153905  0.035699 -0.000397  0.014814 -0.214738  3.212885  1296.1  \n",
       "2022-07-29 -0.652971  0.305799  1.299750  0.141295 -0.485131  2.903255  1299.1  \n",
       "\n",
       "[2459 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = pd.concat([x_scaled,y], axis=1)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_scaled[0:1945]\n",
    "test = df_scaled[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=1):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['전일비_1Y_irs', '전일비_10Y_irs', \n",
    "          '전일비_1Y_crs', '전일비_3Y_crs',        \n",
    "           '국고3년대비', '국고5년대비',\n",
    "           '전일비_ndf', '전날 종가_ex']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1555, 1, 8), (389, 1, 8))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.26608569,  0.93665369,  0.19890922, ..., -0.77460293,\n",
       "          0.18967496, -0.80510453]],\n",
       "\n",
       "       [[ 0.89505842,  0.38863495,  2.01944682, ..., -0.77460293,\n",
       "          0.37307165, -1.03732707]],\n",
       "\n",
       "       [[ 0.02420034, -0.61606607, -0.61021861, ..., -0.50450245,\n",
       "          0.21788983,  0.67823943]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.55637171,  0.57130787, -0.81250056, ..., -0.77460293,\n",
       "         -0.37932505, -0.49907483]],\n",
       "\n",
       "       [[ 1.47563047, -0.3420567 , -0.00337274, ...,  0.84599997,\n",
       "         -0.1394986 , -1.10393354]],\n",
       "\n",
       "       [[ 0.02420034, -0.70740253, -0.40793665, ..., -0.23440197,\n",
       "         -0.26176307, -0.02382871]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((513, 1, 8), (513, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 1)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1267293.5000 - mae: 1124.6080\n",
      "Epoch 1: val_loss improved from inf to 1260385.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 13s 95ms/step - loss: 1268421.1250 - mae: 1125.0865 - val_loss: 1260385.7500 - val_mae: 1121.6221\n",
      "Epoch 2/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1267976.3750 - mae: 1124.8873\n",
      "Epoch 2: val_loss improved from 1260385.75000 to 1259442.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1267672.8750 - mae: 1124.7527 - val_loss: 1259442.5000 - val_mae: 1121.2008\n",
      "Epoch 3/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1266895.8750 - mae: 1124.4093\n",
      "Epoch 3: val_loss improved from 1259442.50000 to 1257525.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 1266235.1250 - mae: 1124.1106 - val_loss: 1257525.5000 - val_mae: 1120.3411\n",
      "Epoch 4/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1263253.7500 - mae: 1122.7744\n",
      "Epoch 4: val_loss improved from 1257525.50000 to 1253961.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1263429.3750 - mae: 1122.8495 - val_loss: 1253961.1250 - val_mae: 1118.7344\n",
      "Epoch 5/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1258688.8750 - mae: 1120.7131\n",
      "Epoch 5: val_loss improved from 1253961.12500 to 1248576.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1258814.1250 - mae: 1120.7690 - val_loss: 1248576.6250 - val_mae: 1116.2960\n",
      "Epoch 6/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1253231.0000 - mae: 1118.2406\n",
      "Epoch 6: val_loss improved from 1248576.62500 to 1241173.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1252264.6250 - mae: 1117.8029 - val_loss: 1241173.7500 - val_mae: 1112.9314\n",
      "Epoch 7/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1244765.5000 - mae: 1114.3923\n",
      "Epoch 7: val_loss improved from 1241173.75000 to 1231725.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1243594.1250 - mae: 1113.8588 - val_loss: 1231725.3750 - val_mae: 1108.6154\n",
      "Epoch 8/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 1234583.1250 - mae: 1109.7671\n",
      "Epoch 8: val_loss improved from 1231725.37500 to 1220608.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1233096.8750 - mae: 1109.0615 - val_loss: 1220608.7500 - val_mae: 1103.5088\n",
      "Epoch 9/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1222391.0000 - mae: 1104.1248\n",
      "Epoch 9: val_loss improved from 1220608.75000 to 1207963.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1220936.7500 - mae: 1103.4700 - val_loss: 1207963.7500 - val_mae: 1097.6646\n",
      "Epoch 10/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1207365.7500 - mae: 1097.1794\n",
      "Epoch 10: val_loss improved from 1207963.75000 to 1193935.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1207296.2500 - mae: 1097.1533 - val_loss: 1193935.7500 - val_mae: 1091.1305\n",
      "Epoch 11/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1192422.1250 - mae: 1090.2118\n",
      "Epoch 11: val_loss improved from 1193935.75000 to 1178729.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1192422.1250 - mae: 1090.2118 - val_loss: 1178729.1250 - val_mae: 1083.9935\n",
      "Epoch 12/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 1176846.6250 - mae: 1082.8763\n",
      "Epoch 12: val_loss improved from 1178729.12500 to 1162313.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 1176335.5000 - mae: 1082.6417 - val_loss: 1162313.3750 - val_mae: 1076.2140\n",
      "Epoch 13/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1158404.7500 - mae: 1074.1246\n",
      "Epoch 13: val_loss improved from 1162313.37500 to 1145059.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 1159297.2500 - mae: 1074.5508 - val_loss: 1145059.0000 - val_mae: 1067.9641\n",
      "Epoch 14/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1142832.3750 - mae: 1066.6414\n",
      "Epoch 14: val_loss improved from 1145059.00000 to 1126999.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1141396.8750 - mae: 1065.9661 - val_loss: 1126999.5000 - val_mae: 1059.2437\n",
      "Epoch 15/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1121980.1250 - mae: 1056.5657\n",
      "Epoch 15: val_loss improved from 1126999.50000 to 1108092.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1122695.7500 - mae: 1056.8977 - val_loss: 1108092.7500 - val_mae: 1050.0122\n",
      "Epoch 16/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 1104700.3750 - mae: 1048.0310\n",
      "Epoch 16: val_loss improved from 1108092.75000 to 1088475.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1103199.1250 - mae: 1047.3424 - val_loss: 1088475.2500 - val_mae: 1040.3218\n",
      "Epoch 17/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1082710.3750 - mae: 1037.1658\n",
      "Epoch 17: val_loss improved from 1088475.25000 to 1068237.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1083097.8750 - mae: 1037.3804 - val_loss: 1068237.3750 - val_mae: 1030.2052\n",
      "Epoch 18/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1061653.3750 - mae: 1026.6067\n",
      "Epoch 18: val_loss improved from 1068237.37500 to 1047502.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1062444.2500 - mae: 1027.0015 - val_loss: 1047502.6250 - val_mae: 1019.7073\n",
      "Epoch 19/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1041963.1250 - mae: 1016.6066\n",
      "Epoch 19: val_loss improved from 1047502.62500 to 1026316.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1041285.7500 - mae: 1016.2361 - val_loss: 1026316.7500 - val_mae: 1008.8364\n",
      "Epoch 20/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1017950.0625 - mae: 1004.3559\n",
      "Epoch 20: val_loss improved from 1026316.75000 to 1004682.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1019728.0000 - mae: 1005.1017 - val_loss: 1004682.6250 - val_mae: 997.5746\n",
      "Epoch 21/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 997586.7500 - mae: 993.5406 \n",
      "Epoch 21: val_loss improved from 1004682.62500 to 982900.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 997877.8750 - mae: 993.6813 - val_loss: 982900.8750 - val_mae: 986.0874\n",
      "Epoch 22/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 979014.1250 - mae: 983.7187\n",
      "Epoch 22: val_loss improved from 982900.87500 to 960819.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 975800.3750 - mae: 982.0051 - val_loss: 960819.1875 - val_mae: 974.2657\n",
      "Epoch 23/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 953489.3125 - mae: 970.0237\n",
      "Epoch 23: val_loss improved from 960819.18750 to 938550.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 953489.3125 - mae: 970.0237 - val_loss: 938550.4375 - val_mae: 962.1458\n",
      "Epoch 24/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 929553.5625 - mae: 957.3239\n",
      "Epoch 24: val_loss improved from 938550.43750 to 916074.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 930979.1250 - mae: 957.8661 - val_loss: 916074.7500 - val_mae: 949.7159\n",
      "Epoch 25/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 908422.0625 - mae: 945.3985\n",
      "Epoch 25: val_loss improved from 916074.75000 to 893538.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 908344.0625 - mae: 945.3806 - val_loss: 893538.1250 - val_mae: 937.0581\n",
      "Epoch 26/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 886812.1250 - mae: 933.2987\n",
      "Epoch 26: val_loss improved from 893538.12500 to 871057.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 885696.1875 - mae: 932.7338 - val_loss: 871057.1875 - val_mae: 924.2235\n",
      "Epoch 27/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 863766.5000 - mae: 920.3073\n",
      "Epoch 27: val_loss improved from 871057.18750 to 848507.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 863023.9375 - mae: 919.7984 - val_loss: 848507.8750 - val_mae: 911.1053\n",
      "Epoch 28/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 840402.2500 - mae: 906.6811\n",
      "Epoch 28: val_loss improved from 848507.87500 to 826006.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 840402.2500 - mae: 906.6811 - val_loss: 826006.1250 - val_mae: 897.8005\n",
      "Epoch 29/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 818802.3750 - mae: 893.8565\n",
      "Epoch 29: val_loss improved from 826006.12500 to 803514.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 817788.4375 - mae: 893.2776 - val_loss: 803514.8750 - val_mae: 884.5885\n",
      "Epoch 30/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 793696.8750 - mae: 878.7015\n",
      "Epoch 30: val_loss improved from 803514.87500 to 781257.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 795366.5000 - mae: 879.8806 - val_loss: 781257.8125 - val_mae: 871.2601\n",
      "Epoch 31/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 777975.0000 - mae: 869.6511\n",
      "Epoch 31: val_loss improved from 781257.81250 to 759130.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 773042.5625 - mae: 866.2142 - val_loss: 759130.7500 - val_mae: 857.7653\n",
      "Epoch 32/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 752327.9375 - mae: 853.2228\n",
      "Epoch 32: val_loss improved from 759130.75000 to 737178.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 750922.4375 - mae: 852.4982 - val_loss: 737178.8125 - val_mae: 844.0642\n",
      "Epoch 33/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 729715.9375 - mae: 839.1450\n",
      "Epoch 33: val_loss improved from 737178.81250 to 715493.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 729080.5000 - mae: 838.5939 - val_loss: 715493.5000 - val_mae: 830.3265\n",
      "Epoch 34/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 712947.8125 - mae: 828.3117\n",
      "Epoch 34: val_loss improved from 715493.50000 to 694108.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 707471.5625 - mae: 824.7102 - val_loss: 694108.8750 - val_mae: 816.7291\n",
      "Epoch 35/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 688052.5000 - mae: 811.9074\n",
      "Epoch 35: val_loss improved from 694108.87500 to 672900.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 686146.3750 - mae: 810.7548 - val_loss: 672900.2500 - val_mae: 802.9307\n",
      "Epoch 36/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 665639.5625 - mae: 797.0156\n",
      "Epoch 36: val_loss improved from 672900.25000 to 651999.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 665055.5000 - mae: 796.5667 - val_loss: 651999.2500 - val_mae: 789.0163\n",
      "Epoch 37/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 642850.0000 - mae: 781.6485\n",
      "Epoch 37: val_loss improved from 651999.25000 to 631417.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 644311.4375 - mae: 782.4689 - val_loss: 631417.3125 - val_mae: 775.0039\n",
      "Epoch 38/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 623906.6875 - mae: 768.3082\n",
      "Epoch 38: val_loss improved from 631417.31250 to 611179.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 623906.6875 - mae: 768.3082 - val_loss: 611179.1250 - val_mae: 760.9564\n",
      "Epoch 39/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 602230.8750 - mae: 752.8525\n",
      "Epoch 39: val_loss improved from 611179.12500 to 591200.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 603853.3125 - mae: 753.9506 - val_loss: 591200.4375 - val_mae: 746.7061\n",
      "Epoch 40/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 584154.2500 - mae: 739.6141\n",
      "Epoch 40: val_loss improved from 591200.43750 to 571685.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 584154.2500 - mae: 739.6141 - val_loss: 571685.0000 - val_mae: 732.5498\n",
      "Epoch 41/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 563722.0000 - mae: 724.9518\n",
      "Epoch 41: val_loss improved from 571685.00000 to 552503.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 564854.1875 - mae: 725.3385 - val_loss: 552503.8750 - val_mae: 718.2762\n",
      "Epoch 42/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 544168.4375 - mae: 710.4212\n",
      "Epoch 42: val_loss improved from 552503.87500 to 533551.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 545966.6875 - mae: 711.0109 - val_loss: 533551.2500 - val_mae: 703.8658\n",
      "Epoch 43/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 525799.1875 - mae: 696.2963\n",
      "Epoch 43: val_loss improved from 533551.25000 to 515225.84375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 527427.5625 - mae: 696.6144 - val_loss: 515225.8438 - val_mae: 689.5881\n",
      "Epoch 44/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 509198.2188 - mae: 682.2594\n",
      "Epoch 44: val_loss improved from 515225.84375 to 497313.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 509372.4062 - mae: 682.3309 - val_loss: 497313.8125 - val_mae: 675.3463\n",
      "Epoch 45/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 490798.1250 - mae: 667.5881\n",
      "Epoch 45: val_loss improved from 497313.81250 to 479704.84375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 491698.1250 - mae: 668.0800 - val_loss: 479704.8438 - val_mae: 660.9793\n",
      "Epoch 46/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 477283.7812 - mae: 656.2538\n",
      "Epoch 46: val_loss improved from 479704.84375 to 462554.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 474473.0312 - mae: 653.7308 - val_loss: 462554.1250 - val_mae: 646.7397\n",
      "Epoch 47/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 458506.7500 - mae: 640.2514\n",
      "Epoch 47: val_loss improved from 462554.12500 to 445823.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 457652.4375 - mae: 639.6703 - val_loss: 445823.2188 - val_mae: 632.5848\n",
      "Epoch 48/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 442381.8750 - mae: 626.7725\n",
      "Epoch 48: val_loss improved from 445823.21875 to 429389.65625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 54ms/step - loss: 441226.0938 - mae: 625.5728 - val_loss: 429389.6562 - val_mae: 618.4604\n",
      "Epoch 49/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 426653.5312 - mae: 612.7088\n",
      "Epoch 49: val_loss improved from 429389.65625 to 413396.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 60ms/step - loss: 425252.7500 - mae: 611.4700 - val_loss: 413396.4375 - val_mae: 604.4155\n",
      "Epoch 50/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 411250.1250 - mae: 598.8965\n",
      "Epoch 50: val_loss improved from 413396.43750 to 397864.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 59ms/step - loss: 409730.3125 - mae: 597.4919 - val_loss: 397864.3125 - val_mae: 590.5777\n",
      "Epoch 51/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 392556.5938 - mae: 584.1678\n",
      "Epoch 51: val_loss improved from 397864.31250 to 382786.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 394655.2500 - mae: 583.7698 - val_loss: 382786.8125 - val_mae: 576.8172\n",
      "Epoch 52/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 374592.9062 - mae: 565.0006\n",
      "Epoch 52: val_loss improved from 382786.81250 to 368158.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 60ms/step - loss: 379908.5000 - mae: 570.1360 - val_loss: 368158.0938 - val_mae: 563.2601\n",
      "Epoch 53/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 367108.4062 - mae: 557.2637\n",
      "Epoch 53: val_loss improved from 368158.09375 to 353779.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 365645.6250 - mae: 556.7185 - val_loss: 353779.0938 - val_mae: 549.7883\n",
      "Epoch 54/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 351667.6875 - mae: 543.1563\n",
      "Epoch 54: val_loss improved from 353779.09375 to 339858.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 351788.6562 - mae: 543.2635 - val_loss: 339858.8750 - val_mae: 536.4706\n",
      "Epoch 55/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 339038.7188 - mae: 530.8167\n",
      "Epoch 55: val_loss improved from 339858.87500 to 326395.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 338296.8750 - mae: 530.1392 - val_loss: 326395.6250 - val_mae: 523.3771\n",
      "Epoch 56/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 323023.8438 - mae: 513.8461\n",
      "Epoch 56: val_loss improved from 326395.62500 to 313301.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 325234.5938 - mae: 517.1200 - val_loss: 313301.5625 - val_mae: 510.3664\n",
      "Epoch 57/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 311980.5625 - mae: 503.5499\n",
      "Epoch 57: val_loss improved from 313301.56250 to 300449.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 312517.4688 - mae: 504.5021 - val_loss: 300449.4688 - val_mae: 497.3970\n",
      "Epoch 58/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 302215.8438 - mae: 494.1129\n",
      "Epoch 58: val_loss improved from 300449.46875 to 288107.03125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 300136.7188 - mae: 492.0887 - val_loss: 288107.0312 - val_mae: 484.9096\n",
      "Epoch 59/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 288711.0000 - mae: 480.4721\n",
      "Epoch 59: val_loss improved from 288107.03125 to 276154.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 288168.1875 - mae: 479.9743 - val_loss: 276154.5000 - val_mae: 472.6247\n",
      "Epoch 60/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 277351.9375 - mae: 468.5830\n",
      "Epoch 60: val_loss improved from 276154.50000 to 264560.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 276587.6250 - mae: 468.0008 - val_loss: 264560.4375 - val_mae: 460.5890\n",
      "Epoch 61/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 265578.2500 - mae: 456.3691\n",
      "Epoch 61: val_loss improved from 264560.43750 to 253303.48438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 265353.4375 - mae: 456.1679 - val_loss: 253303.4844 - val_mae: 448.7815\n",
      "Epoch 62/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 254189.9531 - mae: 444.0880\n",
      "Epoch 62: val_loss improved from 253303.48438 to 242367.98438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 60ms/step - loss: 254471.3750 - mae: 444.7754 - val_loss: 242367.9844 - val_mae: 437.1385\n",
      "Epoch 63/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 245114.3438 - mae: 433.7549\n",
      "Epoch 63: val_loss improved from 242367.98438 to 231817.29688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 243897.5156 - mae: 433.5504 - val_loss: 231817.2969 - val_mae: 425.6898\n",
      "Epoch 64/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 237582.2500 - mae: 424.8530\n",
      "Epoch 64: val_loss improved from 231817.29688 to 221470.32812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 233641.4688 - mae: 422.5845 - val_loss: 221470.3281 - val_mae: 414.2935\n",
      "Epoch 65/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 223392.3281 - mae: 411.2271\n",
      "Epoch 65: val_loss improved from 221470.32812 to 211459.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 223692.7969 - mae: 411.6104 - val_loss: 211459.5625 - val_mae: 403.0273\n",
      "Epoch 66/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 215836.7031 - mae: 402.1672\n",
      "Epoch 66: val_loss improved from 211459.56250 to 201809.96875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 214023.4219 - mae: 400.7847 - val_loss: 201809.9688 - val_mae: 392.0040\n",
      "Epoch 67/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 204542.5938 - mae: 389.7594\n",
      "Epoch 67: val_loss improved from 201809.96875 to 192679.48438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 204793.7500 - mae: 390.3637 - val_loss: 192679.4844 - val_mae: 381.3423\n",
      "Epoch 68/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 189548.3281 - mae: 377.7379\n",
      "Epoch 68: val_loss improved from 192679.48438 to 183625.79688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 195739.9531 - mae: 379.9259 - val_loss: 183625.7969 - val_mae: 370.4808\n",
      "Epoch 69/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 188336.2344 - mae: 370.4309\n",
      "Epoch 69: val_loss improved from 183625.79688 to 174854.70312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 187065.6406 - mae: 369.5641 - val_loss: 174854.7031 - val_mae: 359.8223\n",
      "Epoch 70/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 180871.1250 - mae: 360.9119\n",
      "Epoch 70: val_loss improved from 174854.70312 to 166618.64062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 178653.7969 - mae: 359.4077 - val_loss: 166618.6406 - val_mae: 349.5571\n",
      "Epoch 71/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 170297.2969 - mae: 348.6004\n",
      "Epoch 71: val_loss improved from 166618.64062 to 158522.15625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 170658.6250 - mae: 349.3820 - val_loss: 158522.1562 - val_mae: 339.2097\n",
      "Epoch 72/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 165000.2500 - mae: 341.1039\n",
      "Epoch 72: val_loss improved from 158522.15625 to 150831.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 162780.6094 - mae: 339.3606 - val_loss: 150831.9375 - val_mae: 329.1564\n",
      "Epoch 73/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 159017.9688 - mae: 332.7938\n",
      "Epoch 73: val_loss improved from 150831.93750 to 143389.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 54ms/step - loss: 155286.4688 - mae: 329.6054 - val_loss: 143389.3438 - val_mae: 319.0903\n",
      "Epoch 74/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 148473.1406 - mae: 319.6477\n",
      "Epoch 74: val_loss improved from 143389.34375 to 136231.32812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 148027.4844 - mae: 319.9403 - val_loss: 136231.3281 - val_mae: 309.2179\n",
      "Epoch 75/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 140760.8750 - mae: 309.5393\n",
      "Epoch 75: val_loss improved from 136231.32812 to 129372.67188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 141054.7188 - mae: 310.5122 - val_loss: 129372.6719 - val_mae: 299.4092\n",
      "Epoch 76/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 137570.1875 - mae: 304.2000\n",
      "Epoch 76: val_loss improved from 129372.67188 to 122812.44531, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 134403.8438 - mae: 301.2088 - val_loss: 122812.4453 - val_mae: 289.7835\n",
      "Epoch 77/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 126943.7891 - mae: 293.2866\n",
      "Epoch 77: val_loss improved from 122812.44531 to 116464.75781, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 127994.1641 - mae: 292.2959 - val_loss: 116464.7578 - val_mae: 280.2888\n",
      "Epoch 78/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 121826.9688 - mae: 283.4294\n",
      "Epoch 78: val_loss improved from 116464.75781 to 110488.64844, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 121928.7344 - mae: 283.6619 - val_loss: 110488.6484 - val_mae: 271.1308\n",
      "Epoch 79/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 116782.1328 - mae: 274.1344\n",
      "Epoch 79: val_loss improved from 110488.64844 to 104731.40625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 116060.7578 - mae: 274.8526 - val_loss: 104731.4062 - val_mae: 262.0272\n",
      "Epoch 80/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 112690.7891 - mae: 266.2567\n",
      "Epoch 80: val_loss improved from 104731.40625 to 99256.80469, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 110467.6328 - mae: 266.3051 - val_loss: 99256.8047 - val_mae: 253.3147\n",
      "Epoch 81/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 107153.6406 - mae: 258.5023\n",
      "Epoch 81: val_loss improved from 99256.80469 to 93977.49219, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 105108.0312 - mae: 257.8865 - val_loss: 93977.4922 - val_mae: 244.6014\n",
      "Epoch 82/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 101102.8281 - mae: 249.9402\n",
      "Epoch 82: val_loss improved from 93977.49219 to 88965.84375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 100024.1016 - mae: 249.5213 - val_loss: 88965.8438 - val_mae: 236.1410\n",
      "Epoch 83/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 94727.7188 - mae: 241.6420\n",
      "Epoch 83: val_loss improved from 88965.84375 to 84250.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 95196.6484 - mae: 241.3013 - val_loss: 84250.7500 - val_mae: 227.9959\n",
      "Epoch 84/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 91647.5078 - mae: 233.3968\n",
      "Epoch 84: val_loss improved from 84250.75000 to 79768.33594, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 90573.5078 - mae: 233.3577 - val_loss: 79768.3359 - val_mae: 220.0453\n",
      "Epoch 85/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 85173.0938 - mae: 226.2660\n",
      "Epoch 85: val_loss improved from 79768.33594 to 75482.32031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 86165.2344 - mae: 225.3760 - val_loss: 75482.3203 - val_mae: 212.2618\n",
      "Epoch 86/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 81494.1719 - mae: 217.5511\n",
      "Epoch 86: val_loss improved from 75482.32031 to 71461.54688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 82042.8281 - mae: 217.8586 - val_loss: 71461.5469 - val_mae: 204.7353\n",
      "Epoch 87/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 81061.9766 - mae: 212.3942\n",
      "Epoch 87: val_loss improved from 71461.54688 to 67707.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 78122.7344 - mae: 210.3970 - val_loss: 67707.7188 - val_mae: 197.3559\n",
      "Epoch 88/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 72212.8516 - mae: 202.2294\n",
      "Epoch 88: val_loss improved from 67707.71875 to 64066.07031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 74413.9062 - mae: 202.9811 - val_loss: 64066.0703 - val_mae: 190.1414\n",
      "Epoch 89/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 72753.0938 - mae: 196.7543\n",
      "Epoch 89: val_loss improved from 64066.07031 to 60733.88672, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 70900.1016 - mae: 195.8766 - val_loss: 60733.8867 - val_mae: 183.3089\n",
      "Epoch 90/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 69112.1250 - mae: 187.6025\n",
      "Epoch 90: val_loss improved from 60733.88672 to 57500.45703, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 67568.4219 - mae: 188.6055 - val_loss: 57500.4570 - val_mae: 176.4226\n",
      "Epoch 91/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 65356.1406 - mae: 182.1138\n",
      "Epoch 91: val_loss improved from 57500.45703 to 54498.42188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 64361.0820 - mae: 181.9120 - val_loss: 54498.4219 - val_mae: 169.9122\n",
      "Epoch 92/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 60422.8672 - mae: 174.8263\n",
      "Epoch 92: val_loss improved from 54498.42188 to 51699.45312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 57ms/step - loss: 61405.7500 - mae: 175.2065 - val_loss: 51699.4531 - val_mae: 163.5799\n",
      "Epoch 93/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 54826.9883 - mae: 168.0727\n",
      "Epoch 93: val_loss improved from 51699.45312 to 48945.77734, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 58572.7695 - mae: 168.6867 - val_loss: 48945.7773 - val_mae: 157.2225\n",
      "Epoch 94/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 50134.8281 - mae: 160.2101\n",
      "Epoch 94: val_loss improved from 48945.77734 to 46492.02734, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 54ms/step - loss: 55917.9922 - mae: 162.3957 - val_loss: 46492.0273 - val_mae: 151.2760\n",
      "Epoch 95/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 56143.2461 - mae: 158.8778\n",
      "Epoch 95: val_loss improved from 46492.02734 to 44137.16797, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 53452.3633 - mae: 156.9376 - val_loss: 44137.1680 - val_mae: 146.0950\n",
      "Epoch 96/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 48407.4648 - mae: 150.9826\n",
      "Epoch 96: val_loss improved from 44137.16797 to 42042.33594, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 51133.0156 - mae: 150.6804 - val_loss: 42042.3359 - val_mae: 140.3273\n",
      "Epoch 97/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 50969.7812 - mae: 146.5819\n",
      "Epoch 97: val_loss improved from 42042.33594 to 39945.23438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 48964.0703 - mae: 145.2789 - val_loss: 39945.2344 - val_mae: 135.0549\n",
      "Epoch 98/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 48603.2305 - mae: 140.5090\n",
      "Epoch 98: val_loss improved from 39945.23438 to 38108.91406, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 46942.1211 - mae: 139.8410 - val_loss: 38108.9141 - val_mae: 129.9271\n",
      "Epoch 99/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 43166.0586 - mae: 133.6740\n",
      "Epoch 99: val_loss improved from 38108.91406 to 36380.51953, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 45035.3164 - mae: 134.2881 - val_loss: 36380.5195 - val_mae: 124.7915\n",
      "Epoch 100/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 44142.6289 - mae: 129.3087\n",
      "Epoch 100: val_loss improved from 36380.51953 to 34738.42578, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 43244.9609 - mae: 129.2726 - val_loss: 34738.4258 - val_mae: 119.9301\n",
      "Epoch 101/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 45230.2148 - mae: 125.9840\n",
      "Epoch 101: val_loss improved from 34738.42578 to 33128.28906, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 41548.0156 - mae: 124.1718 - val_loss: 33128.2891 - val_mae: 115.0009\n",
      "Epoch 102/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 42118.9531 - mae: 120.6810\n",
      "Epoch 102: val_loss improved from 33128.28906 to 31667.68359, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 39918.2852 - mae: 119.0202 - val_loss: 31667.6836 - val_mae: 110.2946\n",
      "Epoch 103/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 38183.9492 - mae: 114.1516\n",
      "Epoch 103: val_loss improved from 31667.68359 to 30337.20117, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 60ms/step - loss: 38385.3555 - mae: 114.3628 - val_loss: 30337.2012 - val_mae: 105.7974\n",
      "Epoch 104/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 36849.2383 - mae: 109.6452\n",
      "Epoch 104: val_loss improved from 30337.20117 to 28856.79102, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 36849.2383 - mae: 109.6452 - val_loss: 28856.7910 - val_mae: 101.3737\n",
      "Epoch 105/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 35176.8828 - mae: 105.4740\n",
      "Epoch 105: val_loss improved from 28856.79102 to 27398.72266, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 35176.8828 - mae: 105.4740 - val_loss: 27398.7227 - val_mae: 97.3002\n",
      "Epoch 106/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 25062.1348 - mae: 98.1018 \n",
      "Epoch 106: val_loss improved from 27398.72266 to 26143.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 33234.2773 - mae: 100.8407 - val_loss: 26143.4688 - val_mae: 92.7596\n",
      "Epoch 107/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 28437.4043 - mae: 96.2513\n",
      "Epoch 107: val_loss improved from 26143.46875 to 24426.44922, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 31168.8027 - mae: 96.5525 - val_loss: 24426.4492 - val_mae: 88.8278\n",
      "Epoch 108/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 22085.5176 - mae: 89.2174\n",
      "Epoch 108: val_loss improved from 24426.44922 to 22547.01367, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 29394.7832 - mae: 92.3655 - val_loss: 22547.0137 - val_mae: 84.3965\n",
      "Epoch 109/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 27833.7012 - mae: 88.5137\n",
      "Epoch 109: val_loss improved from 22547.01367 to 21012.27930, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 27833.7012 - mae: 88.5137 - val_loss: 21012.2793 - val_mae: 80.4753\n",
      "Epoch 110/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 26262.0879 - mae: 83.9107\n",
      "Epoch 110: val_loss improved from 21012.27930 to 19420.30469, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 26225.4277 - mae: 84.2508 - val_loss: 19420.3047 - val_mae: 76.2810\n",
      "Epoch 111/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 24629.1680 - mae: 80.2794\n",
      "Epoch 111: val_loss improved from 19420.30469 to 18070.99023, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 24629.1680 - mae: 80.2794 - val_loss: 18070.9902 - val_mae: 72.3585\n",
      "Epoch 112/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 23131.2031 - mae: 76.4470\n",
      "Epoch 112: val_loss improved from 18070.99023 to 16588.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 23093.2910 - mae: 76.3888 - val_loss: 16588.0625 - val_mae: 68.5731\n",
      "Epoch 113/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 21286.7441 - mae: 71.5117\n",
      "Epoch 113: val_loss improved from 16588.06250 to 15305.83203, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 21745.8926 - mae: 72.5023 - val_loss: 15305.8320 - val_mae: 64.7864\n",
      "Epoch 114/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 21593.8066 - mae: 69.3974\n",
      "Epoch 114: val_loss improved from 15305.83203 to 14160.40527, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 20676.1211 - mae: 69.0101 - val_loss: 14160.4053 - val_mae: 61.3789\n",
      "Epoch 115/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 21319.7910 - mae: 66.4065\n",
      "Epoch 115: val_loss improved from 14160.40527 to 13059.00391, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 19800.9102 - mae: 65.4175 - val_loss: 13059.0039 - val_mae: 58.0388\n",
      "Epoch 116/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 19338.1055 - mae: 61.9528\n",
      "Epoch 116: val_loss improved from 13059.00391 to 12164.76074, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 18976.4277 - mae: 62.4550 - val_loss: 12164.7607 - val_mae: 54.8922\n",
      "Epoch 117/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 18244.4570 - mae: 59.4859\n",
      "Epoch 117: val_loss improved from 12164.76074 to 11285.29688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 18227.7793 - mae: 59.5346 - val_loss: 11285.2969 - val_mae: 52.0455\n",
      "Epoch 118/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 18013.3398 - mae: 57.2824\n",
      "Epoch 118: val_loss improved from 11285.29688 to 10408.86523, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 17437.4336 - mae: 56.7210 - val_loss: 10408.8652 - val_mae: 49.3593\n",
      "Epoch 119/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 17088.3926 - mae: 54.6790\n",
      "Epoch 119: val_loss improved from 10408.86523 to 9516.00684, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 16611.6641 - mae: 54.2417 - val_loss: 9516.0068 - val_mae: 46.8617\n",
      "Epoch 120/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 16157.0547 - mae: 51.9889\n",
      "Epoch 120: val_loss improved from 9516.00684 to 8771.23145, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 15706.3047 - mae: 51.5655 - val_loss: 8771.2314 - val_mae: 44.5164\n",
      "Epoch 121/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 15293.5898 - mae: 49.0959\n",
      "Epoch 121: val_loss improved from 8771.23145 to 7848.46924, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 14829.8779 - mae: 49.1496 - val_loss: 7848.4692 - val_mae: 42.2848\n",
      "Epoch 122/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 13489.1074 - mae: 46.0852\n",
      "Epoch 122: val_loss improved from 7848.46924 to 7056.40186, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 14076.2637 - mae: 46.8568 - val_loss: 7056.4019 - val_mae: 39.9832\n",
      "Epoch 123/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 13728.9365 - mae: 44.4018\n",
      "Epoch 123: val_loss improved from 7056.40186 to 6385.42432, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 13389.8008 - mae: 44.6338 - val_loss: 6385.4243 - val_mae: 37.8924\n",
      "Epoch 124/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 12725.5732 - mae: 42.3094\n",
      "Epoch 124: val_loss improved from 6385.42432 to 5683.22803, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 12701.9072 - mae: 42.2592 - val_loss: 5683.2280 - val_mae: 35.8077\n",
      "Epoch 125/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 12290.7812 - mae: 40.5182\n",
      "Epoch 125: val_loss improved from 5683.22803 to 5118.16602, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 11964.9854 - mae: 40.2924 - val_loss: 5118.1660 - val_mae: 33.8932\n",
      "Epoch 126/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 11984.2393 - mae: 38.7826\n",
      "Epoch 126: val_loss improved from 5118.16602 to 4618.85400, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 11312.6777 - mae: 38.5158 - val_loss: 4618.8540 - val_mae: 32.1521\n",
      "Epoch 127/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 10775.6445 - mae: 36.4083\n",
      "Epoch 127: val_loss improved from 4618.85400 to 4239.24463, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 10757.1074 - mae: 36.3961 - val_loss: 4239.2446 - val_mae: 30.3191\n",
      "Epoch 128/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 11624.6475 - mae: 36.2531\n",
      "Epoch 128: val_loss improved from 4239.24463 to 3922.87891, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 10300.4971 - mae: 34.9916 - val_loss: 3922.8789 - val_mae: 28.9611\n",
      "Epoch 129/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 10196.8623 - mae: 33.2095\n",
      "Epoch 129: val_loss improved from 3922.87891 to 3674.08813, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 9882.6270 - mae: 33.3282 - val_loss: 3674.0881 - val_mae: 27.5120\n",
      "Epoch 130/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 9178.8906 - mae: 31.4411 \n",
      "Epoch 130: val_loss improved from 3674.08813 to 3464.73169, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 9522.2188 - mae: 31.9225 - val_loss: 3464.7317 - val_mae: 26.3443\n",
      "Epoch 131/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 9766.8350 - mae: 30.9509 \n",
      "Epoch 131: val_loss improved from 3464.73169 to 3262.47778, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 9190.0479 - mae: 30.5677 - val_loss: 3262.4778 - val_mae: 25.2201\n",
      "Epoch 132/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 7338.7485 - mae: 28.6880\n",
      "Epoch 132: val_loss improved from 3262.47778 to 3100.84253, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 8885.5996 - mae: 29.4206 - val_loss: 3100.8425 - val_mae: 24.1888\n",
      "Epoch 133/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 8940.6387 - mae: 29.0141\n",
      "Epoch 133: val_loss improved from 3100.84253 to 2947.85669, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 8598.3252 - mae: 28.5716 - val_loss: 2947.8567 - val_mae: 23.3055\n",
      "Epoch 134/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 8335.2949 - mae: 27.7930\n",
      "Epoch 134: val_loss improved from 2947.85669 to 2812.50781, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 8335.2949 - mae: 27.7930 - val_loss: 2812.5078 - val_mae: 22.2809\n",
      "Epoch 135/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 8090.6025 - mae: 26.8289\n",
      "Epoch 135: val_loss improved from 2812.50781 to 2674.75879, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 8076.7095 - mae: 26.8299 - val_loss: 2674.7588 - val_mae: 21.5001\n",
      "Epoch 136/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 7948.1831 - mae: 25.8363\n",
      "Epoch 136: val_loss improved from 2674.75879 to 2566.68018, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 7832.3711 - mae: 25.8131 - val_loss: 2566.6802 - val_mae: 20.7884\n",
      "Epoch 137/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 8432.4951 - mae: 26.1921 \n",
      "Epoch 137: val_loss improved from 2566.68018 to 2452.53491, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 7591.6533 - mae: 25.3726 - val_loss: 2452.5349 - val_mae: 20.1121\n",
      "Epoch 138/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 7359.0894 - mae: 24.1881\n",
      "Epoch 138: val_loss improved from 2452.53491 to 2358.31763, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 7367.6211 - mae: 24.5376 - val_loss: 2358.3176 - val_mae: 19.3757\n",
      "Epoch 139/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 7543.4746 - mae: 24.1291\n",
      "Epoch 139: val_loss improved from 2358.31763 to 2261.68701, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 7144.3955 - mae: 24.1131 - val_loss: 2261.6870 - val_mae: 18.8567\n",
      "Epoch 140/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 7000.8306 - mae: 23.5432\n",
      "Epoch 140: val_loss improved from 2261.68701 to 2173.02832, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 6925.7642 - mae: 23.5047 - val_loss: 2173.0283 - val_mae: 18.3424\n",
      "Epoch 141/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 6933.3672 - mae: 23.2974\n",
      "Epoch 141: val_loss improved from 2173.02832 to 2095.82080, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 6717.3667 - mae: 22.9686 - val_loss: 2095.8208 - val_mae: 17.9534\n",
      "Epoch 142/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 5243.0234 - mae: 21.5342\n",
      "Epoch 142: val_loss improved from 2095.82080 to 2026.19922, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 6516.4868 - mae: 22.3679 - val_loss: 2026.1992 - val_mae: 17.5224\n",
      "Epoch 143/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 6527.7476 - mae: 22.2814\n",
      "Epoch 143: val_loss improved from 2026.19922 to 1956.99280, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 6324.7388 - mae: 22.0053 - val_loss: 1956.9928 - val_mae: 17.1964\n",
      "Epoch 144/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 6288.8857 - mae: 21.3804\n",
      "Epoch 144: val_loss improved from 1956.99280 to 1902.35217, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 6135.8564 - mae: 21.3956 - val_loss: 1902.3522 - val_mae: 16.8323\n",
      "Epoch 145/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 6157.4912 - mae: 21.5176\n",
      "Epoch 145: val_loss improved from 1902.35217 to 1839.16736, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 60ms/step - loss: 5969.6240 - mae: 21.3194 - val_loss: 1839.1674 - val_mae: 16.7042\n",
      "Epoch 146/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 5785.2339 - mae: 20.6928\n",
      "Epoch 146: val_loss improved from 1839.16736 to 1788.94104, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 5774.1260 - mae: 20.6625 - val_loss: 1788.9410 - val_mae: 16.4905\n",
      "Epoch 147/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 3730.1045 - mae: 19.5201\n",
      "Epoch 147: val_loss improved from 1788.94104 to 1749.69812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 5602.2979 - mae: 20.3269 - val_loss: 1749.6981 - val_mae: 16.2711\n",
      "Epoch 148/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 5596.5552 - mae: 20.0853\n",
      "Epoch 148: val_loss improved from 1749.69812 to 1690.06030, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 5446.9248 - mae: 20.1483 - val_loss: 1690.0603 - val_mae: 16.2478\n",
      "Epoch 149/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 5330.3848 - mae: 19.7702\n",
      "Epoch 149: val_loss improved from 1690.06030 to 1647.02454, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 5281.7471 - mae: 19.8262 - val_loss: 1647.0245 - val_mae: 16.0509\n",
      "Epoch 150/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 5285.5767 - mae: 19.5772\n",
      "Epoch 150: val_loss improved from 1647.02454 to 1609.36841, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 5123.8662 - mae: 19.4866 - val_loss: 1609.3684 - val_mae: 15.9957\n",
      "Epoch 151/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 5323.1499 - mae: 19.4441\n",
      "Epoch 151: val_loss improved from 1609.36841 to 1571.45715, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 4964.4697 - mae: 19.2811 - val_loss: 1571.4572 - val_mae: 15.8329\n",
      "Epoch 152/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 5071.4287 - mae: 19.0722\n",
      "Epoch 152: val_loss improved from 1571.45715 to 1542.86523, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 4826.4487 - mae: 18.9182 - val_loss: 1542.8652 - val_mae: 15.7855\n",
      "Epoch 153/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 4786.6904 - mae: 18.4252\n",
      "Epoch 153: val_loss improved from 1542.86523 to 1505.51184, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 4684.4878 - mae: 18.5852 - val_loss: 1505.5118 - val_mae: 15.6247\n",
      "Epoch 154/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 4882.9907 - mae: 18.6458\n",
      "Epoch 154: val_loss improved from 1505.51184 to 1480.24304, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 4543.1636 - mae: 18.4472 - val_loss: 1480.2430 - val_mae: 15.5839\n",
      "Epoch 155/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 4657.9585 - mae: 18.0317\n",
      "Epoch 155: val_loss improved from 1480.24304 to 1451.99377, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 4409.9326 - mae: 18.1218 - val_loss: 1451.9938 - val_mae: 15.4908\n",
      "Epoch 156/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 4534.7578 - mae: 17.8678\n",
      "Epoch 156: val_loss improved from 1451.99377 to 1425.07507, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 4279.9990 - mae: 17.8219 - val_loss: 1425.0751 - val_mae: 15.3696\n",
      "Epoch 157/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 4436.4961 - mae: 18.0369\n",
      "Epoch 157: val_loss improved from 1425.07507 to 1402.90259, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 4172.9824 - mae: 17.6946 - val_loss: 1402.9026 - val_mae: 15.3218\n",
      "Epoch 158/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 4208.3198 - mae: 17.5455\n",
      "Epoch 158: val_loss improved from 1402.90259 to 1378.04053, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 4037.6248 - mae: 17.3812 - val_loss: 1378.0405 - val_mae: 15.1750\n",
      "Epoch 159/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 4589.0459 - mae: 17.8837\n",
      "Epoch 159: val_loss improved from 1378.04053 to 1354.57129, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 3928.3823 - mae: 17.1129 - val_loss: 1354.5713 - val_mae: 15.0884\n",
      "Epoch 160/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 2296.3691 - mae: 16.1289\n",
      "Epoch 160: val_loss improved from 1354.57129 to 1337.40674, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 3812.4749 - mae: 16.9050 - val_loss: 1337.4067 - val_mae: 14.9603\n",
      "Epoch 161/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 3900.1594 - mae: 16.9211\n",
      "Epoch 161: val_loss improved from 1337.40674 to 1300.30347, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 3707.8176 - mae: 16.6695 - val_loss: 1300.3035 - val_mae: 14.8509\n",
      "Epoch 162/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 2153.3923 - mae: 15.3736\n",
      "Epoch 162: val_loss improved from 1300.30347 to 1285.30945, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 54ms/step - loss: 3596.0303 - mae: 16.3880 - val_loss: 1285.3094 - val_mae: 14.6754\n",
      "Epoch 163/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 3616.0559 - mae: 16.2250\n",
      "Epoch 163: val_loss improved from 1285.30945 to 1241.40820, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 3494.1599 - mae: 16.1694 - val_loss: 1241.4082 - val_mae: 14.5148\n",
      "Epoch 164/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 3667.6953 - mae: 15.9688\n",
      "Epoch 164: val_loss improved from 1241.40820 to 1220.12305, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 3392.7817 - mae: 16.0048 - val_loss: 1220.1230 - val_mae: 14.3893\n",
      "Epoch 165/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 3433.7188 - mae: 15.9535\n",
      "Epoch 165: val_loss improved from 1220.12305 to 1186.76147, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 3296.9180 - mae: 15.7557 - val_loss: 1186.7615 - val_mae: 14.2468\n",
      "Epoch 166/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 3450.0259 - mae: 15.7898\n",
      "Epoch 166: val_loss improved from 1186.76147 to 1159.87878, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 3197.8655 - mae: 15.5530 - val_loss: 1159.8788 - val_mae: 14.0862\n",
      "Epoch 167/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 3169.2927 - mae: 15.3995\n",
      "Epoch 167: val_loss improved from 1159.87878 to 1125.75391, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 3105.8796 - mae: 15.3446 - val_loss: 1125.7539 - val_mae: 13.8472\n",
      "Epoch 168/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 3308.0427 - mae: 15.3250\n",
      "Epoch 168: val_loss improved from 1125.75391 to 1093.72083, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 54ms/step - loss: 3016.1687 - mae: 15.1234 - val_loss: 1093.7208 - val_mae: 13.6371\n",
      "Epoch 169/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 2986.8057 - mae: 14.8795\n",
      "Epoch 169: val_loss improved from 1093.72083 to 1081.31641, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 2929.9490 - mae: 14.8408 - val_loss: 1081.3164 - val_mae: 13.5202\n",
      "Epoch 170/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 3106.3928 - mae: 15.0520\n",
      "Epoch 170: val_loss improved from 1081.31641 to 1039.46716, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 2846.8416 - mae: 14.7082 - val_loss: 1039.4672 - val_mae: 13.2539\n",
      "Epoch 171/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 2951.5857 - mae: 14.7720\n",
      "Epoch 171: val_loss improved from 1039.46716 to 1022.11987, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 2760.0981 - mae: 14.4677 - val_loss: 1022.1199 - val_mae: 13.2044\n",
      "Epoch 172/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 3014.1812 - mae: 14.4138\n",
      "Epoch 172: val_loss improved from 1022.11987 to 983.75220, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 50ms/step - loss: 2677.8430 - mae: 14.2931 - val_loss: 983.7522 - val_mae: 12.9294\n",
      "Epoch 173/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 2948.6533 - mae: 14.3734\n",
      "Epoch 173: val_loss improved from 983.75220 to 963.97070, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 2599.7693 - mae: 14.0721 - val_loss: 963.9707 - val_mae: 12.8904\n",
      "Epoch 174/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 2658.4773 - mae: 13.9660\n",
      "Epoch 174: val_loss improved from 963.97070 to 937.20142, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 2528.4824 - mae: 13.8048 - val_loss: 937.2014 - val_mae: 12.6071\n",
      "Epoch 175/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 2576.8423 - mae: 13.8795\n",
      "Epoch 175: val_loss improved from 937.20142 to 906.27484, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 2449.3240 - mae: 13.7031 - val_loss: 906.2748 - val_mae: 12.5192\n",
      "Epoch 176/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 2442.1562 - mae: 13.5593\n",
      "Epoch 176: val_loss improved from 906.27484 to 879.17810, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 2374.0007 - mae: 13.5181 - val_loss: 879.1781 - val_mae: 12.3794\n",
      "Epoch 177/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 2493.0095 - mae: 13.4990\n",
      "Epoch 177: val_loss improved from 879.17810 to 842.78955, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 2302.5503 - mae: 13.2972 - val_loss: 842.7896 - val_mae: 12.2442\n",
      "Epoch 178/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 2320.6030 - mae: 13.1891\n",
      "Epoch 178: val_loss improved from 842.78955 to 828.31812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 2232.4685 - mae: 13.0780 - val_loss: 828.3181 - val_mae: 12.0616\n",
      "Epoch 179/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 2130.9006 - mae: 12.7699\n",
      "Epoch 179: val_loss improved from 828.31812 to 797.24969, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 60ms/step - loss: 2171.4998 - mae: 12.9280 - val_loss: 797.2497 - val_mae: 11.8665\n",
      "Epoch 180/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 2375.7058 - mae: 13.0351\n",
      "Epoch 180: val_loss improved from 797.24969 to 774.21790, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 2101.1824 - mae: 12.6726 - val_loss: 774.2179 - val_mae: 11.6995\n",
      "Epoch 181/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 2191.2822 - mae: 12.6835\n",
      "Epoch 181: val_loss improved from 774.21790 to 747.50665, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 2042.3352 - mae: 12.4773 - val_loss: 747.5067 - val_mae: 11.5980\n",
      "Epoch 182/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 2211.7168 - mae: 12.4775\n",
      "Epoch 182: val_loss improved from 747.50665 to 725.06573, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 1974.2791 - mae: 12.3313 - val_loss: 725.0657 - val_mae: 11.4176\n",
      "Epoch 183/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 2179.0293 - mae: 12.3777\n",
      "Epoch 183: val_loss improved from 725.06573 to 702.44232, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 1904.6278 - mae: 12.1903 - val_loss: 702.4423 - val_mae: 11.3005\n",
      "Epoch 184/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1903.2635 - mae: 12.0630\n",
      "Epoch 184: val_loss improved from 702.44232 to 683.54218, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 1829.0026 - mae: 11.9361 - val_loss: 683.5422 - val_mae: 11.1330\n",
      "Epoch 185/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1717.8214 - mae: 11.7810\n",
      "Epoch 185: val_loss improved from 683.54218 to 664.01904, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 54ms/step - loss: 1717.8214 - mae: 11.7810 - val_loss: 664.0190 - val_mae: 11.0784\n",
      "Epoch 186/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 1696.6888 - mae: 11.8551\n",
      "Epoch 186: val_loss improved from 664.01904 to 644.60803, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 59ms/step - loss: 1506.0126 - mae: 11.6416 - val_loss: 644.6080 - val_mae: 10.8738\n",
      "Epoch 187/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1194.4689 - mae: 11.2992\n",
      "Epoch 187: val_loss improved from 644.60803 to 625.07971, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 56ms/step - loss: 1182.1951 - mae: 11.2884 - val_loss: 625.0797 - val_mae: 10.8797\n",
      "Epoch 188/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 952.3916 - mae: 11.1998\n",
      "Epoch 188: val_loss improved from 625.07971 to 597.87048, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 881.6163 - mae: 10.9502 - val_loss: 597.8705 - val_mae: 10.6034\n",
      "Epoch 189/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 552.1835 - mae: 10.3023\n",
      "Epoch 189: val_loss improved from 597.87048 to 578.88727, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 700.5732 - mae: 10.6635 - val_loss: 578.8873 - val_mae: 10.4881\n",
      "Epoch 190/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 623.3405 - mae: 10.3253\n",
      "Epoch 190: val_loss improved from 578.88727 to 543.55328, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 605.8576 - mae: 10.4095 - val_loss: 543.5533 - val_mae: 10.2821\n",
      "Epoch 191/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 553.3038 - mae: 10.3635\n",
      "Epoch 191: val_loss improved from 543.55328 to 516.89539, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 527.1869 - mae: 10.2354 - val_loss: 516.8954 - val_mae: 10.1629\n",
      "Epoch 192/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 484.0818 - mae: 10.0753\n",
      "Epoch 192: val_loss improved from 516.89539 to 496.43674, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 483.1992 - mae: 10.0640 - val_loss: 496.4367 - val_mae: 10.0721\n",
      "Epoch 193/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 464.2452 - mae: 10.0426\n",
      "Epoch 193: val_loss improved from 496.43674 to 464.44931, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 451.4001 - mae: 9.9478 - val_loss: 464.4493 - val_mae: 9.8312\n",
      "Epoch 194/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 426.4315 - mae: 9.7912\n",
      "Epoch 194: val_loss improved from 464.44931 to 462.45767, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 426.4315 - mae: 9.7912 - val_loss: 462.4577 - val_mae: 9.7559\n",
      "Epoch 195/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 282.9410 - mae: 9.3869\n",
      "Epoch 195: val_loss improved from 462.45767 to 435.61670, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 401.3739 - mae: 9.6787 - val_loss: 435.6167 - val_mae: 9.5451\n",
      "Epoch 196/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 407.2957 - mae: 9.7450\n",
      "Epoch 196: val_loss improved from 435.61670 to 427.22189, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 381.8115 - mae: 9.5463 - val_loss: 427.2219 - val_mae: 9.5098\n",
      "Epoch 197/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 379.2736 - mae: 9.4639\n",
      "Epoch 197: val_loss improved from 427.22189 to 416.33096, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 365.3701 - mae: 9.4493 - val_loss: 416.3310 - val_mae: 9.3526\n",
      "Epoch 198/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 357.0030 - mae: 9.4235\n",
      "Epoch 198: val_loss improved from 416.33096 to 407.16187, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 348.3178 - mae: 9.3429 - val_loss: 407.1619 - val_mae: 9.2884\n",
      "Epoch 199/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 334.9574 - mae: 9.3026\n",
      "Epoch 199: val_loss improved from 407.16187 to 385.02417, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 331.4803 - mae: 9.2597 - val_loss: 385.0242 - val_mae: 9.1216\n",
      "Epoch 200/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 322.3982 - mae: 9.1657\n",
      "Epoch 200: val_loss improved from 385.02417 to 376.75873, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 317.6981 - mae: 9.1492 - val_loss: 376.7587 - val_mae: 9.0960\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일: 모델학습을 위한 학습과정 설정단계\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "# print(\"\\n Accuracy: %.4f\" % model.evaluate(x_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19b45e81310>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_feature)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(test_label, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7adfe9f3775c954d01fc005199cfce03b4193bbc6ff21451e4d1eaf7785b4e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
