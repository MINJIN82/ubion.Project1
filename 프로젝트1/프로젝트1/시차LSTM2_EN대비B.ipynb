{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['열1', 'DateTime', '종가_ex', '대비_ex', '증감률_ex', '1Y_Mid_irs',\n",
       "       '1Y_전일비_irs', '2Y_Mid_irs', '2Y_전일비_irs', '3Y_Mid_irs', '3Y_전일비_irs',\n",
       "       '5Y_Mid_irs', '5Y_전일비_irs', '10Y_Mid_irs', '10Y_전일비_irs', '1Y_Mid_crs',\n",
       "       '1Y_전일비_crs', '2Y_Mid_crs', '2Y_전일비_crs', '3Y_Mid_crs', '3Y_전일비_crs',\n",
       "       '5Y_Mid_crs', '5Y_전일비_crs', '10Y_Mid_crs', '10Y_전일비_crs', '국고1년',\n",
       "       '국고3년', '국고5년', '국고10년', '통안364일', '통안2년', 'Bid_ndf', 'Ask_ndf',\n",
       "       'Mid_ndf', '전일비_ndf', '1Y_베이시스', '2Y_베이시스', '3Y_베이시스', '5Y_베이시스',\n",
       "       '10Y_베이시스', 'M1_스왑포인트', '전일대비_종가_ex', '등락률_종가_ex', '전일비_1Y_irs',\n",
       "       '전일비_2Y_irs', '전일비_3Y_irs', '전일비_5Y_irs', '전일비_10Y_irs', '전일비_1Y_crs',\n",
       "       '전일비_2Y_crs', '전일비_3Y_crs', '전일비_5Y_crs', '전일비_10Y_crs', '국고1년대비',\n",
       "       '국고3년대비', '국고5년대비', '국고10년대비', '통안1년대비', '통안2년대비', '전일비_1Y_베이시스',\n",
       "       '전일비_2Y_베이시스', '전일비_3Y_베이시스', '전일비_5Y_베이시스', '전일비_10Y_베이시스', '전날 종가_ex',\n",
       "       '종가_NDF차이'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all = pd.read_excel(\"./xlsx/시차상관분석6Data.xlsx\",index_col=0)\n",
    "all.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2['대비_국고1년'] = all2['국고1년']-all2['국고1년'].shift(1)\n",
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2['대비_국고3년'] = all2['국고3년']-all2['국고3년'].shift(1)\n",
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2['대비_국고5년'] = all2['국고5년']-all2['국고5년'].shift(1)\n",
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2['대비_국고10년'] = all2['국고10년']-all2['국고10년'].shift(1)\n",
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2['대비_통안1년'] = all2['통안364일']-all2['통안364일'].shift(1)\n",
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2['대비_통안2년'] = all2['통안364일']-all2['통안364일'].shift(1)\n",
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2['대비_ndf'] = all2['Mid_ndf']-all2['Mid_ndf'].shift(1)\n",
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2['등락률_ndf'] = ((all2['Mid_ndf']-all2['Mid_ndf'].shift(1))/all2['Mid_ndf'].shift(1))*100\n",
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2['스왑포인트_1월물'] = all2[\"M1_스왑포인트\"]/100\n",
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2['전일종가_ex'] = all2['종가_ex'].shift(1)\n",
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\495733640.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all2.rename({'전일비_1Y_베이시스': '대비_swapbasis_1Y', '전일비_2Y_베이시스': '대비_swapbasis_2Y', '전일비_3Y_베이시스': '대비_swapbasis_3Y', '전일비_5Y_베이시스': '대비_swapbasis_5Y', '전일비_10Y_베이시스': '대비_swapbasis_10Y', '종가_NDF차이':'종가_NDF_차이'}, axis=1, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>종가_ex</th>\n",
       "      <th>대비_swapbasis_1Y</th>\n",
       "      <th>대비_swapbasis_2Y</th>\n",
       "      <th>대비_swapbasis_3Y</th>\n",
       "      <th>대비_swapbasis_5Y</th>\n",
       "      <th>대비_swapbasis_10Y</th>\n",
       "      <th>대비_국고1년</th>\n",
       "      <th>대비_국고3년</th>\n",
       "      <th>대비_국고5년</th>\n",
       "      <th>대비_국고10년</th>\n",
       "      <th>대비_통안1년</th>\n",
       "      <th>대비_통안2년</th>\n",
       "      <th>대비_ndf</th>\n",
       "      <th>등락률_ndf</th>\n",
       "      <th>스왑포인트_1월물</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_NDF_차이</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1131.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1134.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>-6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1129.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-9.50</td>\n",
       "      <td>-0.834798</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1128.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.044307</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1128.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.110717</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>-1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>1313.7</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>1307.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.087796</td>\n",
       "      <td>-0.0070</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>1313.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.038139</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>-2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>1296.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-4.50</td>\n",
       "      <td>-0.343380</td>\n",
       "      <td>-0.0065</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>7.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>1299.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-10.25</td>\n",
       "      <td>-0.784839</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       종가_ex  대비_swapbasis_1Y  대비_swapbasis_2Y  대비_swapbasis_3Y  \\\n",
       "0     1131.7              2.0              8.0              9.0   \n",
       "1     1134.8              2.0              1.5              1.0   \n",
       "2     1129.0             -2.0             -4.5             -5.0   \n",
       "3     1128.8              1.0              1.5              0.0   \n",
       "4     1128.3              0.0             -2.0             -2.0   \n",
       "...      ...              ...              ...              ...   \n",
       "2454  1313.7             -4.0             -1.0              0.0   \n",
       "2455  1307.6              2.0             -1.0              0.0   \n",
       "2456  1313.3              2.0              4.0              5.0   \n",
       "2457  1296.1              1.0              0.0             -1.0   \n",
       "2458  1299.1             -4.0             -3.0             -1.0   \n",
       "\n",
       "      대비_swapbasis_5Y  대비_swapbasis_10Y  대비_국고1년  대비_국고3년  대비_국고5년  대비_국고10년  \\\n",
       "0                 9.0               9.0      NaN      NaN      NaN       NaN   \n",
       "1                -5.0             -13.0    -0.02    -0.02    -0.07     -0.07   \n",
       "2                -6.0              -5.0     0.01     0.01     0.00      0.00   \n",
       "3                -8.0             -10.0    -0.01     0.00     0.00      0.00   \n",
       "4                -4.0              -7.0    -0.01    -0.02    -0.02     -0.02   \n",
       "...               ...               ...      ...      ...      ...       ...   \n",
       "2454             -2.0               0.0    -0.02    -0.06    -0.07     -0.09   \n",
       "2455              1.0               1.0     0.01    -0.03    -0.02     -0.03   \n",
       "2456              5.0               5.0     0.00    -0.03    -0.05     -0.06   \n",
       "2457             -2.0              -5.0     0.01     0.04     0.03      0.08   \n",
       "2458              5.0               3.0    -0.04    -0.13    -0.12     -0.08   \n",
       "\n",
       "      대비_통안1년  대비_통안2년  대비_ndf   등락률_ndf  스왑포인트_1월물  전일종가_ex  종가_NDF_차이  \n",
       "0         NaN      NaN     NaN       NaN     0.0250      NaN      -7.50  \n",
       "1       -0.02    -0.02    4.00  0.352734     0.0240   1131.7      -6.30  \n",
       "2        0.01     0.01   -9.50 -0.834798     0.0240   1134.8       6.30  \n",
       "3       -0.01    -0.01    0.50  0.044307     0.0250   1129.0       0.00  \n",
       "4       -0.01    -0.01    1.25  0.110717     0.0240   1128.8      -1.45  \n",
       "...       ...      ...     ...       ...        ...      ...        ...  \n",
       "2454    -0.01    -0.01    3.35  0.256410    -0.0060   1313.0       3.15  \n",
       "2455     0.01     0.01    1.15  0.087796    -0.0070   1313.7       2.70  \n",
       "2456     0.02     0.02   -0.50 -0.038139    -0.0055   1307.6      -2.90  \n",
       "2457     0.02     0.02   -4.50 -0.343380    -0.0065   1313.3       7.30  \n",
       "2458    -0.03    -0.03  -10.25 -0.784839    -0.0060   1296.1       0.35  \n",
       "\n",
       "[2459 rows x 17 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 컬럼 추출\n",
    "all2 = all[[ '종가_ex', '전일비_1Y_베이시스', '전일비_2Y_베이시스', '전일비_3Y_베이시스', '전일비_5Y_베이시스', '전일비_10Y_베이시스',\n",
    "        '국고1년', '국고3년', '국고5년','국고10년', '통안364일', '통안2년', 'Mid_ndf', '종가_NDF차이', \n",
    "        'M1_스왑포인트']]            # [[]] 대괄호 2개 사용 -> 데이터 프레임형태로 나옴\n",
    "\n",
    "        \n",
    "# all2 = all2.set_index(\"DateTime\")\n",
    "\n",
    "\n",
    "all2['대비_국고1년'] = all2['국고1년']-all2['국고1년'].shift(1)\n",
    "all2['대비_국고3년'] = all2['국고3년']-all2['국고3년'].shift(1)\n",
    "all2['대비_국고5년'] = all2['국고5년']-all2['국고5년'].shift(1)\n",
    "all2['대비_국고10년'] = all2['국고10년']-all2['국고10년'].shift(1)\n",
    "all2['대비_통안1년'] = all2['통안364일']-all2['통안364일'].shift(1)\n",
    "all2['대비_통안2년'] = all2['통안364일']-all2['통안364일'].shift(1)\n",
    "all2['대비_ndf'] = all2['Mid_ndf']-all2['Mid_ndf'].shift(1)\n",
    "all2['등락률_ndf'] = ((all2['Mid_ndf']-all2['Mid_ndf'].shift(1))/all2['Mid_ndf'].shift(1))*100\n",
    "all2['스왑포인트_1월물'] = all2[\"M1_스왑포인트\"]/100 \n",
    "all2['전일종가_ex'] = all2['종가_ex'].shift(1)\n",
    "\n",
    "all2.rename({'전일비_1Y_베이시스': '대비_swapbasis_1Y', '전일비_2Y_베이시스': '대비_swapbasis_2Y', '전일비_3Y_베이시스': '대비_swapbasis_3Y', '전일비_5Y_베이시스': '대비_swapbasis_5Y', '전일비_10Y_베이시스': '대비_swapbasis_10Y', '종가_NDF차이':'종가_NDF_차이'}, axis=1, inplace=True)\n",
    "\n",
    "# all2 = all2.dropna()\n",
    "\n",
    "\n",
    "# 필요 칼럼만 남기기\n",
    "df = all2.copy()\n",
    "df = all2[[\"종가_ex\", \n",
    "            '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y', '대비_swapbasis_10Y', \n",
    "            \"대비_국고1년\", \"대비_국고3년\", \"대비_국고5년\", \"대비_국고10년\", \"대비_통안1년\", \"대비_통안2년\",\n",
    "            \"대비_ndf\", '등락률_ndf',\"스왑포인트_1월물\", \"전일종가_ex\",'종가_NDF_차이']] # , \"전일종가_ex\"\n",
    "\n",
    "\n",
    "df\n",
    "# 결측치 제거\n",
    "# df_1 = df_1.dropna()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['종가_ex', '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y',\n",
       "       '대비_swapbasis_5Y', '대비_swapbasis_10Y', '대비_국고1년', '대비_국고3년', '대비_국고5년',\n",
       "       '대비_국고10년', '대비_통안1년', '대비_통안2년', '대비_ndf', '등락률_ndf', '스왑포인트_1월물',\n",
       "       '전일종가_ex', 'Mid_ndf'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_1736\\1444086427.py:11: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  x.feature = x.columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_swapbasis_1Y</th>\n",
       "      <th>대비_swapbasis_2Y</th>\n",
       "      <th>대비_swapbasis_3Y</th>\n",
       "      <th>대비_swapbasis_5Y</th>\n",
       "      <th>대비_swapbasis_10Y</th>\n",
       "      <th>대비_국고1년</th>\n",
       "      <th>대비_국고3년</th>\n",
       "      <th>대비_국고5년</th>\n",
       "      <th>대비_국고10년</th>\n",
       "      <th>대비_통안1년</th>\n",
       "      <th>대비_통안2년</th>\n",
       "      <th>스왑포인트_1월물</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_NDF_차이</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>1.544080</td>\n",
       "      <td>1.437744</td>\n",
       "      <td>1.627899</td>\n",
       "      <td>1.488363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.648743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.286831</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>-0.910410</td>\n",
       "      <td>-2.158344</td>\n",
       "      <td>-1.133777</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>-1.798842</td>\n",
       "      <td>-0.217667</td>\n",
       "      <td>-0.217667</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.056282</td>\n",
       "      <td>-1.366022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.350946</td>\n",
       "      <td>-0.873707</td>\n",
       "      <td>-0.803456</td>\n",
       "      <td>-1.091718</td>\n",
       "      <td>-0.832269</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>0.159979</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>1.602547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>0.286831</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>-1.454333</td>\n",
       "      <td>-1.661066</td>\n",
       "      <td>-0.568154</td>\n",
       "      <td>-0.001379</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>0.118263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.390150</td>\n",
       "      <td>-0.323199</td>\n",
       "      <td>-0.729102</td>\n",
       "      <td>-1.163788</td>\n",
       "      <td>-0.568154</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>-0.514104</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.108476</td>\n",
       "      <td>-0.223358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-0.196727</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>-0.366487</td>\n",
       "      <td>-0.003472</td>\n",
       "      <td>-1.133777</td>\n",
       "      <td>-0.969524</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>-2.312737</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>3.206786</td>\n",
       "      <td>0.860405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.196727</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>0.177437</td>\n",
       "      <td>0.162288</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>-0.771052</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>-0.987488</td>\n",
       "      <td>3.219385</td>\n",
       "      <td>0.754385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.770388</td>\n",
       "      <td>0.797401</td>\n",
       "      <td>0.902668</td>\n",
       "      <td>0.825326</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-1.350390</td>\n",
       "      <td>-1.541894</td>\n",
       "      <td>0.215640</td>\n",
       "      <td>0.215640</td>\n",
       "      <td>-0.851696</td>\n",
       "      <td>3.109596</td>\n",
       "      <td>-0.564979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>-0.003304</td>\n",
       "      <td>-0.163113</td>\n",
       "      <td>-0.366487</td>\n",
       "      <td>-0.832269</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>0.644052</td>\n",
       "      <td>0.810938</td>\n",
       "      <td>2.055371</td>\n",
       "      <td>0.215640</td>\n",
       "      <td>0.215640</td>\n",
       "      <td>-0.942224</td>\n",
       "      <td>3.212186</td>\n",
       "      <td>1.838148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-0.583573</td>\n",
       "      <td>-0.163113</td>\n",
       "      <td>0.902668</td>\n",
       "      <td>0.493807</td>\n",
       "      <td>-2.265023</td>\n",
       "      <td>-2.099027</td>\n",
       "      <td>-3.241553</td>\n",
       "      <td>-2.055789</td>\n",
       "      <td>-0.325994</td>\n",
       "      <td>-0.325994</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>2.902617</td>\n",
       "      <td>0.200723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      대비_swapbasis_1Y  대비_swapbasis_2Y  대비_swapbasis_3Y  대비_swapbasis_5Y  \\\n",
       "0            0.348741         1.544080         1.437744         1.627899   \n",
       "1            0.348741         0.286831         0.157058        -0.910410   \n",
       "2           -0.350946        -0.873707        -0.803456        -1.091718   \n",
       "3            0.173819         0.286831        -0.003027        -1.454333   \n",
       "4           -0.001103        -0.390150        -0.323199        -0.729102   \n",
       "...               ...              ...              ...              ...   \n",
       "2454        -0.700790        -0.196727        -0.003027        -0.366487   \n",
       "2455         0.348741        -0.196727        -0.003027         0.177437   \n",
       "2456         0.348741         0.770388         0.797401         0.902668   \n",
       "2457         0.173819        -0.003304        -0.163113        -0.366487   \n",
       "2458        -0.700790        -0.583573        -0.163113         0.902668   \n",
       "\n",
       "      대비_swapbasis_10Y   대비_국고1년   대비_국고3년   대비_국고5년  대비_국고10년   대비_통안1년  \\\n",
       "0             1.488363       NaN       NaN       NaN       NaN       NaN   \n",
       "1            -2.158344 -1.133777 -0.324094 -1.890723 -1.798842 -0.217667   \n",
       "2            -0.832269  0.563092  0.159979  0.000440 -0.000209  0.107313   \n",
       "3            -1.661066 -0.568154 -0.001379  0.000440 -0.000209 -0.109340   \n",
       "4            -1.163788 -0.568154 -0.324094 -0.539892 -0.514104 -0.109340   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "2454         -0.003472 -1.133777 -0.969524 -1.890723 -2.312737 -0.109340   \n",
       "2455          0.162288  0.563092 -0.485451 -0.539892 -0.771052  0.107313   \n",
       "2456          0.825326 -0.002531 -0.485451 -1.350390 -1.541894  0.215640   \n",
       "2457         -0.832269  0.563092  0.644052  0.810938  2.055371  0.215640   \n",
       "2458          0.493807 -2.265023 -2.099027 -3.241553 -2.055789 -0.325994   \n",
       "\n",
       "       대비_통안2년  스왑포인트_1월물   전일종가_ex  종가_NDF_차이  \n",
       "0          NaN   1.909409       NaN  -1.648743  \n",
       "1    -0.217667   1.818881 -0.056282  -1.366022  \n",
       "2     0.107313   1.818881 -0.000487   1.602547  \n",
       "3    -0.109340   1.909409 -0.104877   0.118263  \n",
       "4    -0.109340   1.818881 -0.108476  -0.223358  \n",
       "...        ...        ...       ...        ...  \n",
       "2454 -0.109340  -0.896960  3.206786   0.860405  \n",
       "2455  0.107313  -0.987488  3.219385   0.754385  \n",
       "2456  0.215640  -0.851696  3.109596  -0.564979  \n",
       "2457  0.215640  -0.942224  3.212186   1.838148  \n",
       "2458 -0.325994  -0.896960  2.902617   0.200723  \n",
       "\n",
       "[2459 rows x 14 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 쓸 칼럼만 남기고 feature, target 분리해 각각 x,y 에 저장\n",
    "x = df[[ '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y', '대비_swapbasis_10Y',\n",
    "           '대비_국고1년', '대비_국고3년', '대비_국고5년', '대비_국고10년', \n",
    "           '대비_통안1년', '대비_통안2년', '스왑포인트_1월물', '전일종가_ex', \n",
    "           '종가_NDF_차이']]\n",
    "y = df[['종가_ex']]\n",
    "\n",
    "# 이건 이렇게 해야 밑에 코드 8번째 줄 columns에 들어갈 수 있다고 하네요!\n",
    "x.feature = x.columns \n",
    "x.feature\n",
    "\n",
    "# scaling 진행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingDataError",
     "evalue": "exog contains inf or nans",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingDataError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hyeok\\Desktop\\PBL\\py\\프로젝트1\\시차LSTM2_EN대비B.ipynb 셀 5\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hyeok/Desktop/PBL/py/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B81/%EC%8B%9C%EC%B0%A8LSTM2_EN%EB%8C%80%EB%B9%84B.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m feature_add \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39madd_constant(x_scaled, has_constant\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madd\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hyeok/Desktop/PBL/py/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B81/%EC%8B%9C%EC%B0%A8LSTM2_EN%EB%8C%80%EB%B9%84B.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# sm OLS 적합\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hyeok/Desktop/PBL/py/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B81/%EC%8B%9C%EC%B0%A8LSTM2_EN%EB%8C%80%EB%B9%84B.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39;49mOLS(y , feature_add)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hyeok/Desktop/PBL/py/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B81/%EC%8B%9C%EC%B0%A8LSTM2_EN%EB%8C%80%EB%B9%84B.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m fitted_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hyeok/Desktop/PBL/py/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B81/%EC%8B%9C%EC%B0%A8LSTM2_EN%EB%8C%80%EB%B9%84B.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# summary 함수통해 결과출력\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hyeok\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:890\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mAn exception will be raised in the next version.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    889\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[1;32m--> 890\u001b[0m \u001b[39msuper\u001b[39m(OLS, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(endog, exog, missing\u001b[39m=\u001b[39mmissing,\n\u001b[0;32m    891\u001b[0m                           hasconst\u001b[39m=\u001b[39mhasconst, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    892\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_keys:\n\u001b[0;32m    893\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_keys\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hyeok\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:717\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     weights \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m--> 717\u001b[0m \u001b[39msuper\u001b[39m(WLS, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(endog, exog, missing\u001b[39m=\u001b[39mmissing,\n\u001b[0;32m    718\u001b[0m                           weights\u001b[39m=\u001b[39mweights, hasconst\u001b[39m=\u001b[39mhasconst, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    719\u001b[0m nobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexog\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    720\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n",
      "File \u001b[1;32mc:\\Users\\hyeok\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:191\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, endog, exog, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 191\u001b[0m     \u001b[39msuper\u001b[39m(RegressionModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(endog, exog, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    192\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_attr\u001b[39m.\u001b[39mextend([\u001b[39m'\u001b[39m\u001b[39mpinv_wexog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwendog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwexog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\hyeok\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:267\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, endog, exog\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 267\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(endog, exog, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    268\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialize()\n",
      "File \u001b[1;32mc:\\Users\\hyeok\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:92\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m missing \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmissing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     91\u001b[0m hasconst \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mhasconst\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m---> 92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m     93\u001b[0m                               \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_constant \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mk_constant\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexog \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mexog\n",
      "File \u001b[1;32mc:\\Users\\hyeok\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:132\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_handle_data\u001b[39m(\u001b[39mself\u001b[39m, endog, exog, missing, hasconst, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 132\u001b[0m     data \u001b[39m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    133\u001b[0m     \u001b[39m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Users\\hyeok\\anaconda3\\lib\\site-packages\\statsmodels\\base\\data.py:673\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m     exog \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(exog)\n\u001b[0;32m    672\u001b[0m klass \u001b[39m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> 673\u001b[0m \u001b[39mreturn\u001b[39;00m klass(endog, exog\u001b[39m=\u001b[39mexog, missing\u001b[39m=\u001b[39mmissing, hasconst\u001b[39m=\u001b[39mhasconst,\n\u001b[0;32m    674\u001b[0m              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hyeok\\anaconda3\\lib\\site-packages\\statsmodels\\base\\data.py:86\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconst_idx \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_constant \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 86\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_constant(hasconst)\n\u001b[0;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_integrity()\n\u001b[0;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\hyeok\\anaconda3\\lib\\site-packages\\statsmodels\\base\\data.py:132\u001b[0m, in \u001b[0;36mModelData._handle_constant\u001b[1;34m(self, hasconst)\u001b[0m\n\u001b[0;32m    130\u001b[0m exog_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexog, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(exog_max)\u001b[39m.\u001b[39mall():\n\u001b[1;32m--> 132\u001b[0m     \u001b[39mraise\u001b[39;00m MissingDataError(\u001b[39m'\u001b[39m\u001b[39mexog contains inf or nans\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    133\u001b[0m exog_min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexog, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    134\u001b[0m const_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(exog_max \u001b[39m==\u001b[39m exog_min)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[1;31mMissingDataError\u001b[0m: exog contains inf or nans"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.804e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:35:01</td>     <th>  Log-Likelihood:    </th> <td> -6933.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.389e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2445</td>      <th>  BIC:               </th> <td>1.397e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8952</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.734</td> <td> 1135.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.7944</td> <td>    0.123</td> <td>   -6.470</td> <td> 0.000</td> <td>   -1.035</td> <td>   -0.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.1383</td> <td>    0.123</td> <td>   -1.121</td> <td> 0.263</td> <td>   -0.380</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th>  <td>   -0.1345</td> <td>    0.166</td> <td>   -0.812</td> <td> 0.417</td> <td>   -0.459</td> <td>    0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.2585</td> <td>    0.147</td> <td>    1.763</td> <td> 0.078</td> <td>   -0.029</td> <td>    0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>   -0.0988</td> <td>    0.106</td> <td>   -0.936</td> <td> 0.350</td> <td>   -0.306</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.1943</td> <td>    0.091</td> <td>   -2.147</td> <td> 0.032</td> <td>   -0.372</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.0438</td> <td>    0.105</td> <td>   -0.416</td> <td> 0.678</td> <td>   -0.251</td> <td>    0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_1Y</th>         <td>   -0.0086</td> <td>    0.083</td> <td>   -0.104</td> <td> 0.918</td> <td>   -0.172</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0309</td> <td>    0.086</td> <td>    0.357</td> <td> 0.721</td> <td>   -0.139</td> <td>    0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1892</td> <td>    0.092</td> <td>  -12.967</td> <td> 0.000</td> <td>   -1.369</td> <td>   -1.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7451</td> <td>    0.090</td> <td>  622.199</td> <td> 0.000</td> <td>   55.569</td> <td>   55.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0124</td> <td>    0.088</td> <td>  -45.689</td> <td> 0.000</td> <td>   -4.185</td> <td>   -3.840</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>243.194</td> <th>  Durbin-Watson:     </th> <td>   2.099</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1179.934</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.348</td>  <th>  Prob(JB):          </th> <td>6.03e-257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.322</td>  <th>  Cond. No.          </th> <td>    4.45</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 3.804e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:35:01   Log-Likelihood:                -6933.3\n",
       "No. Observations:                2458   AIC:                         1.389e+04\n",
       "Df Residuals:                    2445   BIC:                         1.397e+04\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8952      0.082   1.38e+04      0.000    1134.734    1135.056\n",
       "대비_swapbasis_1Y     -0.7944      0.123     -6.470      0.000      -1.035      -0.554\n",
       "대비_swapbasis_3Y     -0.1383      0.123     -1.121      0.263      -0.380       0.104\n",
       "대비_swapbasis_5Y     -0.1345      0.166     -0.812      0.417      -0.459       0.190\n",
       "대비_swapbasis_10Y     0.2585      0.147      1.763      0.078      -0.029       0.546\n",
       "대비_국고_1Y            -0.0988      0.106     -0.936      0.350      -0.306       0.108\n",
       "대비_국고_3Y            -0.1943      0.091     -2.147      0.032      -0.372      -0.017\n",
       "대비_국고_10Y           -0.0438      0.105     -0.416      0.678      -0.251       0.163\n",
       "대비_통안_1Y            -0.0086      0.083     -0.104      0.918      -0.172       0.154\n",
       "대비_통안_2Y             0.0309      0.086      0.357      0.721      -0.139       0.200\n",
       "스왑포인트_1M            -1.1892      0.092    -12.967      0.000      -1.369      -1.009\n",
       "전일종가_ex             55.7451      0.090    622.199      0.000      55.569      55.921\n",
       "종가_NDF_차이           -4.0124      0.088    -45.689      0.000      -4.185      -3.840\n",
       "==============================================================================\n",
       "Omnibus:                      243.194   Durbin-Watson:                   2.099\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1179.934\n",
       "Skew:                           0.348   Prob(JB):                    6.03e-257\n",
       "Kurtosis:                       6.322   Cond. No.                         4.45\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_2Y', '대비_국고_5Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor           Feature\n",
      "0     2.233068   대비_swapbasis_1Y\n",
      "1     2.254954   대비_swapbasis_3Y\n",
      "2     4.063049   대비_swapbasis_5Y\n",
      "3     3.185518  대비_swapbasis_10Y\n",
      "4     1.639050          대비_국고_1Y\n",
      "5     1.213781          대비_국고_3Y\n",
      "6     1.645510         대비_국고_10Y\n",
      "7     1.106946          대비_통안_2Y\n",
      "8     1.245918          스왑포인트_1M\n",
      "9     1.189116           전일종가_ex\n",
      "10    1.142576         종가_NDF_차이\n"
     ]
    }
   ],
   "source": [
    "x_scaled.drop(['대비_통안_1Y'], axis=1, inplace=True)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>4.152e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:35:02</td>     <th>  Log-Likelihood:    </th> <td> -6933.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.389e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2446</td>      <th>  BIC:               </th> <td>1.396e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8952</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.734</td> <td> 1135.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.7943</td> <td>    0.123</td> <td>   -6.471</td> <td> 0.000</td> <td>   -1.035</td> <td>   -0.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.1383</td> <td>    0.123</td> <td>   -1.121</td> <td> 0.262</td> <td>   -0.380</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th>  <td>   -0.1344</td> <td>    0.166</td> <td>   -0.812</td> <td> 0.417</td> <td>   -0.459</td> <td>    0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.2584</td> <td>    0.147</td> <td>    1.763</td> <td> 0.078</td> <td>   -0.029</td> <td>    0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>   -0.0999</td> <td>    0.105</td> <td>   -0.950</td> <td> 0.342</td> <td>   -0.306</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.1944</td> <td>    0.090</td> <td>   -2.148</td> <td> 0.032</td> <td>   -0.372</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.0442</td> <td>    0.105</td> <td>   -0.419</td> <td> 0.675</td> <td>   -0.251</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0308</td> <td>    0.086</td> <td>    0.356</td> <td> 0.722</td> <td>   -0.139</td> <td>    0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1892</td> <td>    0.092</td> <td>  -12.971</td> <td> 0.000</td> <td>   -1.369</td> <td>   -1.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7451</td> <td>    0.090</td> <td>  622.349</td> <td> 0.000</td> <td>   55.569</td> <td>   55.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0123</td> <td>    0.088</td> <td>  -45.698</td> <td> 0.000</td> <td>   -4.185</td> <td>   -3.840</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>243.181</td> <th>  Durbin-Watson:     </th> <td>   2.099</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1180.040</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.348</td>  <th>  Prob(JB):          </th> <td>5.72e-257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.322</td>  <th>  Cond. No.          </th> <td>    4.45</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 4.152e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:35:02   Log-Likelihood:                -6933.3\n",
       "No. Observations:                2458   AIC:                         1.389e+04\n",
       "Df Residuals:                    2446   BIC:                         1.396e+04\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8952      0.082   1.38e+04      0.000    1134.734    1135.056\n",
       "대비_swapbasis_1Y     -0.7943      0.123     -6.471      0.000      -1.035      -0.554\n",
       "대비_swapbasis_3Y     -0.1383      0.123     -1.121      0.262      -0.380       0.104\n",
       "대비_swapbasis_5Y     -0.1344      0.166     -0.812      0.417      -0.459       0.190\n",
       "대비_swapbasis_10Y     0.2584      0.147      1.763      0.078      -0.029       0.546\n",
       "대비_국고_1Y            -0.0999      0.105     -0.950      0.342      -0.306       0.106\n",
       "대비_국고_3Y            -0.1944      0.090     -2.148      0.032      -0.372      -0.017\n",
       "대비_국고_10Y           -0.0442      0.105     -0.419      0.675      -0.251       0.162\n",
       "대비_통안_2Y             0.0308      0.086      0.356      0.722      -0.139       0.200\n",
       "스왑포인트_1M            -1.1892      0.092    -12.971      0.000      -1.369      -1.009\n",
       "전일종가_ex             55.7451      0.090    622.349      0.000      55.569      55.921\n",
       "종가_NDF_차이           -4.0123      0.088    -45.698      0.000      -4.185      -3.840\n",
       "==============================================================================\n",
       "Omnibus:                      243.181   Durbin-Watson:                   2.099\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1180.040\n",
       "Skew:                           0.348   Prob(JB):                    5.72e-257\n",
       "Kurtosis:                       6.322   Cond. No.                         4.45\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>5.078e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:35:02</td>     <th>  Log-Likelihood:    </th> <td> -6933.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.389e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2448</td>      <th>  BIC:               </th> <td>1.395e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8952</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.734</td> <td> 1135.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.7994</td> <td>    0.122</td> <td>   -6.550</td> <td> 0.000</td> <td>   -1.039</td> <td>   -0.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.1385</td> <td>    0.123</td> <td>   -1.124</td> <td> 0.261</td> <td>   -0.380</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th>  <td>   -0.1327</td> <td>    0.165</td> <td>   -0.803</td> <td> 0.422</td> <td>   -0.457</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.2659</td> <td>    0.146</td> <td>    1.827</td> <td> 0.068</td> <td>   -0.019</td> <td>    0.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>   -0.1133</td> <td>    0.090</td> <td>   -1.254</td> <td> 0.210</td> <td>   -0.290</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.2000</td> <td>    0.089</td> <td>   -2.256</td> <td> 0.024</td> <td>   -0.374</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1887</td> <td>    0.092</td> <td>  -12.972</td> <td> 0.000</td> <td>   -1.368</td> <td>   -1.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7460</td> <td>    0.089</td> <td>  622.863</td> <td> 0.000</td> <td>   55.570</td> <td>   55.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0111</td> <td>    0.088</td> <td>  -45.780</td> <td> 0.000</td> <td>   -4.183</td> <td>   -3.839</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>243.084</td> <th>  Durbin-Watson:     </th> <td>   2.099</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1184.843</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.345</td>  <th>  Prob(JB):          </th> <td>5.18e-258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.330</td>  <th>  Cond. No.          </th> <td>    4.34</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 5.078e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:35:02   Log-Likelihood:                -6933.5\n",
       "No. Observations:                2458   AIC:                         1.389e+04\n",
       "Df Residuals:                    2448   BIC:                         1.395e+04\n",
       "Df Model:                           9                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8952      0.082   1.38e+04      0.000    1134.734    1135.056\n",
       "대비_swapbasis_1Y     -0.7994      0.122     -6.550      0.000      -1.039      -0.560\n",
       "대비_swapbasis_3Y     -0.1385      0.123     -1.124      0.261      -0.380       0.103\n",
       "대비_swapbasis_5Y     -0.1327      0.165     -0.803      0.422      -0.457       0.192\n",
       "대비_swapbasis_10Y     0.2659      0.146      1.827      0.068      -0.019       0.551\n",
       "대비_국고_1Y            -0.1133      0.090     -1.254      0.210      -0.290       0.064\n",
       "대비_국고_3Y            -0.2000      0.089     -2.256      0.024      -0.374      -0.026\n",
       "스왑포인트_1M            -1.1887      0.092    -12.972      0.000      -1.368      -1.009\n",
       "전일종가_ex             55.7460      0.089    622.863      0.000      55.570      55.921\n",
       "종가_NDF_차이           -4.0111      0.088    -45.780      0.000      -4.183      -3.839\n",
       "==============================================================================\n",
       "Omnibus:                      243.084   Durbin-Watson:                   2.099\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1184.843\n",
       "Skew:                           0.345   Prob(JB):                    5.18e-258\n",
       "Kurtosis:                       6.330   Cond. No.                         4.34\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_통안_2Y','대비_국고_10Y' ], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>5.714e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:35:02</td>     <th>  Log-Likelihood:    </th> <td> -6933.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.389e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2449</td>      <th>  BIC:               </th> <td>1.394e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8952</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.734</td> <td> 1135.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.8298</td> <td>    0.116</td> <td>   -7.153</td> <td> 0.000</td> <td>   -1.057</td> <td>   -0.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.1669</td> <td>    0.118</td> <td>   -1.414</td> <td> 0.158</td> <td>   -0.398</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.1955</td> <td>    0.116</td> <td>    1.684</td> <td> 0.092</td> <td>   -0.032</td> <td>    0.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>   -0.1093</td> <td>    0.090</td> <td>   -1.212</td> <td> 0.225</td> <td>   -0.286</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.1985</td> <td>    0.089</td> <td>   -2.239</td> <td> 0.025</td> <td>   -0.372</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1885</td> <td>    0.092</td> <td>  -12.970</td> <td> 0.000</td> <td>   -1.368</td> <td>   -1.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7460</td> <td>    0.089</td> <td>  622.909</td> <td> 0.000</td> <td>   55.571</td> <td>   55.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0099</td> <td>    0.088</td> <td>  -45.776</td> <td> 0.000</td> <td>   -4.182</td> <td>   -3.838</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>243.538</td> <th>  Durbin-Watson:     </th> <td>   2.096</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1184.671</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.347</td>  <th>  Prob(JB):          </th> <td>5.65e-258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.329</td>  <th>  Cond. No.          </th> <td>    2.63</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 5.714e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:35:02   Log-Likelihood:                -6933.8\n",
       "No. Observations:                2458   AIC:                         1.389e+04\n",
       "Df Residuals:                    2449   BIC:                         1.394e+04\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8952      0.082   1.38e+04      0.000    1134.734    1135.056\n",
       "대비_swapbasis_1Y     -0.8298      0.116     -7.153      0.000      -1.057      -0.602\n",
       "대비_swapbasis_3Y     -0.1669      0.118     -1.414      0.158      -0.398       0.065\n",
       "대비_swapbasis_10Y     0.1955      0.116      1.684      0.092      -0.032       0.423\n",
       "대비_국고_1Y            -0.1093      0.090     -1.212      0.225      -0.286       0.067\n",
       "대비_국고_3Y            -0.1985      0.089     -2.239      0.025      -0.372      -0.025\n",
       "스왑포인트_1M            -1.1885      0.092    -12.970      0.000      -1.368      -1.009\n",
       "전일종가_ex             55.7460      0.089    622.909      0.000      55.571      55.922\n",
       "종가_NDF_차이           -4.0099      0.088    -45.776      0.000      -4.182      -3.838\n",
       "==============================================================================\n",
       "Omnibus:                      243.538   Durbin-Watson:                   2.096\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1184.671\n",
       "Skew:                           0.347   Prob(JB):                    5.65e-258\n",
       "Kurtosis:                       6.329   Cond. No.                         2.63\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_5Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>6.529e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:35:02</td>     <th>  Log-Likelihood:    </th> <td> -6934.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.389e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2450</td>      <th>  BIC:               </th> <td>1.393e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8952</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.734</td> <td> 1135.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.8360</td> <td>    0.116</td> <td>   -7.213</td> <td> 0.000</td> <td>   -1.063</td> <td>   -0.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.1564</td> <td>    0.118</td> <td>   -1.328</td> <td> 0.184</td> <td>   -0.387</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.2065</td> <td>    0.116</td> <td>    1.784</td> <td> 0.074</td> <td>   -0.020</td> <td>    0.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.2370</td> <td>    0.083</td> <td>   -2.865</td> <td> 0.004</td> <td>   -0.399</td> <td>   -0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1885</td> <td>    0.092</td> <td>  -12.969</td> <td> 0.000</td> <td>   -1.368</td> <td>   -1.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7371</td> <td>    0.089</td> <td>  624.844</td> <td> 0.000</td> <td>   55.562</td> <td>   55.912</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0025</td> <td>    0.087</td> <td>  -45.799</td> <td> 0.000</td> <td>   -4.174</td> <td>   -3.831</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>253.190</td> <th>  Durbin-Watson:     </th> <td>   2.095</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1259.200</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.364</td>  <th>  Prob(JB):          </th> <td>3.70e-274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.430</td>  <th>  Cond. No.          </th> <td>    2.59</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 6.529e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:35:02   Log-Likelihood:                -6934.5\n",
       "No. Observations:                2458   AIC:                         1.389e+04\n",
       "Df Residuals:                    2450   BIC:                         1.393e+04\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8952      0.082   1.38e+04      0.000    1134.734    1135.056\n",
       "대비_swapbasis_1Y     -0.8360      0.116     -7.213      0.000      -1.063      -0.609\n",
       "대비_swapbasis_3Y     -0.1564      0.118     -1.328      0.184      -0.387       0.075\n",
       "대비_swapbasis_10Y     0.2065      0.116      1.784      0.074      -0.020       0.434\n",
       "대비_국고_3Y            -0.2370      0.083     -2.865      0.004      -0.399      -0.075\n",
       "스왑포인트_1M            -1.1885      0.092    -12.969      0.000      -1.368      -1.009\n",
       "전일종가_ex             55.7371      0.089    624.844      0.000      55.562      55.912\n",
       "종가_NDF_차이           -4.0025      0.087    -45.799      0.000      -4.174      -3.831\n",
       "==============================================================================\n",
       "Omnibus:                      253.190   Durbin-Watson:                   2.095\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1259.200\n",
       "Skew:                           0.364   Prob(JB):                    3.70e-274\n",
       "Kurtosis:                       6.430   Cond. No.                         2.59\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_국고_1Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>7.614e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:35:03</td>     <th>  Log-Likelihood:    </th> <td> -6935.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.388e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2451</td>      <th>  BIC:               </th> <td>1.393e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8952</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.734</td> <td> 1135.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.8974</td> <td>    0.106</td> <td>   -8.442</td> <td> 0.000</td> <td>   -1.106</td> <td>   -0.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.1448</td> <td>    0.106</td> <td>    1.366</td> <td> 0.172</td> <td>   -0.063</td> <td>    0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.2297</td> <td>    0.083</td> <td>   -2.782</td> <td> 0.005</td> <td>   -0.392</td> <td>   -0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1887</td> <td>    0.092</td> <td>  -12.970</td> <td> 0.000</td> <td>   -1.368</td> <td>   -1.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7370</td> <td>    0.089</td> <td>  624.745</td> <td> 0.000</td> <td>   55.562</td> <td>   55.912</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0034</td> <td>    0.087</td> <td>  -45.804</td> <td> 0.000</td> <td>   -4.175</td> <td>   -3.832</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>253.945</td> <th>  Durbin-Watson:     </th> <td>   2.094</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1260.964</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.366</td>  <th>  Prob(JB):          </th> <td>1.53e-274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.432</td>  <th>  Cond. No.          </th> <td>    2.18</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 7.614e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:35:03   Log-Likelihood:                -6935.4\n",
       "No. Observations:                2458   AIC:                         1.388e+04\n",
       "Df Residuals:                    2451   BIC:                         1.393e+04\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8952      0.082   1.38e+04      0.000    1134.734    1135.056\n",
       "대비_swapbasis_1Y     -0.8974      0.106     -8.442      0.000      -1.106      -0.689\n",
       "대비_swapbasis_10Y     0.1448      0.106      1.366      0.172      -0.063       0.353\n",
       "대비_국고_3Y            -0.2297      0.083     -2.782      0.005      -0.392      -0.068\n",
       "스왑포인트_1M            -1.1887      0.092    -12.970      0.000      -1.368      -1.009\n",
       "전일종가_ex             55.7370      0.089    624.745      0.000      55.562      55.912\n",
       "종가_NDF_차이           -4.0034      0.087    -45.804      0.000      -4.175      -3.832\n",
       "==============================================================================\n",
       "Omnibus:                      253.945   Durbin-Watson:                   2.094\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1260.964\n",
       "Skew:                           0.366   Prob(JB):                    1.53e-274\n",
       "Kurtosis:                       6.432   Cond. No.                         2.18\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_3Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>9.134e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:35:03</td>     <th>  Log-Likelihood:    </th> <td> -6936.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.388e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2452</td>      <th>  BIC:               </th> <td>1.392e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td> 1134.8952</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.734</td> <td> 1135.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th> <td>   -0.8069</td> <td>    0.083</td> <td>   -9.707</td> <td> 0.000</td> <td>   -0.970</td> <td>   -0.644</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>        <td>   -0.2392</td> <td>    0.082</td> <td>   -2.907</td> <td> 0.004</td> <td>   -0.401</td> <td>   -0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>        <td>   -1.1906</td> <td>    0.092</td> <td>  -12.990</td> <td> 0.000</td> <td>   -1.370</td> <td>   -1.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>         <td>   55.7350</td> <td>    0.089</td> <td>  624.696</td> <td> 0.000</td> <td>   55.560</td> <td>   55.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>       <td>   -4.0034</td> <td>    0.087</td> <td>  -45.796</td> <td> 0.000</td> <td>   -4.175</td> <td>   -3.832</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>255.959</td> <th>  Durbin-Watson:     </th> <td>   2.098</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1276.436</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.370</td>  <th>  Prob(JB):          </th> <td>6.69e-278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.452</td>  <th>  Cond. No.          </th> <td>    1.64</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 9.134e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:35:03   Log-Likelihood:                -6936.4\n",
       "No. Observations:                2458   AIC:                         1.388e+04\n",
       "Df Residuals:                    2452   BIC:                         1.392e+04\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const            1134.8952      0.082   1.38e+04      0.000    1134.734    1135.056\n",
       "대비_swapbasis_1Y    -0.8069      0.083     -9.707      0.000      -0.970      -0.644\n",
       "대비_국고_3Y           -0.2392      0.082     -2.907      0.004      -0.401      -0.078\n",
       "스왑포인트_1M           -1.1906      0.092    -12.990      0.000      -1.370      -1.011\n",
       "전일종가_ex            55.7350      0.089    624.696      0.000      55.560      55.910\n",
       "종가_NDF_차이          -4.0034      0.087    -45.796      0.000      -4.175      -3.832\n",
       "==============================================================================\n",
       "Omnibus:                      255.959   Durbin-Watson:                   2.098\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1276.436\n",
       "Skew:                           0.370   Prob(JB):                    6.69e-278\n",
       "Kurtosis:                       6.452   Cond. No.                         1.64\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_10Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['대비_swapbasis_1Y', '대비_국고_3Y', '스왑포인트_1M', '전일종가_ex', '종가_NDF_차이'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_swapbasis_1Y</th>\n",
       "      <th>대비_국고_3Y</th>\n",
       "      <th>스왑포인트_1M</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_NDF_차이</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348821</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.056282</td>\n",
       "      <td>-1.367171</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350742</td>\n",
       "      <td>0.159979</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>1.602437</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173930</td>\n",
       "      <td>-0.001379</td>\n",
       "      <td>1.911215</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>0.117633</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.000961</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.108476</td>\n",
       "      <td>-0.224108</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-09</th>\n",
       "      <td>0.698602</td>\n",
       "      <td>0.966767</td>\n",
       "      <td>1.775350</td>\n",
       "      <td>-0.117475</td>\n",
       "      <td>-0.872236</td>\n",
       "      <td>1125.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700523</td>\n",
       "      <td>-0.969524</td>\n",
       "      <td>-0.896666</td>\n",
       "      <td>3.206786</td>\n",
       "      <td>0.860035</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348821</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-0.987243</td>\n",
       "      <td>3.219385</td>\n",
       "      <td>0.753978</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348821</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-0.851378</td>\n",
       "      <td>3.109596</td>\n",
       "      <td>-0.565848</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173930</td>\n",
       "      <td>0.644052</td>\n",
       "      <td>-0.941955</td>\n",
       "      <td>3.212186</td>\n",
       "      <td>1.838120</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700523</td>\n",
       "      <td>-2.099027</td>\n",
       "      <td>-0.896666</td>\n",
       "      <td>2.902617</td>\n",
       "      <td>0.200122</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2458 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_swapbasis_1Y  대비_국고_3Y  스왑포인트_1M   전일종가_ex  종가_NDF_차이   종가_ex\n",
       "DateTime                                                                    \n",
       "2012-08-03         0.348821 -0.324094  1.820638 -0.056282  -1.367171  1134.8\n",
       "2012-08-06        -0.350742  0.159979  1.820638 -0.000487   1.602437  1129.0\n",
       "2012-08-07         0.173930 -0.001379  1.911215 -0.104877   0.117633  1128.8\n",
       "2012-08-08        -0.000961 -0.324094  1.820638 -0.108476  -0.224108  1128.3\n",
       "2012-08-09         0.698602  0.966767  1.775350 -0.117475  -0.872236  1125.5\n",
       "...                     ...       ...       ...       ...        ...     ...\n",
       "2022-07-25        -0.700523 -0.969524 -0.896666  3.206786   0.860035  1313.7\n",
       "2022-07-26         0.348821 -0.485451 -0.987243  3.219385   0.753978  1307.6\n",
       "2022-07-27         0.348821 -0.485451 -0.851378  3.109596  -0.565848  1313.3\n",
       "2022-07-28         0.173930  0.644052 -0.941955  3.212186   1.838120  1296.1\n",
       "2022-07-29        -0.700523 -2.099027 -0.896666  2.902617   0.200122  1299.1\n",
       "\n",
       "[2458 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = pd.concat([x_scaled,y], axis=1)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_scaled[0:1945]\n",
    "test = df_scaled[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=1):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['대비_swapbasis_1Y', '대비_국고_3Y', '스왑포인트_1M', '전일종가_ex', '종가_NDF_차이']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1555, 1, 5), (389, 1, 5))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.73930045e-01,  4.82694045e-01, -1.16839659e+00,\n",
       "          1.30617974e+00, -3.89085769e-01]],\n",
       "\n",
       "       [[ 1.73930045e-01, -1.62736099e-01,  1.50361921e+00,\n",
       "         -1.23066212e+00, -5.18711510e-01]],\n",
       "\n",
       "       [[ 8.73492412e-01, -1.37856316e-03, -3.07916923e-01,\n",
       "          6.58245673e-01,  1.05848876e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6.11156524e-01,  9.66766653e-01,  1.41304240e+00,\n",
       "         -1.33955104e+00,  1.88337984e-01]],\n",
       "\n",
       "       [[-1.75851138e-01, -1.62736099e-01,  1.54890761e+00,\n",
       "         -1.30535452e+00, -5.77632301e-01]],\n",
       "\n",
       "       [[-9.60546374e-04, -1.37856316e-03,  7.79004758e-01,\n",
       "          6.97841644e-01, -3.06596662e-01]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512, 1, 5), (512, 1))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 1)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "77/98 [======================>.......] - ETA: 0s - loss: 1267783.1250 - mae: 1124.7773\n",
      "Epoch 1: val_loss improved from inf to 1263648.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 38ms/step - loss: 1267259.8750 - mae: 1124.5641 - val_loss: 1263648.1250 - val_mae: 1123.0774\n",
      "Epoch 2/200\n",
      "76/98 [======================>.......] - ETA: 0s - loss: 1268625.6250 - mae: 1125.1801\n",
      "Epoch 2: val_loss improved from 1263648.12500 to 1261704.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 1265941.8750 - mae: 1123.9717 - val_loss: 1261704.7500 - val_mae: 1122.2030\n",
      "Epoch 3/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1263469.1250 - mae: 1122.8406\n",
      "Epoch 3: val_loss improved from 1261704.75000 to 1257708.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 1263094.5000 - mae: 1122.6903 - val_loss: 1257708.0000 - val_mae: 1120.4006\n",
      "Epoch 4/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1257020.6250 - mae: 1119.9636\n",
      "Epoch 4: val_loss improved from 1257708.00000 to 1251244.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 1257985.1250 - mae: 1120.3827 - val_loss: 1251244.2500 - val_mae: 1117.4801\n",
      "Epoch 5/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1251223.8750 - mae: 1117.3295\n",
      "Epoch 5: val_loss improved from 1251244.25000 to 1242318.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 1250442.8750 - mae: 1116.9722 - val_loss: 1242318.6250 - val_mae: 1113.4385\n",
      "Epoch 6/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1240672.7500 - mae: 1112.5403\n",
      "Epoch 6: val_loss improved from 1242318.62500 to 1231109.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1240532.1250 - mae: 1112.4778 - val_loss: 1231109.1250 - val_mae: 1108.3436\n",
      "Epoch 7/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1228238.2500 - mae: 1106.8772\n",
      "Epoch 7: val_loss improved from 1231109.12500 to 1218049.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 1228537.1250 - mae: 1107.0140 - val_loss: 1218049.7500 - val_mae: 1102.3800\n",
      "Epoch 8/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1214398.6250 - mae: 1100.5265\n",
      "Epoch 8: val_loss improved from 1218049.75000 to 1203344.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 1214786.8750 - mae: 1100.7205 - val_loss: 1203344.3750 - val_mae: 1095.6277\n",
      "Epoch 9/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1199361.3750 - mae: 1093.6110\n",
      "Epoch 9: val_loss improved from 1203344.37500 to 1187178.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 1199447.8750 - mae: 1093.6604 - val_loss: 1187178.8750 - val_mae: 1088.1570\n",
      "Epoch 10/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1182073.8750 - mae: 1085.6140\n",
      "Epoch 10: val_loss improved from 1187178.87500 to 1169698.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 1182695.3750 - mae: 1085.9009 - val_loss: 1169698.8750 - val_mae: 1080.0183\n",
      "Epoch 11/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1165328.8750 - mae: 1077.7872\n",
      "Epoch 11: val_loss improved from 1169698.87500 to 1151098.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1164677.0000 - mae: 1077.4758 - val_loss: 1151098.6250 - val_mae: 1071.2887\n",
      "Epoch 12/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1145463.3750 - mae: 1068.4155\n",
      "Epoch 12: val_loss improved from 1151098.62500 to 1131387.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1145463.3750 - mae: 1068.4155 - val_loss: 1131387.1250 - val_mae: 1061.9501\n",
      "Epoch 13/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1125354.6250 - mae: 1058.8528\n",
      "Epoch 13: val_loss improved from 1131387.12500 to 1110668.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 1125249.3750 - mae: 1058.7958 - val_loss: 1110668.6250 - val_mae: 1052.0408\n",
      "Epoch 14/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1105043.1250 - mae: 1049.1277\n",
      "Epoch 14: val_loss improved from 1110668.62500 to 1089216.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1104155.3750 - mae: 1048.6582 - val_loss: 1089216.8750 - val_mae: 1041.6735\n",
      "Epoch 15/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 1085447.8750 - mae: 1039.6017\n",
      "Epoch 15: val_loss improved from 1089216.87500 to 1067003.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1082262.7500 - mae: 1038.0184 - val_loss: 1067003.7500 - val_mae: 1030.8235\n",
      "Epoch 16/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1063653.0000 - mae: 1028.9266\n",
      "Epoch 16: val_loss improved from 1067003.75000 to 1044093.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 1059658.6250 - mae: 1026.9120 - val_loss: 1044093.5625 - val_mae: 1019.5054\n",
      "Epoch 17/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1037888.5000 - mae: 1016.1132\n",
      "Epoch 17: val_loss improved from 1044093.56250 to 1020584.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1036361.9375 - mae: 1015.3189 - val_loss: 1020584.2500 - val_mae: 1007.7354\n",
      "Epoch 18/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1015562.6875 - mae: 1004.8735\n",
      "Epoch 18: val_loss improved from 1020584.25000 to 996532.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1012483.5625 - mae: 1003.2906 - val_loss: 996532.7500 - val_mae: 995.5612\n",
      "Epoch 19/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 990500.2500 - mae: 992.1093\n",
      "Epoch 19: val_loss improved from 996532.75000 to 972003.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 988163.5625 - mae: 990.8737 - val_loss: 972003.9375 - val_mae: 982.9635\n",
      "Epoch 20/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 963996.8750 - mae: 978.3519\n",
      "Epoch 20: val_loss improved from 972003.93750 to 947115.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 963401.0000 - mae: 978.0419 - val_loss: 947115.1875 - val_mae: 970.0001\n",
      "Epoch 21/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 938001.8750 - mae: 964.6884\n",
      "Epoch 21: val_loss improved from 947115.18750 to 921968.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 938316.6875 - mae: 964.8641 - val_loss: 921968.0625 - val_mae: 956.7129\n",
      "Epoch 22/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 914262.3125 - mae: 952.0842\n",
      "Epoch 22: val_loss improved from 921968.06250 to 896549.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 912981.6250 - mae: 951.3474 - val_loss: 896549.2500 - val_mae: 943.0823\n",
      "Epoch 23/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 887255.9375 - mae: 937.3118\n",
      "Epoch 23: val_loss improved from 896549.25000 to 870907.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 887378.0625 - mae: 937.4668 - val_loss: 870907.5625 - val_mae: 929.0975\n",
      "Epoch 24/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 863238.2500 - mae: 924.1418\n",
      "Epoch 24: val_loss improved from 870907.56250 to 845153.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 861641.1250 - mae: 923.2643 - val_loss: 845153.2500 - val_mae: 914.8354\n",
      "Epoch 25/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 836349.0625 - mae: 909.0846\n",
      "Epoch 25: val_loss improved from 845153.25000 to 819238.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 835782.8750 - mae: 908.7559 - val_loss: 819238.0000 - val_mae: 900.2175\n",
      "Epoch 26/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 810397.1875 - mae: 894.2460\n",
      "Epoch 26: val_loss improved from 819238.00000 to 793289.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 809807.3750 - mae: 893.9146 - val_loss: 793289.4375 - val_mae: 885.3351\n",
      "Epoch 27/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 785747.2500 - mae: 879.8591\n",
      "Epoch 27: val_loss improved from 793289.43750 to 767447.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 783902.1875 - mae: 878.8212 - val_loss: 767447.5000 - val_mae: 870.2334\n",
      "Epoch 28/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 759241.1250 - mae: 864.1743\n",
      "Epoch 28: val_loss improved from 767447.50000 to 741670.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 758115.4375 - mae: 863.5418 - val_loss: 741670.5000 - val_mae: 854.8682\n",
      "Epoch 29/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 733055.7500 - mae: 848.3834\n",
      "Epoch 29: val_loss improved from 741670.50000 to 716028.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 732417.3750 - mae: 847.9983 - val_loss: 716028.8125 - val_mae: 839.3140\n",
      "Epoch 30/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 706342.4375 - mae: 831.8305\n",
      "Epoch 30: val_loss improved from 716028.81250 to 690609.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 706928.3750 - mae: 832.2545 - val_loss: 690609.1250 - val_mae: 823.5392\n",
      "Epoch 31/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 685005.4375 - mae: 818.8699\n",
      "Epoch 31: val_loss improved from 690609.12500 to 665392.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 681637.2500 - mae: 816.2436 - val_loss: 665392.0625 - val_mae: 807.5934\n",
      "Epoch 32/200\n",
      "73/98 [=====================>........] - ETA: 0s - loss: 666067.9375 - mae: 806.6057\n",
      "Epoch 32: val_loss improved from 665392.06250 to 640460.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 656626.7500 - mae: 800.1314 - val_loss: 640460.3125 - val_mae: 791.4783\n",
      "Epoch 33/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 631281.0625 - mae: 783.7531\n",
      "Epoch 33: val_loss improved from 640460.31250 to 615696.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 631871.5625 - mae: 783.9077 - val_loss: 615696.9375 - val_mae: 775.0924\n",
      "Epoch 34/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 610034.1250 - mae: 769.1452\n",
      "Epoch 34: val_loss improved from 615696.93750 to 591327.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 607420.5625 - mae: 767.4351 - val_loss: 591327.3125 - val_mae: 758.6272\n",
      "Epoch 35/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 585918.4375 - mae: 752.8143\n",
      "Epoch 35: val_loss improved from 591327.31250 to 567281.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 583344.8125 - mae: 750.8298 - val_loss: 567281.7500 - val_mae: 741.9441\n",
      "Epoch 36/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 560126.8125 - mae: 734.4091\n",
      "Epoch 36: val_loss improved from 567281.75000 to 543626.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 559626.0625 - mae: 734.1270 - val_loss: 543626.7500 - val_mae: 725.1273\n",
      "Epoch 37/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 536418.7500 - mae: 717.3776\n",
      "Epoch 37: val_loss improved from 543626.75000 to 520369.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 536346.9375 - mae: 717.3159 - val_loss: 520369.9375 - val_mae: 708.2007\n",
      "Epoch 38/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 513985.4062 - mae: 700.8830\n",
      "Epoch 38: val_loss improved from 520369.93750 to 497643.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 513560.6875 - mae: 700.3765 - val_loss: 497643.6250 - val_mae: 691.1851\n",
      "Epoch 39/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 491980.5312 - mae: 684.0771\n",
      "Epoch 39: val_loss improved from 497643.62500 to 475374.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 491263.3438 - mae: 683.4325 - val_loss: 475374.2188 - val_mae: 674.0740\n",
      "Epoch 40/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 470421.5938 - mae: 667.2223\n",
      "Epoch 40: val_loss improved from 475374.21875 to 453616.28125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 469478.0000 - mae: 666.4763 - val_loss: 453616.2812 - val_mae: 656.9064\n",
      "Epoch 41/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 449109.7188 - mae: 649.9529\n",
      "Epoch 41: val_loss improved from 453616.28125 to 432469.15625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 448254.8125 - mae: 649.5795 - val_loss: 432469.1562 - val_mae: 639.6909\n",
      "Epoch 42/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 429147.9688 - mae: 633.6625\n",
      "Epoch 42: val_loss improved from 432469.15625 to 411812.28125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 427615.9375 - mae: 632.5702 - val_loss: 411812.2812 - val_mae: 622.4039\n",
      "Epoch 43/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 410720.8750 - mae: 618.4869\n",
      "Epoch 43: val_loss improved from 411812.28125 to 391749.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 407552.9062 - mae: 615.7194 - val_loss: 391749.3438 - val_mae: 605.1028\n",
      "Epoch 44/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 388733.1562 - mae: 599.4233\n",
      "Epoch 44: val_loss improved from 391749.34375 to 372334.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 388069.1250 - mae: 598.7705 - val_loss: 372334.3438 - val_mae: 587.8127\n",
      "Epoch 45/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 369750.9062 - mae: 582.2665\n",
      "Epoch 45: val_loss improved from 372334.34375 to 353548.59375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 369245.1875 - mae: 582.0153 - val_loss: 353548.5938 - val_mae: 570.5371\n",
      "Epoch 46/200\n",
      "74/98 [=====================>........] - ETA: 0s - loss: 353579.4688 - mae: 568.0912\n",
      "Epoch 46: val_loss improved from 353548.59375 to 335365.84375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 351096.5000 - mae: 565.2883 - val_loss: 335365.8438 - val_mae: 553.2735\n",
      "Epoch 47/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 334739.4375 - mae: 550.0032\n",
      "Epoch 47: val_loss improved from 335365.84375 to 317887.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 333580.6875 - mae: 548.6777 - val_loss: 317887.9375 - val_mae: 536.0964\n",
      "Epoch 48/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 319129.4375 - mae: 535.2592\n",
      "Epoch 48: val_loss improved from 317887.93750 to 301043.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 316731.1875 - mae: 532.1884 - val_loss: 301043.6250 - val_mae: 519.0160\n",
      "Epoch 49/200\n",
      "74/98 [=====================>........] - ETA: 0s - loss: 303745.7812 - mae: 519.1022\n",
      "Epoch 49: val_loss improved from 301043.62500 to 284864.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 300579.8438 - mae: 515.8986 - val_loss: 284864.1875 - val_mae: 501.9750\n",
      "Epoch 50/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 283822.0312 - mae: 498.0885\n",
      "Epoch 50: val_loss improved from 284864.18750 to 269458.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 285121.5000 - mae: 499.6810 - val_loss: 269458.7188 - val_mae: 485.1593\n",
      "Epoch 51/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 271029.1875 - mae: 484.2313\n",
      "Epoch 51: val_loss improved from 269458.71875 to 254607.92188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 270335.2188 - mae: 483.7970 - val_loss: 254607.9219 - val_mae: 468.5659\n",
      "Epoch 52/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 256214.5469 - mae: 468.2928\n",
      "Epoch 52: val_loss improved from 254607.92188 to 240459.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 256214.5469 - mae: 468.2928 - val_loss: 240459.0000 - val_mae: 452.1346\n",
      "Epoch 53/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 243288.1094 - mae: 454.0521\n",
      "Epoch 53: val_loss improved from 240459.00000 to 227091.92188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 242786.0000 - mae: 453.2221 - val_loss: 227091.9219 - val_mae: 436.2791\n",
      "Epoch 54/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 230194.0469 - mae: 438.8326\n",
      "Epoch 54: val_loss improved from 227091.92188 to 214256.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 230064.1406 - mae: 438.7197 - val_loss: 214256.4688 - val_mae: 420.6102\n",
      "Epoch 55/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 217848.3125 - mae: 424.2466\n",
      "Epoch 55: val_loss improved from 214256.46875 to 202263.85938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 217968.4844 - mae: 424.7017 - val_loss: 202263.8594 - val_mae: 405.8714\n",
      "Epoch 56/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 207191.2188 - mae: 411.7565\n",
      "Epoch 56: val_loss improved from 202263.85938 to 190761.10938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 206562.6094 - mae: 411.4705 - val_loss: 190761.1094 - val_mae: 391.6229\n",
      "Epoch 57/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 192496.9062 - mae: 393.7247\n",
      "Epoch 57: val_loss improved from 190761.10938 to 180031.92188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 195757.6406 - mae: 398.8321 - val_loss: 180031.9219 - val_mae: 378.4915\n",
      "Epoch 58/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 184972.8125 - mae: 386.2997\n",
      "Epoch 58: val_loss improved from 180031.92188 to 169817.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 185609.4062 - mae: 386.7424 - val_loss: 169817.1875 - val_mae: 366.0630\n",
      "Epoch 59/200\n",
      "78/98 [======================>.......] - ETA: 0s - loss: 174251.0469 - mae: 371.9040\n",
      "Epoch 59: val_loss improved from 169817.18750 to 160334.48438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 176098.4219 - mae: 375.4434 - val_loss: 160334.4844 - val_mae: 354.4378\n",
      "Epoch 60/200\n",
      "75/98 [=====================>........] - ETA: 0s - loss: 162212.6406 - mae: 359.4167\n",
      "Epoch 60: val_loss improved from 160334.48438 to 151301.92188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 167167.3281 - mae: 364.3133 - val_loss: 151301.9219 - val_mae: 343.5096\n",
      "Epoch 61/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 158688.7656 - mae: 353.4982\n",
      "Epoch 61: val_loss improved from 151301.92188 to 143021.15625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 158825.2188 - mae: 354.2196 - val_loss: 143021.1562 - val_mae: 333.3286\n",
      "Epoch 62/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 150796.7031 - mae: 343.9585\n",
      "Epoch 62: val_loss improved from 143021.15625 to 135133.04688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 151029.6875 - mae: 344.3380 - val_loss: 135133.0469 - val_mae: 323.4440\n",
      "Epoch 63/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 145296.8594 - mae: 336.8536\n",
      "Epoch 63: val_loss improved from 135133.04688 to 127751.01562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 143694.5156 - mae: 334.8499 - val_loss: 127751.0156 - val_mae: 314.0510\n",
      "Epoch 64/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 138461.5312 - mae: 328.2254\n",
      "Epoch 64: val_loss improved from 127751.01562 to 120887.45312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 136870.2656 - mae: 326.1792 - val_loss: 120887.4531 - val_mae: 305.1036\n",
      "Epoch 65/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 131302.7812 - mae: 318.6740\n",
      "Epoch 65: val_loss improved from 120887.45312 to 114556.48438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 130500.4531 - mae: 317.9681 - val_loss: 114556.4844 - val_mae: 296.7115\n",
      "Epoch 66/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 125416.4922 - mae: 311.1315\n",
      "Epoch 66: val_loss improved from 114556.48438 to 108595.54688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 124581.4219 - mae: 310.1284 - val_loss: 108595.5469 - val_mae: 288.6729\n",
      "Epoch 67/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 120054.5625 - mae: 303.9329\n",
      "Epoch 67: val_loss improved from 108595.54688 to 103015.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 119003.8203 - mae: 302.4982 - val_loss: 103015.1875 - val_mae: 280.8748\n",
      "Epoch 68/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 112131.4062 - mae: 294.2262\n",
      "Epoch 68: val_loss improved from 103015.18750 to 97748.10156, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 113796.6406 - mae: 295.1639 - val_loss: 97748.1016 - val_mae: 273.2508\n",
      "Epoch 69/200\n",
      "74/98 [=====================>........] - ETA: 0s - loss: 111331.3047 - mae: 292.4384\n",
      "Epoch 69: val_loss improved from 97748.10156 to 92901.10156, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 108906.8672 - mae: 288.2894 - val_loss: 92901.1016 - val_mae: 266.2215\n",
      "Epoch 70/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 103285.2344 - mae: 280.2418\n",
      "Epoch 70: val_loss improved from 92901.10156 to 88326.07812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 104335.8438 - mae: 281.4583 - val_loss: 88326.0781 - val_mae: 259.3863\n",
      "Epoch 71/200\n",
      "77/98 [======================>.......] - ETA: 0s - loss: 100703.2891 - mae: 275.6518\n",
      "Epoch 71: val_loss improved from 88326.07812 to 84035.07031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 100032.5781 - mae: 275.1451 - val_loss: 84035.0703 - val_mae: 252.8475\n",
      "Epoch 72/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 95837.6875 - mae: 268.4661\n",
      "Epoch 72: val_loss improved from 84035.07031 to 80157.99219, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 96020.7109 - mae: 268.9516 - val_loss: 80157.9922 - val_mae: 246.9890\n",
      "Epoch 73/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 93370.1250 - mae: 263.6263\n",
      "Epoch 73: val_loss improved from 80157.99219 to 76249.78906, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 92209.9141 - mae: 262.9685 - val_loss: 76249.7891 - val_mae: 241.1024\n",
      "Epoch 74/200\n",
      "76/98 [======================>.......] - ETA: 0s - loss: 83735.1016 - mae: 254.5724\n",
      "Epoch 74: val_loss improved from 76249.78906 to 72704.85156, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 88593.0625 - mae: 257.0184 - val_loss: 72704.8516 - val_mae: 235.7028\n",
      "Epoch 75/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 85180.7734 - mae: 251.2536\n",
      "Epoch 75: val_loss improved from 72704.85156 to 69334.64062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 85180.7734 - mae: 251.2536 - val_loss: 69334.6406 - val_mae: 230.4036\n",
      "Epoch 76/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 81342.4844 - mae: 244.8761\n",
      "Epoch 76: val_loss improved from 69334.64062 to 66230.52344, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 81949.7344 - mae: 245.7956 - val_loss: 66230.5234 - val_mae: 225.3769\n",
      "Epoch 77/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 79462.3203 - mae: 241.7888\n",
      "Epoch 77: val_loss improved from 66230.52344 to 63229.30078, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 78850.3594 - mae: 240.5820 - val_loss: 63229.3008 - val_mae: 220.3021\n",
      "Epoch 78/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 77161.2422 - mae: 236.9272\n",
      "Epoch 78: val_loss improved from 63229.30078 to 60372.14453, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 75890.2656 - mae: 235.2513 - val_loss: 60372.1445 - val_mae: 215.2722\n",
      "Epoch 79/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 73111.9453 - mae: 230.1863\n",
      "Epoch 79: val_loss improved from 60372.14453 to 57636.85938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 73067.8906 - mae: 230.0884 - val_loss: 57636.8594 - val_mae: 210.2840\n",
      "Epoch 80/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 70834.0938 - mae: 225.6083\n",
      "Epoch 80: val_loss improved from 57636.85938 to 54983.76953, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 70330.4609 - mae: 225.0713 - val_loss: 54983.7695 - val_mae: 205.3700\n",
      "Epoch 81/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 68311.3438 - mae: 220.1030\n",
      "Epoch 81: val_loss improved from 54983.76953 to 52486.22656, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 67688.0938 - mae: 220.0728 - val_loss: 52486.2266 - val_mae: 200.6342\n",
      "Epoch 82/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 65395.9570 - mae: 215.3993\n",
      "Epoch 82: val_loss improved from 52486.22656 to 50047.30469, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 65163.1289 - mae: 215.1348 - val_loss: 50047.3047 - val_mae: 195.8719\n",
      "Epoch 83/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 63868.3906 - mae: 211.6280\n",
      "Epoch 83: val_loss improved from 50047.30469 to 47718.42578, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 62720.1914 - mae: 210.3616 - val_loss: 47718.4258 - val_mae: 191.2044\n",
      "Epoch 84/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 60601.0195 - mae: 205.5373\n",
      "Epoch 84: val_loss improved from 47718.42578 to 45472.35156, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 60321.3203 - mae: 205.4028 - val_loss: 45472.3516 - val_mae: 186.6232\n",
      "Epoch 85/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 59308.4414 - mae: 201.4016\n",
      "Epoch 85: val_loss improved from 45472.35156 to 43310.64844, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 58005.0039 - mae: 200.7212 - val_loss: 43310.6484 - val_mae: 182.0372\n",
      "Epoch 86/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 56411.8516 - mae: 196.5562\n",
      "Epoch 86: val_loss improved from 43310.64844 to 41212.17188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 55736.3047 - mae: 195.9463 - val_loss: 41212.1719 - val_mae: 177.4477\n",
      "Epoch 87/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 55029.0898 - mae: 192.5297\n",
      "Epoch 87: val_loss improved from 41212.17188 to 39176.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 53538.9062 - mae: 191.2136 - val_loss: 39176.7500 - val_mae: 172.8751\n",
      "Epoch 88/200\n",
      "75/98 [=====================>........] - ETA: 0s - loss: 47288.3164 - mae: 182.7583\n",
      "Epoch 88: val_loss improved from 39176.75000 to 37254.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 51411.5391 - mae: 186.2995 - val_loss: 37254.3125 - val_mae: 168.3772\n",
      "Epoch 89/200\n",
      "73/98 [=====================>........] - ETA: 0s - loss: 51942.2773 - mae: 182.7840\n",
      "Epoch 89: val_loss improved from 37254.31250 to 35355.38672, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 49313.7500 - mae: 181.6814 - val_loss: 35355.3867 - val_mae: 163.8840\n",
      "Epoch 90/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 46165.6992 - mae: 176.5820\n",
      "Epoch 90: val_loss improved from 35355.38672 to 33556.98828, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 47280.0508 - mae: 177.0464 - val_loss: 33556.9883 - val_mae: 159.4189\n",
      "Epoch 91/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 47133.6562 - mae: 174.8703\n",
      "Epoch 91: val_loss improved from 33556.98828 to 31802.83008, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 45317.9766 - mae: 172.3961 - val_loss: 31802.8301 - val_mae: 154.9783\n",
      "Epoch 92/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 43359.7188 - mae: 167.6066\n",
      "Epoch 92: val_loss improved from 31802.83008 to 30145.60156, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 43411.9609 - mae: 167.4896 - val_loss: 30145.6016 - val_mae: 150.5241\n",
      "Epoch 93/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 41975.0430 - mae: 163.8470\n",
      "Epoch 93: val_loss improved from 30145.60156 to 28559.31055, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 41550.5195 - mae: 163.1635 - val_loss: 28559.3105 - val_mae: 146.3279\n",
      "Epoch 94/200\n",
      "75/98 [=====================>........] - ETA: 0s - loss: 42157.4609 - mae: 159.6639\n",
      "Epoch 94: val_loss improved from 28559.31055 to 26996.91602, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 39720.4648 - mae: 158.5990 - val_loss: 26996.9160 - val_mae: 141.9320\n",
      "Epoch 95/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 38310.6211 - mae: 153.5893\n",
      "Epoch 95: val_loss improved from 26996.91602 to 25529.90039, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 37945.9414 - mae: 153.8600 - val_loss: 25529.9004 - val_mae: 137.6024\n",
      "Epoch 96/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 35623.2461 - mae: 148.4942\n",
      "Epoch 96: val_loss improved from 25529.90039 to 24104.78906, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 36238.5586 - mae: 149.3533 - val_loss: 24104.7891 - val_mae: 133.3182\n",
      "Epoch 97/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 34444.4609 - mae: 144.0645\n",
      "Epoch 97: val_loss improved from 24104.78906 to 22774.16797, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 34592.5273 - mae: 144.7717 - val_loss: 22774.1680 - val_mae: 129.1128\n",
      "Epoch 98/200\n",
      "78/98 [======================>.......] - ETA: 0s - loss: 33555.1055 - mae: 140.5063\n",
      "Epoch 98: val_loss improved from 22774.16797 to 21461.22070, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 32979.8750 - mae: 140.5407 - val_loss: 21461.2207 - val_mae: 124.9017\n",
      "Epoch 99/200\n",
      "74/98 [=====================>........] - ETA: 0s - loss: 33094.3672 - mae: 136.4303\n",
      "Epoch 99: val_loss improved from 21461.22070 to 20225.09570, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 31390.0176 - mae: 135.9420 - val_loss: 20225.0957 - val_mae: 120.7308\n",
      "Epoch 100/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 30091.9922 - mae: 131.8194\n",
      "Epoch 100: val_loss improved from 20225.09570 to 19029.44336, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 29874.9512 - mae: 131.6432 - val_loss: 19029.4434 - val_mae: 116.5747\n",
      "Epoch 101/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 28465.1152 - mae: 127.2560\n",
      "Epoch 101: val_loss improved from 19029.44336 to 17898.19336, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 28419.3672 - mae: 127.1419 - val_loss: 17898.1934 - val_mae: 112.5003\n",
      "Epoch 102/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 26318.3320 - mae: 122.7560\n",
      "Epoch 102: val_loss improved from 17898.19336 to 16811.96875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 27023.7246 - mae: 122.8901 - val_loss: 16811.9688 - val_mae: 108.4580\n",
      "Epoch 103/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 26123.2168 - mae: 120.6882\n",
      "Epoch 103: val_loss improved from 16811.96875 to 15789.83398, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 54ms/step - loss: 25680.1973 - mae: 118.7538 - val_loss: 15789.8340 - val_mae: 104.5799\n",
      "Epoch 104/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 24450.7090 - mae: 114.8221\n",
      "Epoch 104: val_loss improved from 15789.83398 to 14822.38770, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 24409.6934 - mae: 114.7094 - val_loss: 14822.3877 - val_mae: 100.8156\n",
      "Epoch 105/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 23388.4062 - mae: 110.5671\n",
      "Epoch 105: val_loss improved from 14822.38770 to 13905.02539, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 23194.4023 - mae: 110.5868 - val_loss: 13905.0254 - val_mae: 97.0453\n",
      "Epoch 106/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 22056.6211 - mae: 106.6740\n",
      "Epoch 106: val_loss improved from 13905.02539 to 13017.47461, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 22025.3301 - mae: 106.5937 - val_loss: 13017.4746 - val_mae: 93.3594\n",
      "Epoch 107/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 21767.5371 - mae: 103.5192\n",
      "Epoch 107: val_loss improved from 13017.47461 to 12194.91113, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 12s 120ms/step - loss: 20910.6367 - mae: 102.6002 - val_loss: 12194.9111 - val_mae: 89.8422\n",
      "Epoch 108/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 19850.8438 - mae: 99.0987 \n",
      "Epoch 108: val_loss improved from 12194.91113 to 11410.57031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 100ms/step - loss: 19826.6855 - mae: 99.0610 - val_loss: 11410.5703 - val_mae: 86.4381\n",
      "Epoch 109/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 18239.0273 - mae: 94.3444\n",
      "Epoch 109: val_loss improved from 11410.57031 to 10667.79590, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 18796.8711 - mae: 95.1445 - val_loss: 10667.7959 - val_mae: 82.8372\n",
      "Epoch 110/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 17426.4355 - mae: 91.2051\n",
      "Epoch 110: val_loss improved from 10667.79590 to 9914.16016, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 17801.5723 - mae: 91.4457 - val_loss: 9914.1602 - val_mae: 79.7239\n",
      "Epoch 111/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 17450.0449 - mae: 89.3552\n",
      "Epoch 111: val_loss improved from 9914.16016 to 9236.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 16846.3418 - mae: 88.1126 - val_loss: 9236.6875 - val_mae: 76.5222\n",
      "Epoch 112/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 15946.7998 - mae: 84.5090\n",
      "Epoch 112: val_loss improved from 9236.68750 to 8601.96191, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 54ms/step - loss: 15933.6846 - mae: 84.5183 - val_loss: 8601.9619 - val_mae: 73.3226\n",
      "Epoch 113/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 14983.4668 - mae: 80.5609\n",
      "Epoch 113: val_loss improved from 8601.96191 to 8002.14258, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 15057.0059 - mae: 80.8622 - val_loss: 8002.1426 - val_mae: 70.0539\n",
      "Epoch 114/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 14130.9092 - mae: 77.9932\n",
      "Epoch 114: val_loss improved from 8002.14258 to 7431.33691, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 14228.2705 - mae: 77.6493 - val_loss: 7431.3369 - val_mae: 67.2358\n",
      "Epoch 115/200\n",
      "72/98 [=====================>........] - ETA: 0s - loss: 14767.7070 - mae: 75.9389\n",
      "Epoch 115: val_loss improved from 7431.33691 to 6890.94727, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 13436.0117 - mae: 74.7680 - val_loss: 6890.9473 - val_mae: 64.3675\n",
      "Epoch 116/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 12620.2637 - mae: 71.0120\n",
      "Epoch 116: val_loss improved from 6890.94727 to 6394.80615, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 12682.7295 - mae: 71.4824 - val_loss: 6394.8062 - val_mae: 61.5682\n",
      "Epoch 117/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 11986.8838 - mae: 68.4133\n",
      "Epoch 117: val_loss improved from 6394.80615 to 5918.47949, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 11967.4590 - mae: 68.3543 - val_loss: 5918.4795 - val_mae: 58.8329\n",
      "Epoch 118/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 11860.6660 - mae: 66.0476\n",
      "Epoch 118: val_loss improved from 5918.47949 to 5485.77441, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 11303.1924 - mae: 65.4822 - val_loss: 5485.7744 - val_mae: 56.3623\n",
      "Epoch 119/200\n",
      "76/98 [======================>.......] - ETA: 0s - loss: 11613.2871 - mae: 64.1107\n",
      "Epoch 119: val_loss improved from 5485.77441 to 5074.29443, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 10672.7666 - mae: 62.8059 - val_loss: 5074.2944 - val_mae: 53.6470\n",
      "Epoch 120/200\n",
      "75/98 [=====================>........] - ETA: 0s - loss: 9546.3916 - mae: 58.4482\n",
      "Epoch 120: val_loss improved from 5074.29443 to 4698.29590, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 10060.8027 - mae: 59.8046 - val_loss: 4698.2959 - val_mae: 51.2784\n",
      "Epoch 121/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 8810.1455 - mae: 56.8304\n",
      "Epoch 121: val_loss improved from 4698.29590 to 4343.05469, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9494.7412 - mae: 57.2583 - val_loss: 4343.0547 - val_mae: 48.9556\n",
      "Epoch 122/200\n",
      "74/98 [=====================>........] - ETA: 0s - loss: 8943.6074 - mae: 53.8358\n",
      "Epoch 122: val_loss improved from 4343.05469 to 4018.61719, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 8960.3232 - mae: 54.8779 - val_loss: 4018.6172 - val_mae: 46.7422\n",
      "Epoch 123/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 8653.1152 - mae: 52.9100\n",
      "Epoch 123: val_loss improved from 4018.61719 to 3705.25098, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 8456.9238 - mae: 52.4442 - val_loss: 3705.2510 - val_mae: 44.6891\n",
      "Epoch 124/200\n",
      "76/98 [======================>.......] - ETA: 0s - loss: 8034.2632 - mae: 50.8323\n",
      "Epoch 124: val_loss improved from 3705.25098 to 3421.41675, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 7974.4639 - mae: 50.3218 - val_loss: 3421.4167 - val_mae: 42.7055\n",
      "Epoch 125/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 7812.4414 - mae: 48.7463\n",
      "Epoch 125: val_loss improved from 3421.41675 to 3149.61084, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 7527.3994 - mae: 48.2572 - val_loss: 3149.6108 - val_mae: 40.8113\n",
      "Epoch 126/200\n",
      "78/98 [======================>.......] - ETA: 0s - loss: 7809.4634 - mae: 47.0594\n",
      "Epoch 126: val_loss improved from 3149.61084 to 2900.65942, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 7105.6982 - mae: 46.0498 - val_loss: 2900.6594 - val_mae: 38.8047\n",
      "Epoch 127/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 6598.5298 - mae: 44.4278\n",
      "Epoch 127: val_loss improved from 2900.65942 to 2669.53638, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 46ms/step - loss: 6705.5381 - mae: 43.9409 - val_loss: 2669.5364 - val_mae: 37.0071\n",
      "Epoch 128/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 6390.4458 - mae: 42.9299\n",
      "Epoch 128: val_loss improved from 2669.53638 to 2462.27173, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 6331.5576 - mae: 42.1604 - val_loss: 2462.2717 - val_mae: 35.4257\n",
      "Epoch 129/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 6013.4438 - mae: 40.2619\n",
      "Epoch 129: val_loss improved from 2462.27173 to 2268.60986, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 5971.5923 - mae: 40.1920 - val_loss: 2268.6099 - val_mae: 33.8826\n",
      "Epoch 130/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 4799.9795 - mae: 37.5833\n",
      "Epoch 130: val_loss improved from 2268.60986 to 2092.86206, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 5634.2656 - mae: 38.5366 - val_loss: 2092.8621 - val_mae: 32.3406\n",
      "Epoch 131/200\n",
      "75/98 [=====================>........] - ETA: 0s - loss: 6093.6367 - mae: 38.2185\n",
      "Epoch 131: val_loss improved from 2092.86206 to 1919.27502, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 5311.7705 - mae: 37.1528 - val_loss: 1919.2750 - val_mae: 31.0050\n",
      "Epoch 132/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 4973.4189 - mae: 35.5574\n",
      "Epoch 132: val_loss improved from 1919.27502 to 1772.15430, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 5010.5610 - mae: 35.1975 - val_loss: 1772.1543 - val_mae: 29.5359\n",
      "Epoch 133/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 4605.9336 - mae: 33.4478\n",
      "Epoch 133: val_loss improved from 1772.15430 to 1633.28137, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 4731.3809 - mae: 33.6479 - val_loss: 1633.2814 - val_mae: 28.3000\n",
      "Epoch 134/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 4463.2651 - mae: 32.5734\n",
      "Epoch 134: val_loss improved from 1633.28137 to 1505.47107, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 4463.2651 - mae: 32.5734 - val_loss: 1505.4711 - val_mae: 27.1473\n",
      "Epoch 135/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 4326.6982 - mae: 31.1718\n",
      "Epoch 135: val_loss improved from 1505.47107 to 1389.47607, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 4202.6216 - mae: 31.0838 - val_loss: 1389.4761 - val_mae: 26.0702\n",
      "Epoch 136/200\n",
      "73/98 [=====================>........] - ETA: 0s - loss: 3253.6592 - mae: 29.0561\n",
      "Epoch 136: val_loss improved from 1389.47607 to 1283.14148, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3947.9277 - mae: 29.7017 - val_loss: 1283.1415 - val_mae: 25.0387\n",
      "Epoch 137/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 3861.8411 - mae: 28.8887\n",
      "Epoch 137: val_loss improved from 1283.14148 to 1182.13293, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3714.2268 - mae: 28.4729 - val_loss: 1182.1329 - val_mae: 24.0568\n",
      "Epoch 138/200\n",
      "76/98 [======================>.......] - ETA: 0s - loss: 3762.2805 - mae: 27.4802\n",
      "Epoch 138: val_loss improved from 1182.13293 to 1092.89294, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 3491.2759 - mae: 27.1970 - val_loss: 1092.8929 - val_mae: 23.0209\n",
      "Epoch 139/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 3408.3484 - mae: 26.7510\n",
      "Epoch 139: val_loss improved from 1092.89294 to 1009.84631, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 3286.6089 - mae: 26.2428 - val_loss: 1009.8463 - val_mae: 22.1808\n",
      "Epoch 140/200\n",
      "77/98 [======================>.......] - ETA: 0s - loss: 2251.2310 - mae: 23.7522\n",
      "Epoch 140: val_loss improved from 1009.84631 to 935.51752, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 3093.8557 - mae: 25.0295 - val_loss: 935.5175 - val_mae: 21.3766\n",
      "Epoch 141/200\n",
      "78/98 [======================>.......] - ETA: 0s - loss: 2985.8630 - mae: 23.8388\n",
      "Epoch 141: val_loss improved from 935.51752 to 864.49261, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 2917.3926 - mae: 24.2365 - val_loss: 864.4926 - val_mae: 20.5578\n",
      "Epoch 142/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 2955.3044 - mae: 24.0410\n",
      "Epoch 142: val_loss improved from 864.49261 to 799.55719, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 2754.0630 - mae: 23.4582 - val_loss: 799.5572 - val_mae: 19.8319\n",
      "Epoch 143/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 2677.7263 - mae: 22.6040\n",
      "Epoch 143: val_loss improved from 799.55719 to 742.42810, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 2602.0698 - mae: 22.3751 - val_loss: 742.4281 - val_mae: 19.1741\n",
      "Epoch 144/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 2489.4114 - mae: 21.7330\n",
      "Epoch 144: val_loss improved from 742.42810 to 687.51013, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 2463.8325 - mae: 21.6482 - val_loss: 687.5101 - val_mae: 18.4649\n",
      "Epoch 145/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 2297.6482 - mae: 20.8006\n",
      "Epoch 145: val_loss improved from 687.51013 to 638.06384, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 2330.7593 - mae: 20.9033 - val_loss: 638.0638 - val_mae: 17.9390\n",
      "Epoch 146/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 2309.4404 - mae: 20.6728\n",
      "Epoch 146: val_loss improved from 638.06384 to 592.46002, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 2209.0737 - mae: 20.3245 - val_loss: 592.4600 - val_mae: 17.3476\n",
      "Epoch 147/200\n",
      "74/98 [=====================>........] - ETA: 0s - loss: 2350.0674 - mae: 19.9330\n",
      "Epoch 147: val_loss improved from 592.46002 to 554.30481, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2096.8303 - mae: 19.5516 - val_loss: 554.3048 - val_mae: 16.8584\n",
      "Epoch 148/200\n",
      "77/98 [======================>.......] - ETA: 0s - loss: 2029.0193 - mae: 19.0728\n",
      "Epoch 148: val_loss improved from 554.30481 to 515.09290, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 1989.8229 - mae: 18.9113 - val_loss: 515.0929 - val_mae: 16.3650\n",
      "Epoch 149/200\n",
      "79/98 [=======================>......] - ETA: 0s - loss: 2084.1111 - mae: 18.7929\n",
      "Epoch 149: val_loss improved from 515.09290 to 485.56784, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 1890.8911 - mae: 18.4965 - val_loss: 485.5678 - val_mae: 15.8114\n",
      "Epoch 150/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1843.1564 - mae: 17.9926\n",
      "Epoch 150: val_loss improved from 485.56784 to 453.77930, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 1794.0198 - mae: 17.8216 - val_loss: 453.7793 - val_mae: 15.3104\n",
      "Epoch 151/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1720.1836 - mae: 17.3211\n",
      "Epoch 151: val_loss improved from 453.77930 to 428.21179, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 1702.4965 - mae: 17.2591 - val_loss: 428.2118 - val_mae: 14.9365\n",
      "Epoch 152/200\n",
      "75/98 [=====================>........] - ETA: 0s - loss: 1424.5839 - mae: 16.9977\n",
      "Epoch 152: val_loss improved from 428.21179 to 403.17755, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 1613.2828 - mae: 16.7899 - val_loss: 403.1776 - val_mae: 14.5844\n",
      "Epoch 153/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 1662.7053 - mae: 16.4436\n",
      "Epoch 153: val_loss improved from 403.17755 to 378.29904, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1528.7393 - mae: 16.2802 - val_loss: 378.2990 - val_mae: 14.1735\n",
      "Epoch 154/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1592.9556 - mae: 16.2403\n",
      "Epoch 154: val_loss improved from 378.29904 to 356.37503, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 1441.1362 - mae: 15.8192 - val_loss: 356.3750 - val_mae: 13.7765\n",
      "Epoch 155/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1410.1611 - mae: 15.4435\n",
      "Epoch 155: val_loss improved from 356.37503 to 333.27170, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 1351.2372 - mae: 15.2339 - val_loss: 333.2717 - val_mae: 13.3440\n",
      "Epoch 156/200\n",
      "77/98 [======================>.......] - ETA: 0s - loss: 957.9791 - mae: 14.1125\n",
      "Epoch 156: val_loss improved from 333.27170 to 315.39365, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1272.3495 - mae: 14.5491 - val_loss: 315.3936 - val_mae: 13.0116\n",
      "Epoch 157/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1044.4486 - mae: 13.7970\n",
      "Epoch 157: val_loss improved from 315.39365 to 298.09296, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 1200.5778 - mae: 14.1866 - val_loss: 298.0930 - val_mae: 12.5983\n",
      "Epoch 158/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1114.9050 - mae: 13.5999\n",
      "Epoch 158: val_loss improved from 298.09296 to 285.37726, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 1137.1908 - mae: 13.7192 - val_loss: 285.3773 - val_mae: 12.2441\n",
      "Epoch 159/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1117.1182 - mae: 13.3944\n",
      "Epoch 159: val_loss improved from 285.37726 to 271.83615, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 1075.5067 - mae: 13.2531 - val_loss: 271.8362 - val_mae: 11.8415\n",
      "Epoch 160/200\n",
      "78/98 [======================>.......] - ETA: 0s - loss: 1220.4904 - mae: 13.4674\n",
      "Epoch 160: val_loss improved from 271.83615 to 261.24292, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 1025.3029 - mae: 12.9901 - val_loss: 261.2429 - val_mae: 11.5738\n",
      "Epoch 161/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1034.2037 - mae: 12.7857\n",
      "Epoch 161: val_loss improved from 261.24292 to 253.46864, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 984.9728 - mae: 12.6077 - val_loss: 253.4686 - val_mae: 11.3324\n",
      "Epoch 162/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 937.4459 - mae: 12.4880\n",
      "Epoch 162: val_loss did not improve from 253.46864\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 937.6826 - mae: 12.5473 - val_loss: 255.0819 - val_mae: 11.2860\n",
      "Epoch 163/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 612.6600 - mae: 11.6622\n",
      "Epoch 163: val_loss improved from 253.46864 to 242.78365, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 902.1427 - mae: 12.1818 - val_loss: 242.7836 - val_mae: 10.9974\n",
      "Epoch 164/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 861.9166 - mae: 11.8314\n",
      "Epoch 164: val_loss improved from 242.78365 to 236.50354, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 870.4827 - mae: 11.9380 - val_loss: 236.5035 - val_mae: 10.8576\n",
      "Epoch 165/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 832.4442 - mae: 11.7162\n",
      "Epoch 165: val_loss improved from 236.50354 to 228.29747, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 832.4442 - mae: 11.7162 - val_loss: 228.2975 - val_mae: 10.5855\n",
      "Epoch 166/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 810.4072 - mae: 11.3792\n",
      "Epoch 166: val_loss improved from 228.29747 to 219.25609, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 802.8131 - mae: 11.3776 - val_loss: 219.2561 - val_mae: 10.3967\n",
      "Epoch 167/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 729.6368 - mae: 10.9406\n",
      "Epoch 167: val_loss improved from 219.25609 to 212.17862, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 774.2753 - mae: 11.1394 - val_loss: 212.1786 - val_mae: 10.1837\n",
      "Epoch 168/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 736.5682 - mae: 10.8998\n",
      "Epoch 168: val_loss improved from 212.17862 to 205.87222, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 748.1453 - mae: 10.9664 - val_loss: 205.8722 - val_mae: 9.9430\n",
      "Epoch 169/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 705.8573 - mae: 10.5421\n",
      "Epoch 169: val_loss improved from 205.87222 to 197.54529, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 722.2994 - mae: 10.7313 - val_loss: 197.5453 - val_mae: 9.7635\n",
      "Epoch 170/200\n",
      "75/98 [=====================>........] - ETA: 0s - loss: 586.4620 - mae: 10.2310\n",
      "Epoch 170: val_loss improved from 197.54529 to 189.97972, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 697.3507 - mae: 10.4922 - val_loss: 189.9797 - val_mae: 9.5714\n",
      "Epoch 171/200\n",
      "78/98 [======================>.......] - ETA: 0s - loss: 412.6328 - mae: 9.6161\n",
      "Epoch 171: val_loss improved from 189.97972 to 184.08192, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 673.1964 - mae: 10.2744 - val_loss: 184.0819 - val_mae: 9.3599\n",
      "Epoch 172/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 658.4787 - mae: 10.1711\n",
      "Epoch 172: val_loss improved from 184.08192 to 174.61238, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 648.6719 - mae: 10.1495 - val_loss: 174.6124 - val_mae: 9.1629\n",
      "Epoch 173/200\n",
      "75/98 [=====================>........] - ETA: 0s - loss: 758.6054 - mae: 10.3257\n",
      "Epoch 173: val_loss improved from 174.61238 to 170.29378, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 623.5907 - mae: 9.8907 - val_loss: 170.2938 - val_mae: 8.9891\n",
      "Epoch 174/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 627.2019 - mae: 9.7979 \n",
      "Epoch 174: val_loss improved from 170.29378 to 163.53543, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 604.2874 - mae: 9.7051 - val_loss: 163.5354 - val_mae: 8.8164\n",
      "Epoch 175/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 642.6110 - mae: 9.7166\n",
      "Epoch 175: val_loss improved from 163.53543 to 159.03381, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 582.5421 - mae: 9.5532 - val_loss: 159.0338 - val_mae: 8.6766\n",
      "Epoch 176/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 544.4968 - mae: 9.2480\n",
      "Epoch 176: val_loss improved from 159.03381 to 153.79604, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 563.9620 - mae: 9.3488 - val_loss: 153.7960 - val_mae: 8.4926\n",
      "Epoch 177/200\n",
      "79/98 [=======================>......] - ETA: 0s - loss: 642.6890 - mae: 9.5431\n",
      "Epoch 177: val_loss improved from 153.79604 to 149.73592, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 544.6913 - mae: 9.2096 - val_loss: 149.7359 - val_mae: 8.3712\n",
      "Epoch 178/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 464.7289 - mae: 8.9176\n",
      "Epoch 178: val_loss improved from 149.73592 to 145.61551, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 527.1152 - mae: 9.0520 - val_loss: 145.6155 - val_mae: 8.2442\n",
      "Epoch 179/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 531.4056 - mae: 9.1174\n",
      "Epoch 179: val_loss improved from 145.61551 to 142.09038, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 511.5831 - mae: 8.9942 - val_loss: 142.0904 - val_mae: 8.1301\n",
      "Epoch 180/200\n",
      "79/98 [=======================>......] - ETA: 0s - loss: 569.2734 - mae: 9.1015 \n",
      "Epoch 180: val_loss improved from 142.09038 to 137.68164, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 494.7537 - mae: 8.8413 - val_loss: 137.6816 - val_mae: 8.0232\n",
      "Epoch 181/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 509.0872 - mae: 8.7494\n",
      "Epoch 181: val_loss improved from 137.68164 to 135.54007, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 480.9811 - mae: 8.7252 - val_loss: 135.5401 - val_mae: 7.9207\n",
      "Epoch 182/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 397.9633 - mae: 8.2584\n",
      "Epoch 182: val_loss improved from 135.54007 to 133.43065, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 464.1214 - mae: 8.5624 - val_loss: 133.4306 - val_mae: 7.8322\n",
      "Epoch 183/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 393.9398 - mae: 8.4062\n",
      "Epoch 183: val_loss improved from 133.43065 to 132.66257, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 449.5238 - mae: 8.5228 - val_loss: 132.6626 - val_mae: 7.7519\n",
      "Epoch 184/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 439.5883 - mae: 8.3109\n",
      "Epoch 184: val_loss improved from 132.66257 to 131.77116, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 436.4679 - mae: 8.3712 - val_loss: 131.7712 - val_mae: 7.6687\n",
      "Epoch 185/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 419.9099 - mae: 8.1864\n",
      "Epoch 185: val_loss improved from 131.77116 to 127.09073, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 422.1370 - mae: 8.2622 - val_loss: 127.0907 - val_mae: 7.5657\n",
      "Epoch 186/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 431.9249 - mae: 8.2744\n",
      "Epoch 186: val_loss improved from 127.09073 to 125.03794, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 411.4653 - mae: 8.2161 - val_loss: 125.0379 - val_mae: 7.5012\n",
      "Epoch 187/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 437.3567 - mae: 8.1911\n",
      "Epoch 187: val_loss improved from 125.03794 to 121.91393, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 398.5565 - mae: 8.1330 - val_loss: 121.9139 - val_mae: 7.3977\n",
      "Epoch 188/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 396.1881 - mae: 8.0496\n",
      "Epoch 188: val_loss improved from 121.91393 to 120.36748, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 386.6296 - mae: 8.0250 - val_loss: 120.3675 - val_mae: 7.3219\n",
      "Epoch 189/200\n",
      "78/98 [======================>.......] - ETA: 0s - loss: 440.9649 - mae: 8.0871\n",
      "Epoch 189: val_loss improved from 120.36748 to 116.40799, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 374.8452 - mae: 7.9306 - val_loss: 116.4080 - val_mae: 7.2638\n",
      "Epoch 190/200\n",
      "76/98 [======================>.......] - ETA: 0s - loss: 429.1884 - mae: 8.0834\n",
      "Epoch 190: val_loss improved from 116.40799 to 113.73434, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 364.4364 - mae: 7.8680 - val_loss: 113.7343 - val_mae: 7.1847\n",
      "Epoch 191/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 359.1595 - mae: 7.7990\n",
      "Epoch 191: val_loss improved from 113.73434 to 111.38573, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 353.2603 - mae: 7.7931 - val_loss: 111.3857 - val_mae: 7.1510\n",
      "Epoch 192/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 352.4554 - mae: 7.7368\n",
      "Epoch 192: val_loss improved from 111.38573 to 106.02207, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 342.3145 - mae: 7.7289 - val_loss: 106.0221 - val_mae: 7.0729\n",
      "Epoch 193/200\n",
      "76/98 [======================>.......] - ETA: 0s - loss: 398.9123 - mae: 7.8914\n",
      "Epoch 193: val_loss improved from 106.02207 to 101.22098, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 332.5175 - mae: 7.6544 - val_loss: 101.2210 - val_mae: 6.9872\n",
      "Epoch 194/200\n",
      "76/98 [======================>.......] - ETA: 0s - loss: 340.7480 - mae: 7.4321\n",
      "Epoch 194: val_loss improved from 101.22098 to 97.90400, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 321.9482 - mae: 7.5762 - val_loss: 97.9040 - val_mae: 6.9359\n",
      "Epoch 195/200\n",
      "74/98 [=====================>........] - ETA: 0s - loss: 250.4355 - mae: 7.5046\n",
      "Epoch 195: val_loss improved from 97.90400 to 95.25067, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 311.0113 - mae: 7.5302 - val_loss: 95.2507 - val_mae: 6.8670\n",
      "Epoch 196/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 281.9228 - mae: 7.4109\n",
      "Epoch 196: val_loss improved from 95.25067 to 88.98186, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 301.1565 - mae: 7.4123 - val_loss: 88.9819 - val_mae: 6.7893\n",
      "Epoch 197/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 326.5785 - mae: 7.6043\n",
      "Epoch 197: val_loss improved from 88.98186 to 86.22536, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 291.1396 - mae: 7.4019 - val_loss: 86.2254 - val_mae: 6.7602\n",
      "Epoch 198/200\n",
      "78/98 [======================>.......] - ETA: 0s - loss: 318.9378 - mae: 7.4262\n",
      "Epoch 198: val_loss improved from 86.22536 to 83.73856, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 279.3942 - mae: 7.2795 - val_loss: 83.7386 - val_mae: 6.6796\n",
      "Epoch 199/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 296.4735 - mae: 7.2765\n",
      "Epoch 199: val_loss improved from 83.73856 to 81.95739, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 269.2634 - mae: 7.2349 - val_loss: 81.9574 - val_mae: 6.6321\n",
      "Epoch 200/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 274.4091 - mae: 7.2443\n",
      "Epoch 200: val_loss improved from 81.95739 to 80.92496, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 258.4217 - mae: 7.1846 - val_loss: 80.9250 - val_mae: 6.6212\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일: 모델학습을 위한 학습과정 설정단계\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "# print(\"\\n Accuracy: %.4f\" % model.evaluate(x_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1fa2d2f9820>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_feature)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(test_label, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.grid\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7adfe9f3775c954d01fc005199cfce03b4193bbc6ff21451e4d1eaf7785b4e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
