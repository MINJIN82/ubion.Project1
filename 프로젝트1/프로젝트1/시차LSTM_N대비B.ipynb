{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>종가_ex</th>\n",
       "      <th>1Y_Mid_irs</th>\n",
       "      <th>2Y_Mid_irs</th>\n",
       "      <th>3Y_Mid_irs</th>\n",
       "      <th>5Y_Mid_irs</th>\n",
       "      <th>10Y_Mid_irs</th>\n",
       "      <th>1Y_Mid_crs</th>\n",
       "      <th>2Y_Mid_crs</th>\n",
       "      <th>3Y_Mid_crs</th>\n",
       "      <th>...</th>\n",
       "      <th>국고10년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>통안2년대비</th>\n",
       "      <th>전일비_1Y_베이시스</th>\n",
       "      <th>전일비_2Y_베이시스</th>\n",
       "      <th>전일비_3Y_베이시스</th>\n",
       "      <th>전일비_5Y_베이시스</th>\n",
       "      <th>전일비_10Y_베이시스</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_NDF차이</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>2.820</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.860</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.845</td>\n",
       "      <td>1.85</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1126.5</td>\n",
       "      <td>-7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>2</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>2.790</td>\n",
       "      <td>2.660</td>\n",
       "      <td>2.660</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.840</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.830</td>\n",
       "      <td>1.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>-6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>3</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>2.810</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.710</td>\n",
       "      <td>2.850</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.805</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>4</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>2.820</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.870</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.820</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>5</td>\n",
       "      <td>1128.3</td>\n",
       "      <td>2.830</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.740</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.820</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>-1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>2455</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.235</td>\n",
       "      <td>3.205</td>\n",
       "      <td>3.125</td>\n",
       "      <td>2.965</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.730</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>2456</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>3.155</td>\n",
       "      <td>3.215</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.095</td>\n",
       "      <td>2.935</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.68</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>2457</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>3.145</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.115</td>\n",
       "      <td>3.035</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>-2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>2458</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.205</td>\n",
       "      <td>3.165</td>\n",
       "      <td>3.085</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.730</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>7.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>2459</td>\n",
       "      <td>1299.1</td>\n",
       "      <td>3.105</td>\n",
       "      <td>3.065</td>\n",
       "      <td>3.025</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.825</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.560</td>\n",
       "      <td>2.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0   종가_ex  1Y_Mid_irs  2Y_Mid_irs  3Y_Mid_irs  \\\n",
       "DateTime                                                             \n",
       "2012-08-02           1  1131.7       2.820       2.690       2.690   \n",
       "2012-08-03           2  1134.8       2.790       2.660       2.660   \n",
       "2012-08-06           3  1129.0       2.810       2.680       2.680   \n",
       "2012-08-07           4  1128.8       2.820       2.680       2.680   \n",
       "2012-08-08           5  1128.3       2.830       2.700       2.700   \n",
       "...                ...     ...         ...         ...         ...   \n",
       "2022-07-25        2455  1313.7       3.165       3.235       3.205   \n",
       "2022-07-26        2456  1307.6       3.155       3.215       3.175   \n",
       "2022-07-27        2457  1313.3       3.145       3.165       3.115   \n",
       "2022-07-28        2458  1296.1       3.175       3.205       3.165   \n",
       "2022-07-29        2459  1299.1       3.105       3.065       3.025   \n",
       "\n",
       "            5Y_Mid_irs  10Y_Mid_irs  1Y_Mid_crs  2Y_Mid_crs  3Y_Mid_crs  ...  \\\n",
       "DateTime                                                                 ...   \n",
       "2012-08-02       2.720        2.860        2.08       1.845        1.85  ...   \n",
       "2012-08-03       2.690        2.840        2.07       1.830        1.83  ...   \n",
       "2012-08-06       2.710        2.850        2.07       1.805        1.80  ...   \n",
       "2012-08-07       2.720        2.870        2.09       1.820        1.80  ...   \n",
       "2012-08-08       2.740        2.900        2.10       1.820        1.80  ...   \n",
       "...                ...          ...         ...         ...         ...  ...   \n",
       "2022-07-25       3.125        2.965        2.55       2.730        2.71  ...   \n",
       "2022-07-26       3.095        2.935        2.56       2.700        2.68  ...   \n",
       "2022-07-27       3.035        2.875        2.57       2.690        2.67  ...   \n",
       "2022-07-28       3.085        2.945        2.61       2.730        2.71  ...   \n",
       "2022-07-29       2.945        2.825        2.50       2.560        2.56  ...   \n",
       "\n",
       "            국고10년대비  통안1년대비  통안2년대비  전일비_1Y_베이시스  전일비_2Y_베이시스  전일비_3Y_베이시스  \\\n",
       "DateTime                                                                     \n",
       "2012-08-02    -0.21   -0.03    0.02          2.0          8.0          9.0   \n",
       "2012-08-03    -0.03   -0.03    0.00          2.0          1.5          1.0   \n",
       "2012-08-06    -0.03   -0.01    0.02         -2.0         -4.5         -5.0   \n",
       "2012-08-07    -0.04   -0.01   -0.01          1.0          1.5          0.0   \n",
       "2012-08-08    -0.04   -0.06   -0.03          0.0         -2.0         -2.0   \n",
       "...             ...     ...     ...          ...          ...          ...   \n",
       "2022-07-25    -0.11    0.05    0.00         -4.0         -1.0          0.0   \n",
       "2022-07-26    -0.08    0.01   -0.05          2.0         -1.0          0.0   \n",
       "2022-07-27     0.01   -0.01   -0.02          2.0          4.0          5.0   \n",
       "2022-07-28     0.06    0.00    0.00          1.0          0.0         -1.0   \n",
       "2022-07-29     0.02    0.12    0.01         -4.0         -3.0         -1.0   \n",
       "\n",
       "            전일비_5Y_베이시스  전일비_10Y_베이시스  전날 종가_ex  종가_NDF차이  \n",
       "DateTime                                                   \n",
       "2012-08-02          9.0           9.0    1126.5     -7.50  \n",
       "2012-08-03         -5.0         -13.0    1131.7     -6.30  \n",
       "2012-08-06         -6.0          -5.0    1134.8      6.30  \n",
       "2012-08-07         -8.0         -10.0    1129.0      0.00  \n",
       "2012-08-08         -4.0          -7.0    1128.8     -1.45  \n",
       "...                 ...           ...       ...       ...  \n",
       "2022-07-25         -2.0           0.0    1313.0      3.15  \n",
       "2022-07-26          1.0           1.0    1313.7      2.70  \n",
       "2022-07-27          5.0           5.0    1307.6     -2.90  \n",
       "2022-07-28         -2.0          -5.0    1313.3      7.30  \n",
       "2022-07-29          5.0           3.0    1296.1      0.35  \n",
       "\n",
       "[2459 rows x 51 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"./xlsx/시차상관분석4Data.xlsx\",index_col=0)\n",
    "\n",
    "df = df.set_index(\"DateTime\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', '종가_ex', '1Y_Mid_irs', '2Y_Mid_irs', '3Y_Mid_irs',\n",
       "       '5Y_Mid_irs', '10Y_Mid_irs', '1Y_Mid_crs', '2Y_Mid_crs', '3Y_Mid_crs',\n",
       "       '5Y_Mid_crs', '10Y_Mid_crs', '국고1년', '국고3년', '국고5년', '국고10년', '통안364일',\n",
       "       '통안2년', 'Mid_ndf', '전일비_ndf', '1Y_베이시스', '2Y_베이시스', '3Y_베이시스',\n",
       "       '5Y_베이시스', '10Y_베이시스', 'M1_스왑포인트', '전일대비_종가_ex', '등락률_종가_ex',\n",
       "       '전일비_1Y_irs', '전일비_2Y_irs', '전일비_3Y_irs', '전일비_5Y_irs', '전일비_10Y_irs',\n",
       "       '전일비_1Y_crs', '전일비_2Y_crs', '전일비_3Y_crs', '전일비_5Y_crs', '전일비_10Y_crs',\n",
       "       '국고1년대비', '국고3년대비', '국고5년대비', '국고10년대비', '통안1년대비', '통안2년대비',\n",
       "       '전일비_1Y_베이시스', '전일비_2Y_베이시스', '전일비_3Y_베이시스', '전일비_5Y_베이시스',\n",
       "       '전일비_10Y_베이시스', '전날 종가_ex', '종가_NDF차이'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['종가_NDF차이'] = df['전날 종가_ex'] - df['Mid_ndf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_32708\\2998267214.py:9: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  x.feature = x.columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전일비_1Y_베이시스</th>\n",
       "      <th>국고1년대비</th>\n",
       "      <th>국고3년대비</th>\n",
       "      <th>국고5년대비</th>\n",
       "      <th>국고10년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>통안2년대비</th>\n",
       "      <th>전일비_ndf</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>1.898958</td>\n",
       "      <td>-0.488709</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>-5.377146</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>0.267777</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>-0.149841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.767092</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.184972</td>\n",
       "      <td>-0.056232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350946</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>2.466603</td>\n",
       "      <td>-0.767092</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>0.267777</td>\n",
       "      <td>-0.449862</td>\n",
       "      <td>-0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>-0.564203</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>-0.504502</td>\n",
       "      <td>-1.023207</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>-0.111668</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>-0.104837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.564203</td>\n",
       "      <td>-0.324448</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-1.023207</td>\n",
       "      <td>-0.650470</td>\n",
       "      <td>-0.364632</td>\n",
       "      <td>0.055654</td>\n",
       "      <td>-0.108437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-1.179994</td>\n",
       "      <td>1.975212</td>\n",
       "      <td>-0.234402</td>\n",
       "      <td>-2.816005</td>\n",
       "      <td>0.541331</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.154406</td>\n",
       "      <td>3.207485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.667378</td>\n",
       "      <td>0.825382</td>\n",
       "      <td>-1.314804</td>\n",
       "      <td>-2.047663</td>\n",
       "      <td>0.107949</td>\n",
       "      <td>-0.617595</td>\n",
       "      <td>0.050952</td>\n",
       "      <td>3.220086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>0.661120</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>0.257364</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>-0.238150</td>\n",
       "      <td>-0.026639</td>\n",
       "      <td>3.110275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>1.283168</td>\n",
       "      <td>1.153905</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>1.537934</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>-0.214738</td>\n",
       "      <td>3.212885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>0.667378</td>\n",
       "      <td>-0.652971</td>\n",
       "      <td>0.305799</td>\n",
       "      <td>0.513478</td>\n",
       "      <td>1.299750</td>\n",
       "      <td>0.141295</td>\n",
       "      <td>-0.485131</td>\n",
       "      <td>2.903255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            전일비_1Y_베이시스    국고1년대비    국고3년대비    국고5년대비   국고10년대비    통안1년대비  \\\n",
       "DateTime                                                                    \n",
       "2012-08-02     0.348741  1.898958 -0.488709 -1.584904 -5.377146 -0.325433   \n",
       "2012-08-03     0.348741  0.051587  0.004075  0.035699 -0.767092 -0.325433   \n",
       "2012-08-06    -0.350946  0.051587 -0.160187  2.466603 -0.767092 -0.108742   \n",
       "2012-08-07     0.173819 -0.564203 -0.160187 -0.504502 -1.023207 -0.108742   \n",
       "2012-08-08    -0.001103 -0.564203 -0.324448  0.035699 -1.023207 -0.650470   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "2022-07-25    -0.700790 -1.179994  1.975212 -0.234402 -2.816005  0.541331   \n",
       "2022-07-26     0.348741  0.667378  0.825382 -1.314804 -2.047663  0.107949   \n",
       "2022-07-27     0.348741  0.051587  0.661120 -1.584904  0.257364 -0.108742   \n",
       "2022-07-28     0.173819  1.283168  1.153905  0.035699  1.537934 -0.000397   \n",
       "2022-07-29    -0.700790  0.667378 -0.652971  0.305799  0.513478  1.299750   \n",
       "\n",
       "              통안2년대비   전일비_ndf  전날 종가_ex  \n",
       "DateTime                                  \n",
       "2012-08-02  0.267777  0.079167 -0.149841  \n",
       "2012-08-03  0.014814  0.184972 -0.056232  \n",
       "2012-08-06  0.267777 -0.449862 -0.000426  \n",
       "2012-08-07 -0.111668  0.020386 -0.104837  \n",
       "2012-08-08 -0.364632  0.055654 -0.108437  \n",
       "...              ...       ...       ...  \n",
       "2022-07-25  0.014814  0.154406  3.207485  \n",
       "2022-07-26 -0.617595  0.050952  3.220086  \n",
       "2022-07-27 -0.238150 -0.026639  3.110275  \n",
       "2022-07-28  0.014814 -0.214738  3.212885  \n",
       "2022-07-29  0.141295 -0.485131  2.903255  \n",
       "\n",
       "[2459 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df[['전일비_1Y_베이시스', '국고1년대비',  '국고3년대비',  '국고5년대비',  '국고10년대비', '통안1년대비', '통안2년대비',\n",
    "            '전일비_ndf', '전날 종가_ex']]\n",
    "y = df['종가_ex']\n",
    "\n",
    "# 이건 이렇게 해야 밑에 코드 8번째 줄 columns에 들어갈 수 있다고 하네요!\n",
    "x.feature = x.columns \n",
    "x.feature\n",
    "\n",
    "# scaling 진행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전일비_1Y_베이시스</th>\n",
       "      <th>국고1년대비</th>\n",
       "      <th>국고3년대비</th>\n",
       "      <th>국고5년대비</th>\n",
       "      <th>국고10년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>통안2년대비</th>\n",
       "      <th>전일비_ndf</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>1.898958</td>\n",
       "      <td>-0.488709</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>-5.377146</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>0.267777</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>1131.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-0.767092</td>\n",
       "      <td>-0.325433</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.184972</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350946</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>2.466603</td>\n",
       "      <td>-0.767092</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>0.267777</td>\n",
       "      <td>-0.449862</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>-0.564203</td>\n",
       "      <td>-0.160187</td>\n",
       "      <td>-0.504502</td>\n",
       "      <td>-1.023207</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>-0.111668</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.564203</td>\n",
       "      <td>-0.324448</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>-1.023207</td>\n",
       "      <td>-0.650470</td>\n",
       "      <td>-0.364632</td>\n",
       "      <td>0.055654</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-1.179994</td>\n",
       "      <td>1.975212</td>\n",
       "      <td>-0.234402</td>\n",
       "      <td>-2.816005</td>\n",
       "      <td>0.541331</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.154406</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.667378</td>\n",
       "      <td>0.825382</td>\n",
       "      <td>-1.314804</td>\n",
       "      <td>-2.047663</td>\n",
       "      <td>0.107949</td>\n",
       "      <td>-0.617595</td>\n",
       "      <td>0.050952</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>0.661120</td>\n",
       "      <td>-1.584904</td>\n",
       "      <td>0.257364</td>\n",
       "      <td>-0.108742</td>\n",
       "      <td>-0.238150</td>\n",
       "      <td>-0.026639</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>1.283168</td>\n",
       "      <td>1.153905</td>\n",
       "      <td>0.035699</td>\n",
       "      <td>1.537934</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>-0.214738</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>0.667378</td>\n",
       "      <td>-0.652971</td>\n",
       "      <td>0.305799</td>\n",
       "      <td>0.513478</td>\n",
       "      <td>1.299750</td>\n",
       "      <td>0.141295</td>\n",
       "      <td>-0.485131</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            전일비_1Y_베이시스    국고1년대비    국고3년대비    국고5년대비   국고10년대비    통안1년대비  \\\n",
       "DateTime                                                                    \n",
       "2012-08-02     0.348741  1.898958 -0.488709 -1.584904 -5.377146 -0.325433   \n",
       "2012-08-03     0.348741  0.051587  0.004075  0.035699 -0.767092 -0.325433   \n",
       "2012-08-06    -0.350946  0.051587 -0.160187  2.466603 -0.767092 -0.108742   \n",
       "2012-08-07     0.173819 -0.564203 -0.160187 -0.504502 -1.023207 -0.108742   \n",
       "2012-08-08    -0.001103 -0.564203 -0.324448  0.035699 -1.023207 -0.650470   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "2022-07-25    -0.700790 -1.179994  1.975212 -0.234402 -2.816005  0.541331   \n",
       "2022-07-26     0.348741  0.667378  0.825382 -1.314804 -2.047663  0.107949   \n",
       "2022-07-27     0.348741  0.051587  0.661120 -1.584904  0.257364 -0.108742   \n",
       "2022-07-28     0.173819  1.283168  1.153905  0.035699  1.537934 -0.000397   \n",
       "2022-07-29    -0.700790  0.667378 -0.652971  0.305799  0.513478  1.299750   \n",
       "\n",
       "              통안2년대비   전일비_ndf  전날 종가_ex   종가_ex  \n",
       "DateTime                                          \n",
       "2012-08-02  0.267777  0.079167 -0.149841  1131.7  \n",
       "2012-08-03  0.014814  0.184972 -0.056232  1134.8  \n",
       "2012-08-06  0.267777 -0.449862 -0.000426  1129.0  \n",
       "2012-08-07 -0.111668  0.020386 -0.104837  1128.8  \n",
       "2012-08-08 -0.364632  0.055654 -0.108437  1128.3  \n",
       "...              ...       ...       ...     ...  \n",
       "2022-07-25  0.014814  0.154406  3.207485  1313.7  \n",
       "2022-07-26 -0.617595  0.050952  3.220086  1307.6  \n",
       "2022-07-27 -0.238150 -0.026639  3.110275  1313.3  \n",
       "2022-07-28  0.014814 -0.214738  3.212885  1296.1  \n",
       "2022-07-29  0.141295 -0.485131  2.903255  1299.1  \n",
       "\n",
       "[2459 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = pd.concat([x_scaled,y], axis=1)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_scaled[0:1945]\n",
    "test = df_scaled[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=1):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['전일비_1Y_베이시스',  '국고3년대비',  '국고5년대비','전일비_ndf', '전날 종가_ex']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1555, 1, 5), (389, 1, 5))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.52586811, -0.16018659, -0.23440197,  0.1450014 ,\n",
       "         -1.64578612]],\n",
       "\n",
       "       [[ 0.17381924, -0.32444798, -0.23440197, -0.31584158,\n",
       "         -0.47567256]],\n",
       "\n",
       "       [[ 0.52366291,  0.0040748 ,  0.57589949, -0.29232918,\n",
       "          0.44061637]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.52586811,  0.0040748 , -0.23440197, -0.05485397,\n",
       "         -2.13543365]],\n",
       "\n",
       "       [[-0.35094627,  0.49685899,  0.57589949,  0.57762951,\n",
       "          0.60983279]],\n",
       "\n",
       "       [[-0.35094627,  0.1683362 , -1.04470342,  0.1332452 ,\n",
       "         -0.33885928]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((513, 1, 5), (513, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 1)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1268859.8750 - mae: 1125.3273\n",
      "Epoch 1: val_loss improved from inf to 1259105.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 12s 87ms/step - loss: 1268766.5000 - mae: 1125.2881 - val_loss: 1259105.7500 - val_mae: 1120.8568\n",
      "Epoch 2/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1268133.6250 - mae: 1125.0068\n",
      "Epoch 2: val_loss improved from 1259105.75000 to 1258318.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1268133.6250 - mae: 1125.0068 - val_loss: 1258318.5000 - val_mae: 1120.5037\n",
      "Epoch 3/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1266549.6250 - mae: 1124.2982\n",
      "Epoch 3: val_loss improved from 1258318.50000 to 1256731.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 1267003.3750 - mae: 1124.5017 - val_loss: 1256731.1250 - val_mae: 1119.7878\n",
      "Epoch 4/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1265843.5000 - mae: 1123.9724\n",
      "Epoch 4: val_loss improved from 1256731.12500 to 1253917.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1264863.7500 - mae: 1123.5402 - val_loss: 1253917.2500 - val_mae: 1118.5143\n",
      "Epoch 5/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1261978.3750 - mae: 1122.2405\n",
      "Epoch 5: val_loss improved from 1253917.25000 to 1249715.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1261439.0000 - mae: 1121.9994 - val_loss: 1249715.3750 - val_mae: 1116.6068\n",
      "Epoch 6/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1256512.8750 - mae: 1119.7745\n",
      "Epoch 6: val_loss improved from 1249715.37500 to 1243878.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1256554.5000 - mae: 1119.7925 - val_loss: 1243878.7500 - val_mae: 1113.9458\n",
      "Epoch 7/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1249990.1250 - mae: 1116.8239\n",
      "Epoch 7: val_loss improved from 1243878.75000 to 1236208.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1249943.2500 - mae: 1116.7910 - val_loss: 1236208.7500 - val_mae: 1110.4331\n",
      "Epoch 8/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1243030.2500 - mae: 1113.6423\n",
      "Epoch 8: val_loss improved from 1236208.75000 to 1226887.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 1241574.7500 - mae: 1112.9733 - val_loss: 1226887.3750 - val_mae: 1106.1425\n",
      "Epoch 9/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1232614.7500 - mae: 1108.8728\n",
      "Epoch 9: val_loss improved from 1226887.37500 to 1216178.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1231688.7500 - mae: 1108.4386 - val_loss: 1216178.3750 - val_mae: 1101.1848\n",
      "Epoch 10/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1220659.3750 - mae: 1103.3475\n",
      "Epoch 10: val_loss improved from 1216178.37500 to 1204362.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1220625.2500 - mae: 1103.3417 - val_loss: 1204362.0000 - val_mae: 1095.6849\n",
      "Epoch 11/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1208971.5000 - mae: 1097.9414\n",
      "Epoch 11: val_loss improved from 1204362.00000 to 1191651.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1208553.2500 - mae: 1097.7412 - val_loss: 1191651.1250 - val_mae: 1089.7297\n",
      "Epoch 12/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1195882.1250 - mae: 1091.8162\n",
      "Epoch 12: val_loss improved from 1191651.12500 to 1178093.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1195632.7500 - mae: 1091.7167 - val_loss: 1178093.5000 - val_mae: 1083.3368\n",
      "Epoch 13/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1180890.1250 - mae: 1084.7908\n",
      "Epoch 13: val_loss improved from 1178093.50000 to 1163771.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1181912.1250 - mae: 1085.2701 - val_loss: 1163771.1250 - val_mae: 1076.5286\n",
      "Epoch 14/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1168541.1250 - mae: 1078.9281\n",
      "Epoch 14: val_loss improved from 1163771.12500 to 1148790.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1167439.6250 - mae: 1078.4191 - val_loss: 1148790.7500 - val_mae: 1069.3580\n",
      "Epoch 15/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1152290.2500 - mae: 1071.2064\n",
      "Epoch 15: val_loss improved from 1148790.75000 to 1133168.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1152310.8750 - mae: 1071.2042 - val_loss: 1133168.2500 - val_mae: 1061.8123\n",
      "Epoch 16/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1136523.2500 - mae: 1063.6271\n",
      "Epoch 16: val_loss improved from 1133168.25000 to 1117054.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1136617.7500 - mae: 1063.6598 - val_loss: 1117054.0000 - val_mae: 1053.9661\n",
      "Epoch 17/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1120348.6250 - mae: 1055.7380\n",
      "Epoch 17: val_loss improved from 1117054.00000 to 1100310.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 1120336.7500 - mae: 1055.7501 - val_loss: 1100310.0000 - val_mae: 1045.7324\n",
      "Epoch 18/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1105988.2500 - mae: 1048.7332\n",
      "Epoch 18: val_loss improved from 1100310.00000 to 1083098.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1103540.8750 - mae: 1047.5312 - val_loss: 1083098.0000 - val_mae: 1037.1866\n",
      "Epoch 19/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1086952.3750 - mae: 1039.3171\n",
      "Epoch 19: val_loss improved from 1083098.00000 to 1065542.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1086326.0000 - mae: 1039.0208 - val_loss: 1065542.8750 - val_mae: 1028.3817\n",
      "Epoch 20/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1070791.0000 - mae: 1031.2180\n",
      "Epoch 20: val_loss improved from 1065542.87500 to 1047523.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1068654.5000 - mae: 1030.1857 - val_loss: 1047523.0625 - val_mae: 1019.2404\n",
      "Epoch 21/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1051923.0000 - mae: 1021.7233\n",
      "Epoch 21: val_loss improved from 1047523.06250 to 1029191.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 1050643.2500 - mae: 1021.1057 - val_loss: 1029191.2500 - val_mae: 1009.8377\n",
      "Epoch 22/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1035318.0000 - mae: 1013.2092\n",
      "Epoch 22: val_loss improved from 1029191.25000 to 1010702.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1032349.0625 - mae: 1011.7637 - val_loss: 1010702.3750 - val_mae: 1000.2540\n",
      "Epoch 23/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1016967.0625 - mae: 1003.8875\n",
      "Epoch 23: val_loss improved from 1010702.37500 to 991902.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1013794.4375 - mae: 1002.1912 - val_loss: 991902.6875 - val_mae: 990.3855\n",
      "Epoch 24/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 994449.6875 - mae: 992.1655\n",
      "Epoch 24: val_loss improved from 991902.68750 to 972911.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 994987.5625 - mae: 992.3688 - val_loss: 972911.0000 - val_mae: 980.2934\n",
      "Epoch 25/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 975286.8125 - mae: 981.9909\n",
      "Epoch 25: val_loss improved from 972911.00000 to 953739.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 976023.1875 - mae: 982.3557 - val_loss: 953739.6250 - val_mae: 969.9698\n",
      "Epoch 26/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 957228.7500 - mae: 972.3045\n",
      "Epoch 26: val_loss improved from 953739.62500 to 934556.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 956900.2500 - mae: 972.1234 - val_loss: 934556.6250 - val_mae: 959.5229\n",
      "Epoch 27/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 938509.8750 - mae: 962.1921\n",
      "Epoch 27: val_loss improved from 934556.62500 to 915202.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 937655.0625 - mae: 961.7070 - val_loss: 915202.0625 - val_mae: 948.8257\n",
      "Epoch 28/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 916993.5000 - mae: 950.3622\n",
      "Epoch 28: val_loss improved from 915202.06250 to 895732.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 96ms/step - loss: 918340.3125 - mae: 951.0970 - val_loss: 895732.8750 - val_mae: 937.9127\n",
      "Epoch 29/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 895659.1875 - mae: 938.5449\n",
      "Epoch 29: val_loss improved from 895732.87500 to 876320.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 898919.0625 - mae: 940.3080 - val_loss: 876320.0625 - val_mae: 926.8841\n",
      "Epoch 30/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 879216.9375 - mae: 929.1447\n",
      "Epoch 30: val_loss improved from 876320.06250 to 856854.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 879464.9375 - mae: 929.3373 - val_loss: 856854.1875 - val_mae: 915.6580\n",
      "Epoch 31/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 858076.6875 - mae: 917.1254\n",
      "Epoch 31: val_loss improved from 856854.18750 to 837378.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 108ms/step - loss: 859978.5625 - mae: 918.1877 - val_loss: 837378.3750 - val_mae: 904.2507\n",
      "Epoch 32/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 840972.6250 - mae: 907.1188\n",
      "Epoch 32: val_loss improved from 837378.37500 to 818013.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 840567.0000 - mae: 906.9199 - val_loss: 818013.7500 - val_mae: 892.7614\n",
      "Epoch 33/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 820272.2500 - mae: 895.2488\n",
      "Epoch 33: val_loss improved from 818013.75000 to 798611.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 96ms/step - loss: 821119.0000 - mae: 895.4521 - val_loss: 798611.8125 - val_mae: 881.0333\n",
      "Epoch 34/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 801444.1875 - mae: 883.6794\n",
      "Epoch 34: val_loss improved from 798611.81250 to 779346.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 801755.4375 - mae: 883.8811 - val_loss: 779346.1250 - val_mae: 869.2530\n",
      "Epoch 35/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 782542.5000 - mae: 872.3264\n",
      "Epoch 35: val_loss improved from 779346.12500 to 760144.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 96ms/step - loss: 782474.9375 - mae: 872.1865 - val_loss: 760144.1875 - val_mae: 857.5367\n",
      "Epoch 36/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 764512.4375 - mae: 861.2628\n",
      "Epoch 36: val_loss improved from 760144.18750 to 741139.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 763296.2500 - mae: 860.3890 - val_loss: 741139.6875 - val_mae: 845.7638\n",
      "Epoch 37/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 742016.3125 - mae: 847.0627\n",
      "Epoch 37: val_loss improved from 741139.68750 to 722181.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 744214.6250 - mae: 848.4222 - val_loss: 722181.6875 - val_mae: 833.8013\n",
      "Epoch 38/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 727820.6875 - mae: 838.1216\n",
      "Epoch 38: val_loss improved from 722181.68750 to 703445.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 101ms/step - loss: 725234.6875 - mae: 836.3372 - val_loss: 703445.6250 - val_mae: 821.8053\n",
      "Epoch 39/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 706474.8125 - mae: 824.0133\n",
      "Epoch 39: val_loss improved from 703445.62500 to 684836.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 706420.3750 - mae: 824.1507 - val_loss: 684836.6250 - val_mae: 809.6638\n",
      "Epoch 40/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 688914.4375 - mae: 812.8349\n",
      "Epoch 40: val_loss improved from 684836.62500 to 666397.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 687748.1250 - mae: 811.9159 - val_loss: 666397.0625 - val_mae: 797.4067\n",
      "Epoch 41/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 671908.5625 - mae: 801.4776\n",
      "Epoch 41: val_loss improved from 666397.06250 to 648147.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 669255.8750 - mae: 799.6006 - val_loss: 648147.0625 - val_mae: 785.0557\n",
      "Epoch 42/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 652082.1250 - mae: 787.9606\n",
      "Epoch 42: val_loss improved from 648147.06250 to 630104.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 650954.9375 - mae: 787.3152 - val_loss: 630104.5625 - val_mae: 772.6314\n",
      "Epoch 43/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 634234.3125 - mae: 775.8094\n",
      "Epoch 43: val_loss improved from 630104.56250 to 612298.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 632846.6250 - mae: 774.8922 - val_loss: 612298.1875 - val_mae: 760.1348\n",
      "Epoch 44/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 619586.1250 - mae: 765.8952\n",
      "Epoch 44: val_loss improved from 612298.18750 to 594736.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 614954.3125 - mae: 762.4268 - val_loss: 594736.0000 - val_mae: 747.5959\n",
      "Epoch 45/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 598518.0000 - mae: 750.6370\n",
      "Epoch 45: val_loss improved from 594736.00000 to 577387.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 597269.8125 - mae: 749.8862 - val_loss: 577387.1250 - val_mae: 734.9466\n",
      "Epoch 46/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 580574.2500 - mae: 738.1695\n",
      "Epoch 46: val_loss improved from 577387.12500 to 560269.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 579818.6250 - mae: 737.3553 - val_loss: 560269.1875 - val_mae: 722.2617\n",
      "Epoch 47/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 563768.5000 - mae: 725.6038\n",
      "Epoch 47: val_loss improved from 560269.18750 to 543492.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 562633.6250 - mae: 724.8137 - val_loss: 543492.3750 - val_mae: 709.5834\n",
      "Epoch 48/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 545702.5000 - mae: 712.2744\n",
      "Epoch 48: val_loss improved from 543492.37500 to 526884.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 545639.3750 - mae: 712.2761 - val_loss: 526884.3750 - val_mae: 696.7950\n",
      "Epoch 49/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 528248.6875 - mae: 699.3427\n",
      "Epoch 49: val_loss improved from 526884.37500 to 510489.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 528888.8125 - mae: 699.6900 - val_loss: 510489.6250 - val_mae: 683.9283\n",
      "Epoch 50/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 512384.6250 - mae: 687.0529\n",
      "Epoch 50: val_loss improved from 510489.62500 to 494400.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 512384.6250 - mae: 687.0529 - val_loss: 494400.7500 - val_mae: 671.1440\n",
      "Epoch 51/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 495462.0000 - mae: 673.3774\n",
      "Epoch 51: val_loss improved from 494400.75000 to 478624.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 496167.5625 - mae: 674.4584 - val_loss: 478624.1250 - val_mae: 658.2959\n",
      "Epoch 52/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 481663.9062 - mae: 663.5588\n",
      "Epoch 52: val_loss improved from 478624.12500 to 463065.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 480201.3125 - mae: 661.8334 - val_loss: 463065.0000 - val_mae: 645.4148\n",
      "Epoch 53/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 466892.3125 - mae: 650.7608\n",
      "Epoch 53: val_loss improved from 463065.00000 to 447776.78125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 464489.3750 - mae: 649.1440 - val_loss: 447776.7812 - val_mae: 632.7271\n",
      "Epoch 54/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 449888.9688 - mae: 637.1270\n",
      "Epoch 54: val_loss improved from 447776.78125 to 432755.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 449037.5312 - mae: 636.4612 - val_loss: 432755.5312 - val_mae: 620.0439\n",
      "Epoch 55/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 438841.6562 - mae: 627.6499\n",
      "Epoch 55: val_loss improved from 432755.53125 to 418005.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 433836.8125 - mae: 623.8802 - val_loss: 418005.3125 - val_mae: 607.4437\n",
      "Epoch 56/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 419730.0625 - mae: 611.7397\n",
      "Epoch 56: val_loss improved from 418005.31250 to 403585.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 418887.2500 - mae: 611.1891 - val_loss: 403585.1875 - val_mae: 594.8365\n",
      "Epoch 57/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 403939.9062 - mae: 598.3500\n",
      "Epoch 57: val_loss improved from 403585.18750 to 389333.03125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 404204.2500 - mae: 598.6112 - val_loss: 389333.0312 - val_mae: 582.3571\n",
      "Epoch 58/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 389718.4375 - mae: 586.7073\n",
      "Epoch 58: val_loss improved from 389333.03125 to 375346.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 389749.7188 - mae: 586.0394 - val_loss: 375346.3438 - val_mae: 569.8234\n",
      "Epoch 59/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 374779.9062 - mae: 573.1423\n",
      "Epoch 59: val_loss improved from 375346.34375 to 361692.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 375587.7812 - mae: 573.5836 - val_loss: 361692.7500 - val_mae: 557.3807\n",
      "Epoch 60/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 362410.7812 - mae: 561.8080\n",
      "Epoch 60: val_loss improved from 361692.75000 to 348177.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 361661.0000 - mae: 561.2507 - val_loss: 348177.9375 - val_mae: 544.9763\n",
      "Epoch 61/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 348022.1250 - mae: 548.9865\n",
      "Epoch 61: val_loss improved from 348177.93750 to 335008.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 348022.1250 - mae: 548.9865 - val_loss: 335008.3125 - val_mae: 532.6011\n",
      "Epoch 62/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 340660.2812 - mae: 543.3530\n",
      "Epoch 62: val_loss improved from 335008.31250 to 322020.78125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 334682.0625 - mae: 536.6155 - val_loss: 322020.7812 - val_mae: 520.3230\n",
      "Epoch 63/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 324235.0312 - mae: 528.3369\n",
      "Epoch 63: val_loss improved from 322020.78125 to 309280.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 321532.5312 - mae: 524.3759 - val_loss: 309280.0938 - val_mae: 508.0914\n",
      "Epoch 64/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 309172.2188 - mae: 512.3407\n",
      "Epoch 64: val_loss improved from 309280.09375 to 296831.96875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 308609.4062 - mae: 512.1887 - val_loss: 296831.9688 - val_mae: 495.9566\n",
      "Epoch 65/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 295724.0938 - mae: 499.7054\n",
      "Epoch 65: val_loss improved from 296831.96875 to 284654.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 295954.0625 - mae: 500.0029 - val_loss: 284654.5625 - val_mae: 483.9257\n",
      "Epoch 66/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 283541.0938 - mae: 488.0124\n",
      "Epoch 66: val_loss improved from 284654.56250 to 272472.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 283507.4062 - mae: 487.9414 - val_loss: 272472.5625 - val_mae: 472.0193\n",
      "Epoch 67/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 271472.9688 - mae: 476.2194\n",
      "Epoch 67: val_loss improved from 272472.56250 to 260686.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 271288.7812 - mae: 475.9319 - val_loss: 260686.5625 - val_mae: 460.2566\n",
      "Epoch 68/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 259009.2969 - mae: 463.6848\n",
      "Epoch 68: val_loss improved from 260686.56250 to 249081.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 259287.8906 - mae: 464.0149 - val_loss: 249081.8750 - val_mae: 448.5443\n",
      "Epoch 69/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 246529.4844 - mae: 450.5251\n",
      "Epoch 69: val_loss improved from 249081.87500 to 237620.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 247499.6719 - mae: 452.1398 - val_loss: 237620.3438 - val_mae: 436.7773\n",
      "Epoch 70/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 236081.5781 - mae: 440.4112\n",
      "Epoch 70: val_loss improved from 237620.34375 to 226447.92188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 236007.2188 - mae: 440.3477 - val_loss: 226447.9219 - val_mae: 425.1784\n",
      "Epoch 71/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 224758.1875 - mae: 428.8810\n",
      "Epoch 71: val_loss improved from 226447.92188 to 215882.59375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 224984.6094 - mae: 428.9567 - val_loss: 215882.5938 - val_mae: 413.9855\n",
      "Epoch 72/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 214218.7031 - mae: 417.6736\n",
      "Epoch 72: val_loss improved from 215882.59375 to 205227.51562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 214011.3906 - mae: 417.3586 - val_loss: 205227.5156 - val_mae: 402.4960\n",
      "Epoch 73/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 207323.5625 - mae: 410.1224\n",
      "Epoch 73: val_loss improved from 205227.51562 to 194720.60938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 203272.3906 - mae: 405.7159 - val_loss: 194720.6094 - val_mae: 391.0661\n",
      "Epoch 74/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 194163.9844 - mae: 396.0986\n",
      "Epoch 74: val_loss improved from 194720.60938 to 184634.01562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 192806.2500 - mae: 394.2279 - val_loss: 184634.0156 - val_mae: 379.7113\n",
      "Epoch 75/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 182511.3750 - mae: 382.4678\n",
      "Epoch 75: val_loss improved from 184634.01562 to 174786.26562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 182648.6250 - mae: 382.7639 - val_loss: 174786.2656 - val_mae: 368.4197\n",
      "Epoch 76/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 173656.1562 - mae: 372.0121\n",
      "Epoch 76: val_loss improved from 174786.26562 to 165214.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 172747.4062 - mae: 371.3096 - val_loss: 165214.0938 - val_mae: 357.3338\n",
      "Epoch 77/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 161252.0312 - mae: 357.8928\n",
      "Epoch 77: val_loss improved from 165214.09375 to 155852.14062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 163067.3906 - mae: 359.9778 - val_loss: 155852.1406 - val_mae: 346.3468\n",
      "Epoch 78/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 154782.3750 - mae: 350.8275\n",
      "Epoch 78: val_loss improved from 155852.14062 to 146644.10938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 153654.9531 - mae: 348.5538 - val_loss: 146644.1094 - val_mae: 335.3036\n",
      "Epoch 79/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 145590.2500 - mae: 338.6720\n",
      "Epoch 79: val_loss improved from 146644.10938 to 137942.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 144591.5000 - mae: 337.4250 - val_loss: 137942.7188 - val_mae: 324.5763\n",
      "Epoch 80/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 135251.7031 - mae: 326.4459\n",
      "Epoch 80: val_loss improved from 137942.71875 to 129393.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 135794.2656 - mae: 326.1657 - val_loss: 129393.5312 - val_mae: 313.7196\n",
      "Epoch 81/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 127339.4297 - mae: 315.0385\n",
      "Epoch 81: val_loss improved from 129393.53125 to 121181.82031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 127339.4297 - mae: 315.0385 - val_loss: 121181.8203 - val_mae: 303.0294\n",
      "Epoch 82/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 119008.0078 - mae: 303.7738\n",
      "Epoch 82: val_loss improved from 121181.82031 to 113364.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 119180.0781 - mae: 304.0508 - val_loss: 113364.6875 - val_mae: 292.4628\n",
      "Epoch 83/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 111258.0078 - mae: 293.0215\n",
      "Epoch 83: val_loss improved from 113364.68750 to 105720.55469, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 111291.0078 - mae: 293.0515 - val_loss: 105720.5547 - val_mae: 281.9272\n",
      "Epoch 84/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 103813.7812 - mae: 282.2361\n",
      "Epoch 84: val_loss improved from 105720.55469 to 98513.36719, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 103761.8594 - mae: 282.2618 - val_loss: 98513.3672 - val_mae: 271.6221\n",
      "Epoch 85/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 96762.5938 - mae: 271.8741\n",
      "Epoch 85: val_loss improved from 98513.36719 to 91603.54688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 96576.2266 - mae: 271.5258 - val_loss: 91603.5469 - val_mae: 261.3474\n",
      "Epoch 86/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 89502.6250 - mae: 259.5821\n",
      "Epoch 86: val_loss improved from 91603.54688 to 85066.00781, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 89741.6016 - mae: 260.9271 - val_loss: 85066.0078 - val_mae: 251.2252\n",
      "Epoch 87/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 83856.6172 - mae: 251.1798\n",
      "Epoch 87: val_loss improved from 85066.00781 to 78744.14062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 83199.2656 - mae: 250.4640 - val_loss: 78744.1406 - val_mae: 241.0282\n",
      "Epoch 88/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 77005.3828 - mae: 240.2579\n",
      "Epoch 88: val_loss improved from 78744.14062 to 72803.28125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 77005.3828 - mae: 240.2579 - val_loss: 72803.2812 - val_mae: 231.2392\n",
      "Epoch 89/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 71250.8281 - mae: 230.4451\n",
      "Epoch 89: val_loss improved from 72803.28125 to 67255.04688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 71158.1250 - mae: 230.2430 - val_loss: 67255.0469 - val_mae: 221.7131\n",
      "Epoch 90/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 65472.1406 - mae: 219.7377\n",
      "Epoch 90: val_loss improved from 67255.04688 to 61899.92969, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 65657.0000 - mae: 220.2581 - val_loss: 61899.9297 - val_mae: 212.1590\n",
      "Epoch 91/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 60641.9180 - mae: 210.8128\n",
      "Epoch 91: val_loss improved from 61899.92969 to 56892.35938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 60452.4414 - mae: 210.5741 - val_loss: 56892.3594 - val_mae: 202.7172\n",
      "Epoch 92/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 55647.4688 - mae: 201.8924\n",
      "Epoch 92: val_loss improved from 56892.35938 to 52255.42578, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 55588.3008 - mae: 200.9966 - val_loss: 52255.4258 - val_mae: 193.4426\n",
      "Epoch 93/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 51046.2812 - mae: 191.7208\n",
      "Epoch 93: val_loss improved from 52255.42578 to 47855.70312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 51046.2812 - mae: 191.7208 - val_loss: 47855.7031 - val_mae: 184.4661\n",
      "Epoch 94/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 47486.0391 - mae: 183.8605\n",
      "Epoch 94: val_loss improved from 47855.70312 to 43753.07812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 46800.6367 - mae: 182.6095 - val_loss: 43753.0781 - val_mae: 175.5528\n",
      "Epoch 95/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 41048.8398 - mae: 172.2101\n",
      "Epoch 95: val_loss improved from 43753.07812 to 39929.98828, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 42839.9648 - mae: 173.5475 - val_loss: 39929.9883 - val_mae: 166.7415\n",
      "Epoch 96/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 39160.8945 - mae: 164.7718\n",
      "Epoch 96: val_loss improved from 39929.98828 to 36521.91797, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 39233.1328 - mae: 165.0664 - val_loss: 36521.9180 - val_mae: 158.7021\n",
      "Epoch 97/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 36098.5625 - mae: 157.4277\n",
      "Epoch 97: val_loss improved from 36521.91797 to 33277.57422, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 35855.8359 - mae: 156.6867 - val_loss: 33277.5742 - val_mae: 150.5812\n",
      "Epoch 98/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 33051.1406 - mae: 149.2917\n",
      "Epoch 98: val_loss improved from 33277.57422 to 30325.92578, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 32786.9531 - mae: 148.4124 - val_loss: 30325.9258 - val_mae: 142.7213\n",
      "Epoch 99/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 29996.5234 - mae: 140.4816\n",
      "Epoch 99: val_loss improved from 30325.92578 to 27554.60156, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 29967.3203 - mae: 140.5442 - val_loss: 27554.6016 - val_mae: 135.2664\n",
      "Epoch 100/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 27342.7676 - mae: 132.6645\n",
      "Epoch 100: val_loss improved from 27554.60156 to 24947.20312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 27364.7383 - mae: 132.7690 - val_loss: 24947.2031 - val_mae: 127.8702\n",
      "Epoch 101/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 25241.9688 - mae: 125.8772\n",
      "Epoch 101: val_loss improved from 24947.20312 to 22491.14648, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 24994.0176 - mae: 125.3399 - val_loss: 22491.1465 - val_mae: 120.6328\n",
      "Epoch 102/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 22880.7109 - mae: 118.2825\n",
      "Epoch 102: val_loss improved from 22491.14648 to 20207.49023, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 22852.4082 - mae: 118.2065 - val_loss: 20207.4902 - val_mae: 113.7913\n",
      "Epoch 103/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 21843.0488 - mae: 112.7351\n",
      "Epoch 103: val_loss improved from 20207.49023 to 18012.63086, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 20926.7773 - mae: 111.5884 - val_loss: 18012.6309 - val_mae: 107.1603\n",
      "Epoch 104/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 19507.5215 - mae: 104.9586\n",
      "Epoch 104: val_loss improved from 18012.63086 to 15944.57031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 19174.8262 - mae: 105.0154 - val_loss: 15944.5703 - val_mae: 100.6847\n",
      "Epoch 105/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 17005.2617 - mae: 99.0949 \n",
      "Epoch 105: val_loss improved from 15944.57031 to 14241.23730, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 17584.6582 - mae: 98.6284 - val_loss: 14241.2373 - val_mae: 94.4606\n",
      "Epoch 106/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 15844.5908 - mae: 92.9951\n",
      "Epoch 106: val_loss improved from 14241.23730 to 12322.29785, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 16137.6670 - mae: 92.6839 - val_loss: 12322.2979 - val_mae: 88.4178\n",
      "Epoch 107/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 14093.7803 - mae: 86.3194\n",
      "Epoch 107: val_loss improved from 12322.29785 to 10840.38672, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 14831.4658 - mae: 87.0761 - val_loss: 10840.3867 - val_mae: 82.7893\n",
      "Epoch 108/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 12857.3184 - mae: 81.4839\n",
      "Epoch 108: val_loss improved from 10840.38672 to 9639.74609, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 13636.5889 - mae: 81.8987 - val_loss: 9639.7461 - val_mae: 77.7748\n",
      "Epoch 109/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 12801.3389 - mae: 76.8033\n",
      "Epoch 109: val_loss improved from 9639.74609 to 8538.73535, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 12572.7129 - mae: 77.0390 - val_loss: 8538.7354 - val_mae: 72.8348\n",
      "Epoch 110/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 12310.6787 - mae: 73.1618\n",
      "Epoch 110: val_loss improved from 8538.73535 to 7649.13281, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 11603.8018 - mae: 72.3236 - val_loss: 7649.1328 - val_mae: 68.2673\n",
      "Epoch 111/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 11010.1865 - mae: 67.6508\n",
      "Epoch 111: val_loss improved from 7649.13281 to 6867.20264, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 10702.8223 - mae: 67.4236 - val_loss: 6867.2026 - val_mae: 63.8044\n",
      "Epoch 112/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 10231.0635 - mae: 63.8754\n",
      "Epoch 112: val_loss improved from 6867.20264 to 6185.37793, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 9872.0742 - mae: 63.2825 - val_loss: 6185.3779 - val_mae: 59.6553\n",
      "Epoch 113/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 9847.7734 - mae: 60.0245\n",
      "Epoch 113: val_loss improved from 6185.37793 to 5574.07959, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 9066.1338 - mae: 58.9555 - val_loss: 5574.0796 - val_mae: 55.6157\n",
      "Epoch 114/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 7921.4248 - mae: 55.0213\n",
      "Epoch 114: val_loss improved from 5574.07959 to 5034.14404, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 8341.8525 - mae: 55.0947 - val_loss: 5034.1440 - val_mae: 51.8028\n",
      "Epoch 115/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 7993.0708 - mae: 51.2981\n",
      "Epoch 115: val_loss improved from 5034.14404 to 4552.15234, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 7668.5640 - mae: 51.1852 - val_loss: 4552.1523 - val_mae: 48.2126\n",
      "Epoch 116/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 7193.9741 - mae: 47.6668\n",
      "Epoch 116: val_loss improved from 4552.15234 to 4059.90723, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 7036.2290 - mae: 47.4430 - val_loss: 4059.9072 - val_mae: 44.7247\n",
      "Epoch 117/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 6774.9224 - mae: 44.7768\n",
      "Epoch 117: val_loss improved from 4059.90723 to 3650.20337, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 6463.8120 - mae: 44.2611 - val_loss: 3650.2034 - val_mae: 41.5985\n",
      "Epoch 118/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 5182.0093 - mae: 40.8593\n",
      "Epoch 118: val_loss improved from 3650.20337 to 3266.24902, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 5957.4609 - mae: 41.0415 - val_loss: 3266.2490 - val_mae: 38.3901\n",
      "Epoch 119/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 5837.9067 - mae: 38.6268\n",
      "Epoch 119: val_loss improved from 3266.24902 to 2930.10986, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 5514.5430 - mae: 38.2955 - val_loss: 2930.1099 - val_mae: 35.6268\n",
      "Epoch 120/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 5139.5215 - mae: 35.6959\n",
      "Epoch 120: val_loss improved from 2930.10986 to 2724.73193, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 5132.8340 - mae: 35.7010 - val_loss: 2724.7319 - val_mae: 33.1870\n",
      "Epoch 121/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 4624.2310 - mae: 33.0328\n",
      "Epoch 121: val_loss did not improve from 2724.73193\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 4788.9507 - mae: 33.1973 - val_loss: 2737.7798 - val_mae: 31.5915\n",
      "Epoch 122/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 4516.2393 - mae: 31.1675\n",
      "Epoch 122: val_loss did not improve from 2724.73193\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 4508.0356 - mae: 31.1290 - val_loss: 2872.8577 - val_mae: 30.0059\n",
      "Epoch 123/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 2744.6206 - mae: 27.8459\n",
      "Epoch 123: val_loss did not improve from 2724.73193\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 4238.3218 - mae: 28.9969 - val_loss: 2852.3333 - val_mae: 28.3110\n",
      "Epoch 124/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 4035.5676 - mae: 27.3927\n",
      "Epoch 124: val_loss improved from 2724.73193 to 2717.40942, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 4028.7234 - mae: 27.3771 - val_loss: 2717.4094 - val_mae: 26.6712\n",
      "Epoch 125/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 3832.5044 - mae: 25.7879\n",
      "Epoch 125: val_loss improved from 2717.40942 to 2611.72900, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 3825.5110 - mae: 25.7628 - val_loss: 2611.7290 - val_mae: 25.1087\n",
      "Epoch 126/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 3846.8403 - mae: 24.5748\n",
      "Epoch 126: val_loss improved from 2611.72900 to 2585.55347, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 3645.6826 - mae: 24.2787 - val_loss: 2585.5535 - val_mae: 23.7265\n",
      "Epoch 127/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 3492.1177 - mae: 22.8430\n",
      "Epoch 127: val_loss improved from 2585.55347 to 2462.14771, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 3486.5154 - mae: 22.8431 - val_loss: 2462.1477 - val_mae: 22.4198\n",
      "Epoch 128/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 3386.5532 - mae: 21.7834\n",
      "Epoch 128: val_loss improved from 2462.14771 to 2357.70605, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 3325.0100 - mae: 21.7723 - val_loss: 2357.7061 - val_mae: 21.1779\n",
      "Epoch 129/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 3448.6472 - mae: 21.0505\n",
      "Epoch 129: val_loss improved from 2357.70605 to 2264.53833, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 3174.3145 - mae: 20.8171 - val_loss: 2264.5383 - val_mae: 20.2418\n",
      "Epoch 130/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 3257.4055 - mae: 20.0989\n",
      "Epoch 130: val_loss improved from 2264.53833 to 2041.27319, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 3011.9541 - mae: 19.7471 - val_loss: 2041.2732 - val_mae: 19.1514\n",
      "Epoch 131/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 3019.1445 - mae: 19.0291\n",
      "Epoch 131: val_loss did not improve from 2041.27319\n",
      "98/98 [==============================] - 1s 6ms/step - loss: 2846.1226 - mae: 18.7524 - val_loss: 2083.9670 - val_mae: 18.5868\n",
      "Epoch 132/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 2674.1094 - mae: 18.4159\n",
      "Epoch 132: val_loss improved from 2041.27319 to 1872.62561, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 2674.1094 - mae: 18.4159 - val_loss: 1872.6256 - val_mae: 17.8111\n",
      "Epoch 133/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 2603.1997 - mae: 17.9050\n",
      "Epoch 133: val_loss improved from 1872.62561 to 1866.25525, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 2494.5867 - mae: 17.6156 - val_loss: 1866.2552 - val_mae: 17.3018\n",
      "Epoch 134/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 2345.2227 - mae: 16.8414\n",
      "Epoch 134: val_loss improved from 1866.25525 to 1811.10510, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 2333.9255 - mae: 16.8818 - val_loss: 1811.1051 - val_mae: 16.8314\n",
      "Epoch 135/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 2215.2161 - mae: 16.4032\n",
      "Epoch 135: val_loss improved from 1811.10510 to 1664.91418, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 2211.1335 - mae: 16.3878 - val_loss: 1664.9142 - val_mae: 16.2528\n",
      "Epoch 136/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 2344.4460 - mae: 16.5976\n",
      "Epoch 136: val_loss improved from 1664.91418 to 1569.86279, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 2120.2097 - mae: 16.0305 - val_loss: 1569.8628 - val_mae: 15.8084\n",
      "Epoch 137/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 2175.0901 - mae: 15.6301\n",
      "Epoch 137: val_loss improved from 1569.86279 to 1507.33179, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 2043.0161 - mae: 15.5269 - val_loss: 1507.3318 - val_mae: 15.3890\n",
      "Epoch 138/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 2070.5659 - mae: 15.2154\n",
      "Epoch 138: val_loss improved from 1507.33179 to 1426.06213, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 1981.0049 - mae: 15.1904 - val_loss: 1426.0621 - val_mae: 14.9680\n",
      "Epoch 139/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1327.6813 - mae: 14.4815\n",
      "Epoch 139: val_loss improved from 1426.06213 to 1342.82654, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1923.4008 - mae: 14.8639 - val_loss: 1342.8265 - val_mae: 14.6431\n",
      "Epoch 140/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1877.6036 - mae: 14.5663\n",
      "Epoch 140: val_loss improved from 1342.82654 to 1304.96436, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 1867.8333 - mae: 14.6770 - val_loss: 1304.9644 - val_mae: 14.3673\n",
      "Epoch 141/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1867.8455 - mae: 14.5371\n",
      "Epoch 141: val_loss improved from 1304.96436 to 1209.83240, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1816.3114 - mae: 14.6231 - val_loss: 1209.8324 - val_mae: 13.9937\n",
      "Epoch 142/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1760.2931 - mae: 14.1744\n",
      "Epoch 142: val_loss improved from 1209.83240 to 1190.86487, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1760.2931 - mae: 14.1744 - val_loss: 1190.8649 - val_mae: 13.6896\n",
      "Epoch 143/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1730.4575 - mae: 14.0405\n",
      "Epoch 143: val_loss improved from 1190.86487 to 1130.34863, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1711.1836 - mae: 13.9860 - val_loss: 1130.3486 - val_mae: 13.4693\n",
      "Epoch 144/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1564.8971 - mae: 13.4680\n",
      "Epoch 144: val_loss improved from 1130.34863 to 1100.98669, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1660.2393 - mae: 13.7142 - val_loss: 1100.9867 - val_mae: 13.2272\n",
      "Epoch 145/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1627.7515 - mae: 13.7668\n",
      "Epoch 145: val_loss improved from 1100.98669 to 1007.79095, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1609.3621 - mae: 13.6969 - val_loss: 1007.7910 - val_mae: 12.9118\n",
      "Epoch 146/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1564.3988 - mae: 13.3817\n",
      "Epoch 146: val_loss did not improve from 1007.79095\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 1561.4550 - mae: 13.3653 - val_loss: 1025.7930 - val_mae: 12.8318\n",
      "Epoch 147/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 1511.7643 - mae: 13.1745\n",
      "Epoch 147: val_loss improved from 1007.79095 to 973.87915, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1512.7754 - mae: 13.2936 - val_loss: 973.8792 - val_mae: 12.6497\n",
      "Epoch 148/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1498.4395 - mae: 13.1354\n",
      "Epoch 148: val_loss improved from 973.87915 to 967.11224, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1466.6586 - mae: 12.9966 - val_loss: 967.1122 - val_mae: 12.5261\n",
      "Epoch 149/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1559.7186 - mae: 13.3518\n",
      "Epoch 149: val_loss improved from 967.11224 to 874.54651, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1427.6470 - mae: 13.0254 - val_loss: 874.5465 - val_mae: 12.2373\n",
      "Epoch 150/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1385.5447 - mae: 12.7471\n",
      "Epoch 150: val_loss did not improve from 874.54651\n",
      "98/98 [==============================] - 1s 5ms/step - loss: 1371.7997 - mae: 12.7412 - val_loss: 887.2449 - val_mae: 12.1859\n",
      "Epoch 151/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1335.8066 - mae: 12.4948\n",
      "Epoch 151: val_loss improved from 874.54651 to 866.47443, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1325.6045 - mae: 12.4908 - val_loss: 866.4744 - val_mae: 12.0683\n",
      "Epoch 152/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1421.3271 - mae: 12.7906\n",
      "Epoch 152: val_loss improved from 866.47443 to 843.41138, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1277.5815 - mae: 12.4558 - val_loss: 843.4114 - val_mae: 12.0202\n",
      "Epoch 153/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 726.3620 - mae: 11.7609\n",
      "Epoch 153: val_loss improved from 843.41138 to 808.42145, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1227.4495 - mae: 12.2040 - val_loss: 808.4214 - val_mae: 11.8013\n",
      "Epoch 154/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1171.6395 - mae: 12.0710\n",
      "Epoch 154: val_loss did not improve from 808.42145\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 1171.6395 - mae: 12.0710 - val_loss: 826.2517 - val_mae: 11.8080\n",
      "Epoch 155/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1143.8214 - mae: 11.9866\n",
      "Epoch 155: val_loss improved from 808.42145 to 792.92401, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1110.0944 - mae: 11.8628 - val_loss: 792.9240 - val_mae: 11.6507\n",
      "Epoch 156/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1053.2444 - mae: 11.5734\n",
      "Epoch 156: val_loss improved from 792.92401 to 747.12854, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1051.4653 - mae: 11.5716 - val_loss: 747.1285 - val_mae: 11.4863\n",
      "Epoch 157/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1071.5004 - mae: 11.7109\n",
      "Epoch 157: val_loss did not improve from 747.12854\n",
      "98/98 [==============================] - 1s 5ms/step - loss: 987.6593 - mae: 11.4709 - val_loss: 770.7587 - val_mae: 11.4590\n",
      "Epoch 158/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 868.2803 - mae: 11.1440 \n",
      "Epoch 158: val_loss improved from 747.12854 to 733.94470, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 925.2681 - mae: 11.3112 - val_loss: 733.9447 - val_mae: 11.3105\n",
      "Epoch 159/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 873.4434 - mae: 11.0558\n",
      "Epoch 159: val_loss did not improve from 733.94470\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 869.4082 - mae: 11.0828 - val_loss: 754.3599 - val_mae: 11.2328\n",
      "Epoch 160/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 856.5436 - mae: 11.1119\n",
      "Epoch 160: val_loss improved from 733.94470 to 728.56854, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 819.6480 - mae: 10.9516 - val_loss: 728.5685 - val_mae: 11.1950\n",
      "Epoch 161/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 791.9297 - mae: 10.7918\n",
      "Epoch 161: val_loss did not improve from 728.56854\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 779.6432 - mae: 10.7997 - val_loss: 755.7358 - val_mae: 11.2203\n",
      "Epoch 162/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 756.1074 - mae: 10.7513\n",
      "Epoch 162: val_loss improved from 728.56854 to 708.56677, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 748.2543 - mae: 10.7213 - val_loss: 708.5668 - val_mae: 11.0219\n",
      "Epoch 163/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 777.1305 - mae: 10.7468\n",
      "Epoch 163: val_loss did not improve from 708.56677\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 724.4514 - mae: 10.6174 - val_loss: 757.1982 - val_mae: 11.0838\n",
      "Epoch 164/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 687.5218 - mae: 10.3695\n",
      "Epoch 164: val_loss did not improve from 708.56677\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 701.2510 - mae: 10.4406 - val_loss: 755.5156 - val_mae: 11.0001\n",
      "Epoch 165/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 766.8658 - mae: 10.4850\n",
      "Epoch 165: val_loss did not improve from 708.56677\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 681.9675 - mae: 10.4444 - val_loss: 768.9265 - val_mae: 11.0675\n",
      "Epoch 166/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 730.0861 - mae: 10.4441\n",
      "Epoch 166: val_loss did not improve from 708.56677\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 662.2444 - mae: 10.3337 - val_loss: 742.6374 - val_mae: 11.0087\n",
      "Epoch 167/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 650.3704 - mae: 10.2468\n",
      "Epoch 167: val_loss did not improve from 708.56677\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 643.8258 - mae: 10.2157 - val_loss: 767.8005 - val_mae: 10.9810\n",
      "Epoch 168/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 649.7942 - mae: 9.8857\n",
      "Epoch 168: val_loss did not improve from 708.56677\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 627.9514 - mae: 10.1140 - val_loss: 788.2990 - val_mae: 10.9682\n",
      "Epoch 169/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 667.6057 - mae: 10.0091\n",
      "Epoch 169: val_loss did not improve from 708.56677\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 611.9596 - mae: 10.0658 - val_loss: 742.2379 - val_mae: 10.9397\n",
      "Epoch 170/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 646.1804 - mae: 10.0860\n",
      "Epoch 170: val_loss did not improve from 708.56677\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 597.0727 - mae: 10.0008 - val_loss: 803.7464 - val_mae: 11.0051\n",
      "Epoch 171/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 620.5844 - mae: 9.9943\n",
      "Epoch 171: val_loss improved from 708.56677 to 696.34863, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 581.6498 - mae: 9.9391 - val_loss: 696.3486 - val_mae: 10.7553\n",
      "Epoch 172/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 582.5042 - mae: 9.8749\n",
      "Epoch 172: val_loss did not improve from 696.34863\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 567.5004 - mae: 9.8346 - val_loss: 874.6819 - val_mae: 10.9564\n",
      "Epoch 173/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 624.7502 - mae: 10.0326\n",
      "Epoch 173: val_loss improved from 696.34863 to 654.23444, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 553.6378 - mae: 9.7765 - val_loss: 654.2344 - val_mae: 10.6036\n",
      "Epoch 174/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 546.1197 - mae: 9.7077\n",
      "Epoch 174: val_loss did not improve from 654.23444\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 540.6840 - mae: 9.6870 - val_loss: 792.0797 - val_mae: 10.7766\n",
      "Epoch 175/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 546.3173 - mae: 9.5295\n",
      "Epoch 175: val_loss did not improve from 654.23444\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 526.3649 - mae: 9.5756 - val_loss: 696.2767 - val_mae: 10.6184\n",
      "Epoch 176/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 530.9059 - mae: 9.5581\n",
      "Epoch 176: val_loss did not improve from 654.23444\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 514.4264 - mae: 9.5521 - val_loss: 721.8308 - val_mae: 10.6706\n",
      "Epoch 177/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 509.1112 - mae: 9.5134\n",
      "Epoch 177: val_loss did not improve from 654.23444\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 501.5461 - mae: 9.4882 - val_loss: 687.0289 - val_mae: 10.5463\n",
      "Epoch 178/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 489.7778 - mae: 9.3604\n",
      "Epoch 178: val_loss did not improve from 654.23444\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 489.0949 - mae: 9.3568 - val_loss: 671.6039 - val_mae: 10.4534\n",
      "Epoch 179/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 477.0250 - mae: 9.3139\n",
      "Epoch 179: val_loss improved from 654.23444 to 643.78387, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 476.1768 - mae: 9.3062 - val_loss: 643.7839 - val_mae: 10.3738\n",
      "Epoch 180/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 468.0954 - mae: 9.2370\n",
      "Epoch 180: val_loss improved from 643.78387 to 630.93286, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 465.2522 - mae: 9.2361 - val_loss: 630.9329 - val_mae: 10.2916\n",
      "Epoch 181/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 456.1921 - mae: 9.1671\n",
      "Epoch 181: val_loss improved from 630.93286 to 623.40125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 453.9958 - mae: 9.2028 - val_loss: 623.4012 - val_mae: 10.2021\n",
      "Epoch 182/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 171.9156 - mae: 8.7754\n",
      "Epoch 182: val_loss improved from 623.40125 to 599.68719, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 443.0210 - mae: 9.1189 - val_loss: 599.6872 - val_mae: 10.1271\n",
      "Epoch 183/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 457.8549 - mae: 9.1304\n",
      "Epoch 183: val_loss improved from 599.68719 to 547.50500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 436.1166 - mae: 9.1347 - val_loss: 547.5050 - val_mae: 9.9782\n",
      "Epoch 184/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 426.8481 - mae: 9.0358\n",
      "Epoch 184: val_loss did not improve from 547.50500\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 423.2912 - mae: 9.0426 - val_loss: 569.8035 - val_mae: 10.0101\n",
      "Epoch 185/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 461.1173 - mae: 9.1093\n",
      "Epoch 185: val_loss improved from 547.50500 to 471.56451, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 411.6925 - mae: 8.9734 - val_loss: 471.5645 - val_mae: 9.7162\n",
      "Epoch 186/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 403.3878 - mae: 8.9445\n",
      "Epoch 186: val_loss did not improve from 471.56451\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 402.6631 - mae: 8.9370 - val_loss: 519.0325 - val_mae: 9.7967\n",
      "Epoch 187/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 392.8645 - mae: 8.8489\n",
      "Epoch 187: val_loss improved from 471.56451 to 403.70264, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 393.1476 - mae: 8.8686 - val_loss: 403.7026 - val_mae: 9.4810\n",
      "Epoch 188/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 383.4343 - mae: 8.7864\n",
      "Epoch 188: val_loss did not improve from 403.70264\n",
      "98/98 [==============================] - 1s 5ms/step - loss: 383.5291 - mae: 8.8096 - val_loss: 434.7332 - val_mae: 9.4907\n",
      "Epoch 189/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 374.2423 - mae: 8.7566\n",
      "Epoch 189: val_loss improved from 403.70264 to 371.99426, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 373.7448 - mae: 8.7527 - val_loss: 371.9943 - val_mae: 9.2676\n",
      "Epoch 190/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 359.3981 - mae: 8.5323\n",
      "Epoch 190: val_loss did not improve from 371.99426\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 364.0807 - mae: 8.6951 - val_loss: 380.8564 - val_mae: 9.2735\n",
      "Epoch 191/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 356.1621 - mae: 8.6849\n",
      "Epoch 191: val_loss improved from 371.99426 to 337.56876, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 354.1476 - mae: 8.6900 - val_loss: 337.5688 - val_mae: 9.1369\n",
      "Epoch 192/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 386.4153 - mae: 8.8647\n",
      "Epoch 192: val_loss did not improve from 337.56876\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 344.0818 - mae: 8.6501 - val_loss: 347.0038 - val_mae: 9.1800\n",
      "Epoch 193/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 376.9514 - mae: 8.7872\n",
      "Epoch 193: val_loss improved from 337.56876 to 322.37625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 334.8120 - mae: 8.6404 - val_loss: 322.3763 - val_mae: 9.0641\n",
      "Epoch 194/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 358.2705 - mae: 8.6872\n",
      "Epoch 194: val_loss improved from 322.37625 to 320.53571, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 322.1533 - mae: 8.5963 - val_loss: 320.5357 - val_mae: 9.0353\n",
      "Epoch 195/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 329.7354 - mae: 8.5300\n",
      "Epoch 195: val_loss improved from 320.53571 to 296.48965, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 309.6129 - mae: 8.5164 - val_loss: 296.4897 - val_mae: 8.9326\n",
      "Epoch 196/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 301.5012 - mae: 8.5016\n",
      "Epoch 196: val_loss did not improve from 296.48965\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 294.4668 - mae: 8.4599 - val_loss: 300.3280 - val_mae: 8.9101\n",
      "Epoch 197/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 126.8177 - mae: 8.0864\n",
      "Epoch 197: val_loss improved from 296.48965 to 270.29904, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 277.5096 - mae: 8.4416 - val_loss: 270.2990 - val_mae: 8.7571\n",
      "Epoch 198/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 266.7882 - mae: 8.4205\n",
      "Epoch 198: val_loss did not improve from 270.29904\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 266.4294 - mae: 8.4200 - val_loss: 290.1895 - val_mae: 8.9159\n",
      "Epoch 199/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 236.3724 - mae: 8.3581\n",
      "Epoch 199: val_loss improved from 270.29904 to 251.61203, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 234.8178 - mae: 8.3531 - val_loss: 251.6120 - val_mae: 8.6995\n",
      "Epoch 200/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 229.5670 - mae: 8.3794\n",
      "Epoch 200: val_loss did not improve from 251.61203\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 209.9177 - mae: 8.3052 - val_loss: 268.1848 - val_mae: 8.7695\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일: 모델학습을 위한 학습과정 설정단계\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "# print(\"\\n Accuracy: %.4f\" % model.evaluate(x_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25b37198220>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_feature)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(test_label, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7adfe9f3775c954d01fc005199cfce03b4193bbc6ff21451e4d1eaf7785b4e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
