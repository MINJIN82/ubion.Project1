{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"./xlsx/시차상관분석6Data.xlsx\",index_col=0)\n",
    "df = df.set_index(\"DateTime\")\n",
    "\n",
    "# 대비 계산\n",
    "df['대비_irs_1Y'] = df['1Y_Mid_irs'] - df['1Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_2Y'] = df['2Y_Mid_irs'] - df['2Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_3Y'] = df['3Y_Mid_irs'] - df['3Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_5Y'] = df['5Y_Mid_irs'] - df['5Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_10Y'] = df['10Y_Mid_irs'] - df['10Y_Mid_irs'].shift(1) \n",
    "\n",
    "df['대비_crs_1Y'] = df['1Y_Mid_crs'] - df['1Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_2Y'] = df['2Y_Mid_crs'] - df['2Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_3Y'] = df['3Y_Mid_crs'] - df['3Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_5Y'] = df['5Y_Mid_crs'] - df['5Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_10Y'] = df['10Y_Mid_crs'] - df['10Y_Mid_crs'].shift(1)\n",
    "\n",
    "df['대비_swapbasis_1Y'] = df['1Y_베이시스']-df['1Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_2Y'] = df['2Y_베이시스']-df['2Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_3Y'] = df['3Y_베이시스']-df['3Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_5Y'] = df['5Y_베이시스']-df['5Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_10Y'] = df['10Y_베이시스']-df['10Y_베이시스'].shift(1)\n",
    "\n",
    "df['대비_국고_1Y'] = df['국고1년']-df['국고1년'].shift(1)\n",
    "df['대비_국고_3Y'] = df['국고3년']-df['국고3년'].shift(1)\n",
    "df['대비_국고_5Y'] = df['국고5년']-df['국고5년'].shift(1)\n",
    "df['대비_국고_10Y'] = df['국고10년']-df['국고10년'].shift(1)\n",
    "\n",
    "df['대비_통안_1Y'] = df['통안364일']-df['통안364일'].shift(1)\n",
    "df['대비_통안_2Y'] = df['통안2년']-df['통안2년'].shift(1)\n",
    "\n",
    "df['대비_ndf'] = df['Mid_ndf']-df['Mid_ndf'].shift(1)\n",
    "df['스왑포인트_1M'] = df[\"M1_스왑포인트\"]/100 \n",
    "df['전일종가_ex'] = df['종가_ex'].shift(1)\n",
    "df['종가_NDF_차이'] = df['전일종가_ex'] - df['Mid_ndf']\n",
    "\n",
    "# 필요한 칼럼만 추출\n",
    "df_1 = df[['대비_irs_1Y', '대비_irs_2Y', '대비_irs_3Y', '대비_irs_5Y', '대비_irs_10Y',\n",
    "           '대비_crs_1Y', '대비_crs_2Y', '대비_crs_3Y', '대비_crs_5Y', '대비_crs_10Y', \n",
    "           '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y', '대비_swapbasis_10Y',\n",
    "           '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y', \n",
    "           '대비_통안_1Y', '대비_통안_2Y', '대비_ndf', '스왑포인트_1M', '전일종가_ex', \n",
    "           '종가_ex', '종가_NDF_차이' ]]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['열1', '종가_ex', '대비_ex', '증감률_ex', '1Y_Mid_irs', '1Y_전일비_irs',\n",
       "       '2Y_Mid_irs', '2Y_전일비_irs', '3Y_Mid_irs', '3Y_전일비_irs', '5Y_Mid_irs',\n",
       "       '5Y_전일비_irs', '10Y_Mid_irs', '10Y_전일비_irs', '1Y_Mid_crs', '1Y_전일비_crs',\n",
       "       '2Y_Mid_crs', '2Y_전일비_crs', '3Y_Mid_crs', '3Y_전일비_crs', '5Y_Mid_crs',\n",
       "       '5Y_전일비_crs', '10Y_Mid_crs', '10Y_전일비_crs', '국고1년', '국고3년', '국고5년',\n",
       "       '국고10년', '통안364일', '통안2년', 'Bid_ndf', 'Ask_ndf', 'Mid_ndf', '전일비_ndf',\n",
       "       '1Y_베이시스', '2Y_베이시스', '3Y_베이시스', '5Y_베이시스', '10Y_베이시스', 'M1_스왑포인트',\n",
       "       '전일대비_종가_ex', '등락률_종가_ex', '전일비_1Y_irs', '전일비_2Y_irs', '전일비_3Y_irs',\n",
       "       '전일비_5Y_irs', '전일비_10Y_irs', '전일비_1Y_crs', '전일비_2Y_crs', '전일비_3Y_crs',\n",
       "       '전일비_5Y_crs', '전일비_10Y_crs', '국고1년대비', '국고3년대비', '국고5년대비', '국고10년대비',\n",
       "       '통안1년대비', '통안2년대비', '전일비_1Y_베이시스', '전일비_2Y_베이시스', '전일비_3Y_베이시스',\n",
       "       '전일비_5Y_베이시스', '전일비_10Y_베이시스', '전날 종가_ex', '종가_NDF차이', '대비_irs_1Y',\n",
       "       '대비_irs_2Y', '대비_irs_3Y', '대비_irs_5Y', '대비_irs_10Y', '대비_crs_1Y',\n",
       "       '대비_crs_2Y', '대비_crs_3Y', '대비_crs_5Y', '대비_crs_10Y', '대비_swapbasis_1Y',\n",
       "       '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y',\n",
       "       '대비_swapbasis_10Y', '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y',\n",
       "       '대비_통안_1Y', '대비_통안_2Y', '대비_ndf', '스왑포인트_1M', '전일종가_ex', '종가_NDF_차이'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "대비_irs_1Y           0\n",
       "대비_irs_2Y           0\n",
       "대비_irs_3Y           0\n",
       "대비_irs_5Y           0\n",
       "대비_irs_10Y          0\n",
       "대비_crs_1Y           0\n",
       "대비_crs_2Y           0\n",
       "대비_crs_3Y           0\n",
       "대비_crs_5Y           0\n",
       "대비_crs_10Y          0\n",
       "대비_swapbasis_1Y     0\n",
       "대비_swapbasis_2Y     0\n",
       "대비_swapbasis_3Y     0\n",
       "대비_swapbasis_5Y     0\n",
       "대비_swapbasis_10Y    0\n",
       "대비_국고_1Y            0\n",
       "대비_국고_3Y            0\n",
       "대비_국고_5Y            0\n",
       "대비_국고_10Y           0\n",
       "대비_통안_1Y            0\n",
       "대비_통안_2Y            0\n",
       "대비_ndf              0\n",
       "스왑포인트_1M            0\n",
       "전일종가_ex             0\n",
       "종가_ex               0\n",
       "종가_NDF_차이           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyeok\\AppData\\Local\\Temp\\ipykernel_43956\\1840118312.py:6: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  x.feature = x.columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_swapbasis_1Y</th>\n",
       "      <th>대비_swapbasis_2Y</th>\n",
       "      <th>대비_swapbasis_3Y</th>\n",
       "      <th>대비_swapbasis_5Y</th>\n",
       "      <th>대비_swapbasis_10Y</th>\n",
       "      <th>대비_국고_1Y</th>\n",
       "      <th>대비_국고_3Y</th>\n",
       "      <th>대비_국고_5Y</th>\n",
       "      <th>대비_국고_10Y</th>\n",
       "      <th>대비_통안_1Y</th>\n",
       "      <th>대비_통안_2Y</th>\n",
       "      <th>대비_ndf</th>\n",
       "      <th>스왑포인트_1M</th>\n",
       "      <th>전일종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348821</td>\n",
       "      <td>0.287540</td>\n",
       "      <td>0.157678</td>\n",
       "      <td>-0.910053</td>\n",
       "      <td>-2.158273</td>\n",
       "      <td>-1.133777</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>-1.798842</td>\n",
       "      <td>-0.217667</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>0.686282</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.056282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350742</td>\n",
       "      <td>-0.873325</td>\n",
       "      <td>-0.803045</td>\n",
       "      <td>-1.091422</td>\n",
       "      <td>-0.831869</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>0.159979</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>0.123726</td>\n",
       "      <td>-1.668663</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.000487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173930</td>\n",
       "      <td>0.287540</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>-1.454160</td>\n",
       "      <td>-1.660871</td>\n",
       "      <td>-0.568154</td>\n",
       "      <td>-0.001379</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>0.075741</td>\n",
       "      <td>1.911215</td>\n",
       "      <td>-0.104877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.000961</td>\n",
       "      <td>-0.389631</td>\n",
       "      <td>-0.322684</td>\n",
       "      <td>-0.728685</td>\n",
       "      <td>-1.163470</td>\n",
       "      <td>-0.568154</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>-0.514104</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>0.206571</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.108476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-09</th>\n",
       "      <td>0.698602</td>\n",
       "      <td>-0.389631</td>\n",
       "      <td>-0.322684</td>\n",
       "      <td>-0.184578</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>2.825583</td>\n",
       "      <td>0.966767</td>\n",
       "      <td>1.351270</td>\n",
       "      <td>1.284529</td>\n",
       "      <td>0.432293</td>\n",
       "      <td>0.747946</td>\n",
       "      <td>0.381012</td>\n",
       "      <td>1.775350</td>\n",
       "      <td>-0.117475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700523</td>\n",
       "      <td>-0.196154</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>-0.365947</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>-1.133777</td>\n",
       "      <td>-0.969524</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>-2.312737</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.625337</td>\n",
       "      <td>0.572896</td>\n",
       "      <td>-0.896666</td>\n",
       "      <td>3.206786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348821</td>\n",
       "      <td>-0.196154</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>0.178159</td>\n",
       "      <td>0.162934</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>-0.771052</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>0.189127</td>\n",
       "      <td>-0.987243</td>\n",
       "      <td>3.219385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348821</td>\n",
       "      <td>0.771234</td>\n",
       "      <td>0.798160</td>\n",
       "      <td>0.903634</td>\n",
       "      <td>0.826135</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-1.350390</td>\n",
       "      <td>-1.541894</td>\n",
       "      <td>0.215640</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>-0.098699</td>\n",
       "      <td>-0.851378</td>\n",
       "      <td>3.109596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173930</td>\n",
       "      <td>-0.002676</td>\n",
       "      <td>-0.162563</td>\n",
       "      <td>-0.365947</td>\n",
       "      <td>-0.831869</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>0.644052</td>\n",
       "      <td>0.810938</td>\n",
       "      <td>2.055371</td>\n",
       "      <td>0.215640</td>\n",
       "      <td>0.373414</td>\n",
       "      <td>-0.796461</td>\n",
       "      <td>-0.941955</td>\n",
       "      <td>3.212186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700523</td>\n",
       "      <td>-0.583109</td>\n",
       "      <td>-0.162563</td>\n",
       "      <td>0.903634</td>\n",
       "      <td>0.494535</td>\n",
       "      <td>-2.265023</td>\n",
       "      <td>-2.099027</td>\n",
       "      <td>-3.241553</td>\n",
       "      <td>-2.055789</td>\n",
       "      <td>-0.325994</td>\n",
       "      <td>-1.374400</td>\n",
       "      <td>-1.799493</td>\n",
       "      <td>-0.896666</td>\n",
       "      <td>2.902617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2458 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_swapbasis_1Y  대비_swapbasis_2Y  대비_swapbasis_3Y  \\\n",
       "DateTime                                                        \n",
       "2012-08-03         0.348821         0.287540         0.157678   \n",
       "2012-08-06        -0.350742        -0.873325        -0.803045   \n",
       "2012-08-07         0.173930         0.287540        -0.002443   \n",
       "2012-08-08        -0.000961        -0.389631        -0.322684   \n",
       "2012-08-09         0.698602        -0.389631        -0.322684   \n",
       "...                     ...              ...              ...   \n",
       "2022-07-25        -0.700523        -0.196154        -0.002443   \n",
       "2022-07-26         0.348821        -0.196154        -0.002443   \n",
       "2022-07-27         0.348821         0.771234         0.798160   \n",
       "2022-07-28         0.173930        -0.002676        -0.162563   \n",
       "2022-07-29        -0.700523        -0.583109        -0.162563   \n",
       "\n",
       "            대비_swapbasis_5Y  대비_swapbasis_10Y  대비_국고_1Y  대비_국고_3Y  대비_국고_5Y  \\\n",
       "DateTime                                                                      \n",
       "2012-08-03        -0.910053         -2.158273 -1.133777 -0.324094 -1.890723   \n",
       "2012-08-06        -1.091422         -0.831869  0.563092  0.159979  0.000440   \n",
       "2012-08-07        -1.454160         -1.660871 -0.568154 -0.001379  0.000440   \n",
       "2012-08-08        -0.728685         -1.163470 -0.568154 -0.324094 -0.539892   \n",
       "2012-08-09        -0.184578         -0.002867  2.825583  0.966767  1.351270   \n",
       "...                     ...               ...       ...       ...       ...   \n",
       "2022-07-25        -0.365947         -0.002867 -1.133777 -0.969524 -1.890723   \n",
       "2022-07-26         0.178159          0.162934  0.563092 -0.485451 -0.539892   \n",
       "2022-07-27         0.903634          0.826135 -0.002531 -0.485451 -1.350390   \n",
       "2022-07-28        -0.365947         -0.831869  0.563092  0.644052  0.810938   \n",
       "2022-07-29         0.903634          0.494535 -2.265023 -2.099027 -3.241553   \n",
       "\n",
       "            대비_국고_10Y  대비_통안_1Y  대비_통안_2Y    대비_ndf  스왑포인트_1M   전일종가_ex  \n",
       "DateTime                                                                 \n",
       "2012-08-03  -1.798842 -0.217667 -0.125961  0.686282  1.820638 -0.056282  \n",
       "2012-08-06  -0.000209  0.107313  0.123726 -1.668663  1.820638 -0.000487  \n",
       "2012-08-07  -0.000209 -0.109340 -0.125961  0.075741  1.911215 -0.104877  \n",
       "2012-08-08  -0.514104 -0.109340 -0.125961  0.206571  1.820638 -0.108476  \n",
       "2012-08-09   1.284529  0.432293  0.747946  0.381012  1.775350 -0.117475  \n",
       "...               ...       ...       ...       ...       ...       ...  \n",
       "2022-07-25  -2.312737 -0.109340 -0.625337  0.572896 -0.896666  3.206786  \n",
       "2022-07-26  -0.771052  0.107313 -0.001117  0.189127 -0.987243  3.219385  \n",
       "2022-07-27  -1.541894  0.215640 -0.125961 -0.098699 -0.851378  3.109596  \n",
       "2022-07-28   2.055371  0.215640  0.373414 -0.796461 -0.941955  3.212186  \n",
       "2022-07-29  -2.055789 -0.325994 -1.374400 -1.799493 -0.896666  2.902617  \n",
       "\n",
       "[2458 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df_1[['대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y', '대비_swapbasis_10Y',   \n",
    "          '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y', '대비_통안_1Y', '대비_통안_2Y', '대비_ndf', '스왑포인트_1M',\n",
    "          '전일종가_ex']]\n",
    "y = df_1['종가_ex']\n",
    "\n",
    "x.feature = x.columns \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor           Feature\n",
      "0     3.156830   대비_swapbasis_1Y\n",
      "1     5.913226   대비_swapbasis_2Y\n",
      "2     2.406082   대비_swapbasis_3Y\n",
      "3     4.766315   대비_swapbasis_5Y\n",
      "4     3.270022  대비_swapbasis_10Y\n",
      "5     1.832973          대비_국고_1Y\n",
      "6     1.243944          대비_국고_3Y\n",
      "7     5.823271          대비_국고_5Y\n",
      "8     4.883435         대비_국고_10Y\n",
      "9     1.024826          대비_통안_1Y\n",
      "10    1.113971          대비_통안_2Y\n",
      "11    1.028486            대비_ndf\n",
      "12    1.169340          스왑포인트_1M\n",
      "13    1.178408           전일종가_ex\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.184e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:57:07</td>     <th>  Log-Likelihood:    </th> <td> -7421.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.487e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2443</td>      <th>  BIC:               </th> <td>1.496e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    14</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8952</td> <td>    0.100</td> <td> 1.13e+04</td> <td> 0.000</td> <td> 1134.699</td> <td> 1135.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -1.2019</td> <td>    0.178</td> <td>   -6.748</td> <td> 0.000</td> <td>   -1.551</td> <td>   -0.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_2Y</th>  <td>    0.0140</td> <td>    0.244</td> <td>    0.057</td> <td> 0.954</td> <td>   -0.464</td> <td>    0.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.2029</td> <td>    0.156</td> <td>   -1.305</td> <td> 0.192</td> <td>   -0.508</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th>  <td>    0.2616</td> <td>    0.219</td> <td>    1.195</td> <td> 0.232</td> <td>   -0.168</td> <td>    0.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.1530</td> <td>    0.181</td> <td>    0.844</td> <td> 0.399</td> <td>   -0.203</td> <td>    0.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>    0.0693</td> <td>    0.136</td> <td>    0.511</td> <td> 0.609</td> <td>   -0.197</td> <td>    0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.2896</td> <td>    0.112</td> <td>   -2.590</td> <td> 0.010</td> <td>   -0.509</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>    0.2410</td> <td>    0.242</td> <td>    0.996</td> <td> 0.319</td> <td>   -0.233</td> <td>    0.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.1365</td> <td>    0.222</td> <td>   -0.616</td> <td> 0.538</td> <td>   -0.571</td> <td>    0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_1Y</th>         <td>   -0.0471</td> <td>    0.101</td> <td>   -0.464</td> <td> 0.642</td> <td>   -0.246</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0547</td> <td>    0.106</td> <td>    0.517</td> <td> 0.605</td> <td>   -0.153</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>           <td>    2.4762</td> <td>    0.102</td> <td>   24.356</td> <td> 0.000</td> <td>    2.277</td> <td>    2.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -0.1131</td> <td>    0.108</td> <td>   -1.044</td> <td> 0.297</td> <td>   -0.326</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.3022</td> <td>    0.109</td> <td>  508.163</td> <td> 0.000</td> <td>   55.089</td> <td>   55.516</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>69.692</td> <th>  Durbin-Watson:     </th> <td>   2.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 167.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.068</td> <th>  Prob(JB):          </th> <td>4.39e-37</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.271</td> <th>  Cond. No.          </th> <td>    6.54</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 2.184e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:57:07   Log-Likelihood:                -7421.6\n",
       "No. Observations:                2458   AIC:                         1.487e+04\n",
       "Df Residuals:                    2443   BIC:                         1.496e+04\n",
       "Df Model:                          14                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8952      0.100   1.13e+04      0.000    1134.699    1135.092\n",
       "대비_swapbasis_1Y     -1.2019      0.178     -6.748      0.000      -1.551      -0.853\n",
       "대비_swapbasis_2Y      0.0140      0.244      0.057      0.954      -0.464       0.492\n",
       "대비_swapbasis_3Y     -0.2029      0.156     -1.305      0.192      -0.508       0.102\n",
       "대비_swapbasis_5Y      0.2616      0.219      1.195      0.232      -0.168       0.691\n",
       "대비_swapbasis_10Y     0.1530      0.181      0.844      0.399      -0.203       0.508\n",
       "대비_국고_1Y             0.0693      0.136      0.511      0.609      -0.197       0.335\n",
       "대비_국고_3Y            -0.2896      0.112     -2.590      0.010      -0.509      -0.070\n",
       "대비_국고_5Y             0.2410      0.242      0.996      0.319      -0.233       0.715\n",
       "대비_국고_10Y           -0.1365      0.222     -0.616      0.538      -0.571       0.298\n",
       "대비_통안_1Y            -0.0471      0.101     -0.464      0.642      -0.246       0.152\n",
       "대비_통안_2Y             0.0547      0.106      0.517      0.605      -0.153       0.262\n",
       "대비_ndf               2.4762      0.102     24.356      0.000       2.277       2.676\n",
       "스왑포인트_1M            -0.1131      0.108     -1.044      0.297      -0.326       0.099\n",
       "전일종가_ex             55.3022      0.109    508.163      0.000      55.089      55.516\n",
       "==============================================================================\n",
       "Omnibus:                       69.692   Durbin-Watson:                   2.662\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              167.434\n",
       "Skew:                           0.068   Prob(JB):                     4.39e-37\n",
       "Kurtosis:                       4.271   Cond. No.                         6.54\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.550e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:57:14</td>     <th>  Log-Likelihood:    </th> <td> -7421.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.487e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2445</td>      <th>  BIC:               </th> <td>1.494e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8952</td> <td>    0.100</td> <td> 1.13e+04</td> <td> 0.000</td> <td> 1134.699</td> <td> 1135.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -1.1963</td> <td>    0.149</td> <td>   -8.019</td> <td> 0.000</td> <td>   -1.489</td> <td>   -0.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.2009</td> <td>    0.151</td> <td>   -1.335</td> <td> 0.182</td> <td>   -0.496</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th>  <td>    0.2667</td> <td>    0.202</td> <td>    1.318</td> <td> 0.187</td> <td>   -0.130</td> <td>    0.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.1542</td> <td>    0.179</td> <td>    0.862</td> <td> 0.389</td> <td>   -0.197</td> <td>    0.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>    0.0640</td> <td>    0.135</td> <td>    0.473</td> <td> 0.636</td> <td>   -0.201</td> <td>    0.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.2901</td> <td>    0.112</td> <td>   -2.596</td> <td> 0.009</td> <td>   -0.509</td> <td>   -0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>    0.2382</td> <td>    0.242</td> <td>    0.986</td> <td> 0.324</td> <td>   -0.236</td> <td>    0.712</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.1361</td> <td>    0.221</td> <td>   -0.615</td> <td> 0.539</td> <td>   -0.570</td> <td>    0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0541</td> <td>    0.106</td> <td>    0.511</td> <td> 0.609</td> <td>   -0.153</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>           <td>    2.4753</td> <td>    0.102</td> <td>   24.379</td> <td> 0.000</td> <td>    2.276</td> <td>    2.674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -0.1133</td> <td>    0.108</td> <td>   -1.046</td> <td> 0.296</td> <td>   -0.326</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.3018</td> <td>    0.109</td> <td>  508.380</td> <td> 0.000</td> <td>   55.088</td> <td>   55.515</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>69.557</td> <th>  Durbin-Watson:     </th> <td>   2.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 167.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.067</td> <th>  Prob(JB):          </th> <td>5.31e-37</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.270</td> <th>  Cond. No.          </th> <td>    5.98</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 2.550e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:57:14   Log-Likelihood:                -7421.7\n",
       "No. Observations:                2458   AIC:                         1.487e+04\n",
       "Df Residuals:                    2445   BIC:                         1.494e+04\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8952      0.100   1.13e+04      0.000    1134.699    1135.092\n",
       "대비_swapbasis_1Y     -1.1963      0.149     -8.019      0.000      -1.489      -0.904\n",
       "대비_swapbasis_3Y     -0.2009      0.151     -1.335      0.182      -0.496       0.094\n",
       "대비_swapbasis_5Y      0.2667      0.202      1.318      0.187      -0.130       0.663\n",
       "대비_swapbasis_10Y     0.1542      0.179      0.862      0.389      -0.197       0.505\n",
       "대비_국고_1Y             0.0640      0.135      0.473      0.636      -0.201       0.329\n",
       "대비_국고_3Y            -0.2901      0.112     -2.596      0.009      -0.509      -0.071\n",
       "대비_국고_5Y             0.2382      0.242      0.986      0.324      -0.236       0.712\n",
       "대비_국고_10Y           -0.1361      0.221     -0.615      0.539      -0.570       0.298\n",
       "대비_통안_2Y             0.0541      0.106      0.511      0.609      -0.153       0.261\n",
       "대비_ndf               2.4753      0.102     24.379      0.000       2.276       2.674\n",
       "스왑포인트_1M            -0.1133      0.108     -1.046      0.296      -0.326       0.099\n",
       "전일종가_ex             55.3018      0.109    508.380      0.000      55.088      55.515\n",
       "==============================================================================\n",
       "Omnibus:                       69.557   Durbin-Watson:                   2.662\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              167.051\n",
       "Skew:                           0.067   Prob(JB):                     5.31e-37\n",
       "Kurtosis:                       4.270   Cond. No.                         5.98\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_2Y', '대비_통안_1Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor           Feature\n",
      "0     2.215397   대비_swapbasis_1Y\n",
      "1     2.253619   대비_swapbasis_3Y\n",
      "2     4.072407   대비_swapbasis_5Y\n",
      "3     3.185493  대비_swapbasis_10Y\n",
      "4     1.219101          대비_국고_3Y\n",
      "5     5.236750          대비_국고_5Y\n",
      "6     4.879939         대비_국고_10Y\n",
      "7     1.092920          대비_통안_2Y\n",
      "8     1.024945            대비_ndf\n",
      "9     1.169067          스왑포인트_1M\n",
      "10    1.169460           전일종가_ex\n"
     ]
    }
   ],
   "source": [
    "x_scaled.drop(['대비_국고_1Y'], axis=1, inplace=True)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.783e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:57:23</td>     <th>  Log-Likelihood:    </th> <td> -7421.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.487e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2446</td>      <th>  BIC:               </th> <td>1.494e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8952</td> <td>    0.100</td> <td> 1.13e+04</td> <td> 0.000</td> <td> 1134.699</td> <td> 1135.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -1.1971</td> <td>    0.149</td> <td>   -8.026</td> <td> 0.000</td> <td>   -1.490</td> <td>   -0.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.2032</td> <td>    0.150</td> <td>   -1.351</td> <td> 0.177</td> <td>   -0.498</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th>  <td>    0.2648</td> <td>    0.202</td> <td>    1.310</td> <td> 0.190</td> <td>   -0.132</td> <td>    0.661</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.1566</td> <td>    0.179</td> <td>    0.876</td> <td> 0.381</td> <td>   -0.194</td> <td>    0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.2828</td> <td>    0.111</td> <td>   -2.556</td> <td> 0.011</td> <td>   -0.500</td> <td>   -0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>    0.2743</td> <td>    0.229</td> <td>    1.196</td> <td> 0.232</td> <td>   -0.175</td> <td>    0.724</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.1359</td> <td>    0.221</td> <td>   -0.614</td> <td> 0.539</td> <td>   -0.570</td> <td>    0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0609</td> <td>    0.105</td> <td>    0.581</td> <td> 0.561</td> <td>   -0.145</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>           <td>    2.4734</td> <td>    0.101</td> <td>   24.382</td> <td> 0.000</td> <td>    2.274</td> <td>    2.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -0.1126</td> <td>    0.108</td> <td>   -1.039</td> <td> 0.299</td> <td>   -0.325</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.3062</td> <td>    0.108</td> <td>  510.407</td> <td> 0.000</td> <td>   55.094</td> <td>   55.519</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>68.835</td> <th>  Durbin-Watson:     </th> <td>   2.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 164.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.063</td> <th>  Prob(JB):          </th> <td>1.52e-36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.263</td> <th>  Cond. No.          </th> <td>    5.68</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 2.783e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:57:23   Log-Likelihood:                -7421.8\n",
       "No. Observations:                2458   AIC:                         1.487e+04\n",
       "Df Residuals:                    2446   BIC:                         1.494e+04\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8952      0.100   1.13e+04      0.000    1134.699    1135.092\n",
       "대비_swapbasis_1Y     -1.1971      0.149     -8.026      0.000      -1.490      -0.905\n",
       "대비_swapbasis_3Y     -0.2032      0.150     -1.351      0.177      -0.498       0.092\n",
       "대비_swapbasis_5Y      0.2648      0.202      1.310      0.190      -0.132       0.661\n",
       "대비_swapbasis_10Y     0.1566      0.179      0.876      0.381      -0.194       0.507\n",
       "대비_국고_3Y            -0.2828      0.111     -2.556      0.011      -0.500      -0.066\n",
       "대비_국고_5Y             0.2743      0.229      1.196      0.232      -0.175       0.724\n",
       "대비_국고_10Y           -0.1359      0.221     -0.614      0.539      -0.570       0.298\n",
       "대비_통안_2Y             0.0609      0.105      0.581      0.561      -0.145       0.266\n",
       "대비_ndf               2.4734      0.101     24.382      0.000       2.274       2.672\n",
       "스왑포인트_1M            -0.1126      0.108     -1.039      0.299      -0.325       0.100\n",
       "전일종가_ex             55.3062      0.108    510.407      0.000      55.094      55.519\n",
       "==============================================================================\n",
       "Omnibus:                       68.835   Durbin-Watson:                   2.662\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              164.942\n",
       "Skew:                           0.063   Prob(JB):                     1.52e-36\n",
       "Kurtosis:                       4.263   Cond. No.                         5.68\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.403e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:57:36</td>     <th>  Log-Likelihood:    </th> <td> -7422.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.486e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2448</td>      <th>  BIC:               </th> <td>1.492e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8952</td> <td>    0.100</td> <td> 1.13e+04</td> <td> 0.000</td> <td> 1134.699</td> <td> 1135.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -1.2013</td> <td>    0.149</td> <td>   -8.064</td> <td> 0.000</td> <td>   -1.493</td> <td>   -0.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.2059</td> <td>    0.150</td> <td>   -1.369</td> <td> 0.171</td> <td>   -0.501</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th>  <td>    0.2634</td> <td>    0.202</td> <td>    1.303</td> <td> 0.193</td> <td>   -0.133</td> <td>    0.660</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.1660</td> <td>    0.178</td> <td>    0.931</td> <td> 0.352</td> <td>   -0.184</td> <td>    0.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.2788</td> <td>    0.110</td> <td>   -2.523</td> <td> 0.012</td> <td>   -0.495</td> <td>   -0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>    0.1697</td> <td>    0.114</td> <td>    1.485</td> <td> 0.138</td> <td>   -0.054</td> <td>    0.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>           <td>    2.4755</td> <td>    0.101</td> <td>   24.417</td> <td> 0.000</td> <td>    2.277</td> <td>    2.674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -0.1118</td> <td>    0.108</td> <td>   -1.032</td> <td> 0.302</td> <td>   -0.324</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.3068</td> <td>    0.108</td> <td>  510.561</td> <td> 0.000</td> <td>   55.094</td> <td>   55.519</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>68.888</td> <th>  Durbin-Watson:     </th> <td>   2.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 165.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.057</td> <th>  Prob(JB):          </th> <td>9.40e-37</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.268</td> <th>  Cond. No.          </th> <td>    4.37</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 3.403e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:57:36   Log-Likelihood:                -7422.2\n",
       "No. Observations:                2458   AIC:                         1.486e+04\n",
       "Df Residuals:                    2448   BIC:                         1.492e+04\n",
       "Df Model:                           9                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8952      0.100   1.13e+04      0.000    1134.699    1135.092\n",
       "대비_swapbasis_1Y     -1.2013      0.149     -8.064      0.000      -1.493      -0.909\n",
       "대비_swapbasis_3Y     -0.2059      0.150     -1.369      0.171      -0.501       0.089\n",
       "대비_swapbasis_5Y      0.2634      0.202      1.303      0.193      -0.133       0.660\n",
       "대비_swapbasis_10Y     0.1660      0.178      0.931      0.352      -0.184       0.516\n",
       "대비_국고_3Y            -0.2788      0.110     -2.523      0.012      -0.495      -0.062\n",
       "대비_국고_5Y             0.1697      0.114      1.485      0.138      -0.054       0.394\n",
       "대비_ndf               2.4755      0.101     24.417      0.000       2.277       2.674\n",
       "스왑포인트_1M            -0.1118      0.108     -1.032      0.302      -0.324       0.101\n",
       "전일종가_ex             55.3068      0.108    510.561      0.000      55.094      55.519\n",
       "==============================================================================\n",
       "Omnibus:                       68.888   Durbin-Watson:                   2.663\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              165.909\n",
       "Skew:                           0.057   Prob(JB):                     9.40e-37\n",
       "Kurtosis:                       4.268   Cond. No.                         4.37\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_통안_2Y','대비_국고_10Y' ], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>4.376e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:58:46</td>     <th>  Log-Likelihood:    </th> <td> -7423.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.486e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2450</td>      <th>  BIC:               </th> <td>1.491e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td> 1134.8952</td> <td>    0.100</td> <td> 1.13e+04</td> <td> 0.000</td> <td> 1134.699</td> <td> 1135.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th> <td>   -1.1878</td> <td>    0.148</td> <td>   -8.013</td> <td> 0.000</td> <td>   -1.478</td> <td>   -0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th> <td>   -0.1884</td> <td>    0.149</td> <td>   -1.263</td> <td> 0.207</td> <td>   -0.481</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th> <td>    0.3768</td> <td>    0.162</td> <td>    2.321</td> <td> 0.020</td> <td>    0.058</td> <td>    0.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>        <td>   -0.2764</td> <td>    0.110</td> <td>   -2.503</td> <td> 0.012</td> <td>   -0.493</td> <td>   -0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>        <td>    0.1579</td> <td>    0.114</td> <td>    1.388</td> <td> 0.165</td> <td>   -0.065</td> <td>    0.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>          <td>    2.4797</td> <td>    0.101</td> <td>   24.472</td> <td> 0.000</td> <td>    2.281</td> <td>    2.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>         <td>   55.3487</td> <td>    0.100</td> <td>  552.222</td> <td> 0.000</td> <td>   55.152</td> <td>   55.545</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>68.648</td> <th>  Durbin-Watson:     </th> <td>   2.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 164.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.058</td> <th>  Prob(JB):          </th> <td>1.61e-36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.263</td> <th>  Cond. No.          </th> <td>    3.07</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 4.376e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:58:46   Log-Likelihood:                -7423.1\n",
       "No. Observations:                2458   AIC:                         1.486e+04\n",
       "Df Residuals:                    2450   BIC:                         1.491e+04\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const            1134.8952      0.100   1.13e+04      0.000    1134.699    1135.092\n",
       "대비_swapbasis_1Y    -1.1878      0.148     -8.013      0.000      -1.478      -0.897\n",
       "대비_swapbasis_3Y    -0.1884      0.149     -1.263      0.207      -0.481       0.104\n",
       "대비_swapbasis_5Y     0.3768      0.162      2.321      0.020       0.058       0.695\n",
       "대비_국고_3Y           -0.2764      0.110     -2.503      0.012      -0.493      -0.060\n",
       "대비_국고_5Y            0.1579      0.114      1.388      0.165      -0.065       0.381\n",
       "대비_ndf              2.4797      0.101     24.472      0.000       2.281       2.678\n",
       "전일종가_ex            55.3487      0.100    552.222      0.000      55.152      55.545\n",
       "==============================================================================\n",
       "Omnibus:                       68.648   Durbin-Watson:                   2.664\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              164.829\n",
       "Skew:                           0.058   Prob(JB):                     1.61e-36\n",
       "Kurtosis:                       4.263   Cond. No.                         3.07\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_10Y', '스왑포인트_1M'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>5.104e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:59:02</td>     <th>  Log-Likelihood:    </th> <td> -7423.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.486e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2451</td>      <th>  BIC:               </th> <td>1.490e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td> 1134.8952</td> <td>    0.100</td> <td> 1.13e+04</td> <td> 0.000</td> <td> 1134.699</td> <td> 1135.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th> <td>   -1.2431</td> <td>    0.142</td> <td>   -8.776</td> <td> 0.000</td> <td>   -1.521</td> <td>   -0.965</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th> <td>    0.2840</td> <td>    0.145</td> <td>    1.961</td> <td> 0.050</td> <td> 7.16e-05</td> <td>    0.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>        <td>   -0.2730</td> <td>    0.110</td> <td>   -2.473</td> <td> 0.013</td> <td>   -0.490</td> <td>   -0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>        <td>    0.1667</td> <td>    0.114</td> <td>    1.468</td> <td> 0.142</td> <td>   -0.056</td> <td>    0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>          <td>    2.4781</td> <td>    0.101</td> <td>   24.455</td> <td> 0.000</td> <td>    2.279</td> <td>    2.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>         <td>   55.3483</td> <td>    0.100</td> <td>  552.154</td> <td> 0.000</td> <td>   55.152</td> <td>   55.545</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>68.111</td> <th>  Durbin-Watson:     </th> <td>   2.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 162.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.060</td> <th>  Prob(JB):          </th> <td>4.92e-36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.254</td> <th>  Cond. No.          </th> <td>    2.59</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 5.104e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:59:02   Log-Likelihood:                -7423.9\n",
       "No. Observations:                2458   AIC:                         1.486e+04\n",
       "Df Residuals:                    2451   BIC:                         1.490e+04\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const            1134.8952      0.100   1.13e+04      0.000    1134.699    1135.092\n",
       "대비_swapbasis_1Y    -1.2431      0.142     -8.776      0.000      -1.521      -0.965\n",
       "대비_swapbasis_5Y     0.2840      0.145      1.961      0.050    7.16e-05       0.568\n",
       "대비_국고_3Y           -0.2730      0.110     -2.473      0.013      -0.490      -0.057\n",
       "대비_국고_5Y            0.1667      0.114      1.468      0.142      -0.056       0.389\n",
       "대비_ndf              2.4781      0.101     24.455      0.000       2.279       2.677\n",
       "전일종가_ex            55.3483      0.100    552.154      0.000      55.152      55.545\n",
       "==============================================================================\n",
       "Omnibus:                       68.111   Durbin-Watson:                   2.664\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              162.600\n",
       "Skew:                           0.060   Prob(JB):                     4.92e-36\n",
       "Kurtosis:                       4.254   Cond. No.                         2.59\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_3Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>6.121e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:59:28</td>     <th>  Log-Likelihood:    </th> <td> -7425.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.486e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2452</td>      <th>  BIC:               </th> <td>1.490e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td> 1134.8952</td> <td>    0.100</td> <td> 1.13e+04</td> <td> 0.000</td> <td> 1134.699</td> <td> 1135.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th> <td>   -1.2250</td> <td>    0.141</td> <td>   -8.679</td> <td> 0.000</td> <td>   -1.502</td> <td>   -0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th> <td>    0.2411</td> <td>    0.142</td> <td>    1.700</td> <td> 0.089</td> <td>   -0.037</td> <td>    0.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>        <td>   -0.2072</td> <td>    0.101</td> <td>   -2.053</td> <td> 0.040</td> <td>   -0.405</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>          <td>    2.4892</td> <td>    0.101</td> <td>   24.627</td> <td> 0.000</td> <td>    2.291</td> <td>    2.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>         <td>   55.3487</td> <td>    0.100</td> <td>  552.031</td> <td> 0.000</td> <td>   55.152</td> <td>   55.545</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>66.625</td> <th>  Durbin-Watson:     </th> <td>   2.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 157.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.058</td> <th>  Prob(JB):          </th> <td>6.61e-35</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.234</td> <th>  Cond. No.          </th> <td>    2.44</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 6.121e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:59:28   Log-Likelihood:                -7425.0\n",
       "No. Observations:                2458   AIC:                         1.486e+04\n",
       "Df Residuals:                    2452   BIC:                         1.490e+04\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const            1134.8952      0.100   1.13e+04      0.000    1134.699    1135.092\n",
       "대비_swapbasis_1Y    -1.2250      0.141     -8.679      0.000      -1.502      -0.948\n",
       "대비_swapbasis_5Y     0.2411      0.142      1.700      0.089      -0.037       0.519\n",
       "대비_국고_3Y           -0.2072      0.101     -2.053      0.040      -0.405      -0.009\n",
       "대비_ndf              2.4892      0.101     24.627      0.000       2.291       2.687\n",
       "전일종가_ex            55.3487      0.100    552.031      0.000      55.152      55.545\n",
       "==============================================================================\n",
       "Omnibus:                       66.625   Durbin-Watson:                   2.666\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              157.405\n",
       "Skew:                           0.058   Prob(JB):                     6.61e-35\n",
       "Kurtosis:                       4.234   Cond. No.                         2.44\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_국고_5Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>7.646e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:59:44</td>     <th>  Log-Likelihood:    </th> <td> -7426.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.486e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2453</td>      <th>  BIC:               </th> <td>1.489e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td> 1134.8952</td> <td>    0.100</td> <td> 1.13e+04</td> <td> 0.000</td> <td> 1134.699</td> <td> 1135.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th> <td>   -1.0575</td> <td>    0.101</td> <td>  -10.461</td> <td> 0.000</td> <td>   -1.256</td> <td>   -0.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>        <td>   -0.2246</td> <td>    0.100</td> <td>   -2.236</td> <td> 0.025</td> <td>   -0.422</td> <td>   -0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>          <td>    2.4802</td> <td>    0.101</td> <td>   24.562</td> <td> 0.000</td> <td>    2.282</td> <td>    2.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>         <td>   55.3469</td> <td>    0.100</td> <td>  551.833</td> <td> 0.000</td> <td>   55.150</td> <td>   55.544</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>69.969</td> <th>  Durbin-Watson:     </th> <td>   2.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 170.669</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.052</td> <th>  Prob(JB):          </th> <td>8.70e-38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.287</td> <th>  Cond. No.          </th> <td>    1.14</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 7.646e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:59:44   Log-Likelihood:                -7426.5\n",
       "No. Observations:                2458   AIC:                         1.486e+04\n",
       "Df Residuals:                    2453   BIC:                         1.489e+04\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const            1134.8952      0.100   1.13e+04      0.000    1134.699    1135.092\n",
       "대비_swapbasis_1Y    -1.0575      0.101    -10.461      0.000      -1.256      -0.859\n",
       "대비_국고_3Y           -0.2246      0.100     -2.236      0.025      -0.422      -0.028\n",
       "대비_ndf              2.4802      0.101     24.562      0.000       2.282       2.678\n",
       "전일종가_ex            55.3469      0.100    551.833      0.000      55.150      55.544\n",
       "==============================================================================\n",
       "Omnibus:                       69.969   Durbin-Watson:                   2.672\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              170.669\n",
       "Skew:                           0.052   Prob(JB):                     8.70e-38\n",
       "Kurtosis:                       4.287   Cond. No.                         1.14\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_5Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['대비_swapbasis_1Y', '대비_국고_3Y', '대비_ndf', '전일종가_ex'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_swapbasis_1Y</th>\n",
       "      <th>대비_국고_3Y</th>\n",
       "      <th>대비_국고_5Y</th>\n",
       "      <th>대비_ndf</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348821</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>0.686282</td>\n",
       "      <td>-0.056282</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350742</td>\n",
       "      <td>0.159979</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-1.668663</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173930</td>\n",
       "      <td>-0.001379</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.075741</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.000961</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>0.206571</td>\n",
       "      <td>-0.108476</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-09</th>\n",
       "      <td>0.698602</td>\n",
       "      <td>0.966767</td>\n",
       "      <td>1.351270</td>\n",
       "      <td>0.381012</td>\n",
       "      <td>-0.117475</td>\n",
       "      <td>1125.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700523</td>\n",
       "      <td>-0.969524</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>0.572896</td>\n",
       "      <td>3.206786</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348821</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>0.189127</td>\n",
       "      <td>3.219385</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348821</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-1.350390</td>\n",
       "      <td>-0.098699</td>\n",
       "      <td>3.109596</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173930</td>\n",
       "      <td>0.644052</td>\n",
       "      <td>0.810938</td>\n",
       "      <td>-0.796461</td>\n",
       "      <td>3.212186</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700523</td>\n",
       "      <td>-2.099027</td>\n",
       "      <td>-3.241553</td>\n",
       "      <td>-1.799493</td>\n",
       "      <td>2.902617</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2458 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_swapbasis_1Y  대비_국고_3Y  대비_국고_5Y    대비_ndf   전일종가_ex   종가_ex\n",
       "DateTime                                                                   \n",
       "2012-08-03         0.348821 -0.324094 -1.890723  0.686282 -0.056282  1134.8\n",
       "2012-08-06        -0.350742  0.159979  0.000440 -1.668663 -0.000487  1129.0\n",
       "2012-08-07         0.173930 -0.001379  0.000440  0.075741 -0.104877  1128.8\n",
       "2012-08-08        -0.000961 -0.324094 -0.539892  0.206571 -0.108476  1128.3\n",
       "2012-08-09         0.698602  0.966767  1.351270  0.381012 -0.117475  1125.5\n",
       "...                     ...       ...       ...       ...       ...     ...\n",
       "2022-07-25        -0.700523 -0.969524 -1.890723  0.572896  3.206786  1313.7\n",
       "2022-07-26         0.348821 -0.485451 -0.539892  0.189127  3.219385  1307.6\n",
       "2022-07-27         0.348821 -0.485451 -1.350390 -0.098699  3.109596  1313.3\n",
       "2022-07-28         0.173930  0.644052  0.810938 -0.796461  3.212186  1296.1\n",
       "2022-07-29        -0.700523 -2.099027 -3.241553 -1.799493  2.902617  1299.1\n",
       "\n",
       "[2458 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = pd.concat([x_scaled,y], axis=1)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_scaled[0:1945]\n",
    "test = df_scaled[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=1):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['대비_swapbasis_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_ndf', '전일종가_ex']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1555, 1, 5), (389, 1, 5))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 8.73492412e-01, -1.37856316e-03, -2.69726372e-01,\n",
       "          1.13110547e+00, -5.78228510e-01]],\n",
       "\n",
       "       [[-9.60546374e-04,  3.21336509e-01,  2.70605675e-01,\n",
       "         -1.10173163e+00, -5.09835470e-01]],\n",
       "\n",
       "       [[ 6.98601820e-01,  1.59978973e-01,  2.70605675e-01,\n",
       "         -2.20807622e-01, -2.27365599e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.48820637e-01,  1.59978973e-01,  2.70605675e-01,\n",
       "         -1.07421363e-01,  2.47101382e-02]],\n",
       "\n",
       "       [[-1.75851138e-01,  1.59978973e-01,  8.10937723e-01,\n",
       "          5.99062251e-01,  6.71196959e-03]],\n",
       "\n",
       "       [[-3.50741730e-01, -1.62736099e-01, -2.69726372e-01,\n",
       "          6.51394371e-01,  5.70054647e-01]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512, 1, 5), (512, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 1)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1263666.7500 - mae: 1122.9882\n",
      "Epoch 1: val_loss improved from inf to 1278714.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 13s 97ms/step - loss: 1263926.5000 - mae: 1123.0988 - val_loss: 1278714.1250 - val_mae: 1129.7279\n",
      "Epoch 2/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1263667.7500 - mae: 1122.9835\n",
      "Epoch 2: val_loss improved from 1278714.12500 to 1277692.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1263170.3750 - mae: 1122.7631 - val_loss: 1277692.0000 - val_mae: 1129.2765\n",
      "Epoch 3/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1260262.0000 - mae: 1121.4662\n",
      "Epoch 3: val_loss improved from 1277692.00000 to 1275631.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 1261665.6250 - mae: 1122.0934 - val_loss: 1275631.6250 - val_mae: 1128.3618\n",
      "Epoch 4/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1258770.1250 - mae: 1120.7970\n",
      "Epoch 4: val_loss improved from 1275631.62500 to 1271946.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 93ms/step - loss: 1258770.1250 - mae: 1120.7970 - val_loss: 1271946.3750 - val_mae: 1126.7198\n",
      "Epoch 5/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1254003.1250 - mae: 1118.6598\n",
      "Epoch 5: val_loss improved from 1271946.37500 to 1266406.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 93ms/step - loss: 1254018.0000 - mae: 1118.6595 - val_loss: 1266406.7500 - val_mae: 1124.2380\n",
      "Epoch 6/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1248009.2500 - mae: 1115.9374\n",
      "Epoch 6: val_loss improved from 1266406.75000 to 1258965.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 1247253.3750 - mae: 1115.5994 - val_loss: 1258965.0000 - val_mae: 1120.8876\n",
      "Epoch 7/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1238912.2500 - mae: 1111.7958\n",
      "Epoch 7: val_loss improved from 1258965.00000 to 1249841.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 1238555.6250 - mae: 1111.6426 - val_loss: 1249841.2500 - val_mae: 1116.7544\n",
      "Epoch 8/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1230002.7500 - mae: 1107.7322\n",
      "Epoch 8: val_loss improved from 1249841.25000 to 1239338.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1228202.3750 - mae: 1106.9015 - val_loss: 1239338.6250 - val_mae: 1111.9684\n",
      "Epoch 9/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1217193.7500 - mae: 1101.8492\n",
      "Epoch 9: val_loss improved from 1239338.62500 to 1227512.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1216418.1250 - mae: 1101.4739 - val_loss: 1227512.1250 - val_mae: 1106.5442\n",
      "Epoch 10/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1205179.2500 - mae: 1096.2646\n",
      "Epoch 10: val_loss improved from 1227512.12500 to 1214586.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1203353.6250 - mae: 1095.4094 - val_loss: 1214586.6250 - val_mae: 1100.5804\n",
      "Epoch 11/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1191103.3750 - mae: 1089.6624\n",
      "Epoch 11: val_loss improved from 1214586.62500 to 1200595.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 99ms/step - loss: 1189159.5000 - mae: 1088.7731 - val_loss: 1200595.1250 - val_mae: 1094.0734\n",
      "Epoch 12/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1174145.8750 - mae: 1081.6940\n",
      "Epoch 12: val_loss improved from 1200595.12500 to 1185634.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 104ms/step - loss: 1173969.8750 - mae: 1081.6178 - val_loss: 1185634.0000 - val_mae: 1087.0601\n",
      "Epoch 13/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1157831.7500 - mae: 1073.9424\n",
      "Epoch 13: val_loss improved from 1185634.00000 to 1169964.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1157831.7500 - mae: 1073.9424 - val_loss: 1169964.7500 - val_mae: 1079.6543\n",
      "Epoch 14/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1142987.5000 - mae: 1066.8210\n",
      "Epoch 14: val_loss improved from 1169964.75000 to 1153539.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1140939.7500 - mae: 1065.8318 - val_loss: 1153539.0000 - val_mae: 1071.8192\n",
      "Epoch 15/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1123280.1250 - mae: 1057.2721\n",
      "Epoch 15: val_loss improved from 1153539.00000 to 1136368.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1123280.1250 - mae: 1057.2721 - val_loss: 1136368.5000 - val_mae: 1063.5519\n",
      "Epoch 16/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1107393.2500 - mae: 1049.4355\n",
      "Epoch 16: val_loss improved from 1136368.50000 to 1118752.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1104984.1250 - mae: 1048.3140 - val_loss: 1118752.0000 - val_mae: 1054.9866\n",
      "Epoch 17/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1086916.8750 - mae: 1039.3317\n",
      "Epoch 17: val_loss improved from 1118752.00000 to 1100517.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1086099.1250 - mae: 1038.9490 - val_loss: 1100517.0000 - val_mae: 1046.0239\n",
      "Epoch 18/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1067285.1250 - mae: 1029.4890\n",
      "Epoch 18: val_loss improved from 1100517.00000 to 1081768.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 1066624.2500 - mae: 1029.1931 - val_loss: 1081768.2500 - val_mae: 1036.7054\n",
      "Epoch 19/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1047356.8750 - mae: 1019.4123\n",
      "Epoch 19: val_loss improved from 1081768.25000 to 1062615.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 1046759.2500 - mae: 1019.1195 - val_loss: 1062615.7500 - val_mae: 1027.0833\n",
      "Epoch 20/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1026868.4375 - mae: 1008.8632\n",
      "Epoch 20: val_loss improved from 1062615.75000 to 1043135.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 1026452.7500 - mae: 1008.6615 - val_loss: 1043135.3125 - val_mae: 1017.1812\n",
      "Epoch 21/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1005825.5000 - mae: 997.9421\n",
      "Epoch 21: val_loss improved from 1043135.31250 to 1023267.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1005825.5000 - mae: 997.9421 - val_loss: 1023267.6250 - val_mae: 1006.9537\n",
      "Epoch 22/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 989199.4375 - mae: 989.0121\n",
      "Epoch 22: val_loss improved from 1023267.62500 to 1003216.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 984865.5625 - mae: 986.8709 - val_loss: 1003216.4375 - val_mae: 996.4970\n",
      "Epoch 23/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 965514.0000 - mae: 976.4753\n",
      "Epoch 23: val_loss improved from 1003216.43750 to 982844.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 963719.0625 - mae: 975.5411 - val_loss: 982844.3125 - val_mae: 985.7239\n",
      "Epoch 24/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 943791.3750 - mae: 964.6982\n",
      "Epoch 24: val_loss improved from 982844.31250 to 962366.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 942419.4375 - mae: 963.9670 - val_loss: 962366.1875 - val_mae: 974.7560\n",
      "Epoch 25/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 921018.4375 - mae: 952.0596\n",
      "Epoch 25: val_loss improved from 962366.18750 to 941731.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 921040.6875 - mae: 952.1666 - val_loss: 941731.6875 - val_mae: 963.5413\n",
      "Epoch 26/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 901939.1875 - mae: 941.4469\n",
      "Epoch 26: val_loss improved from 941731.68750 to 921068.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 899526.9375 - mae: 940.0841 - val_loss: 921068.5625 - val_mae: 952.1650\n",
      "Epoch 27/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 877274.8125 - mae: 927.3561\n",
      "Epoch 27: val_loss improved from 921068.56250 to 900270.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 877965.1875 - mae: 927.8083 - val_loss: 900270.3750 - val_mae: 940.5148\n",
      "Epoch 28/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 855018.7500 - mae: 914.6236\n",
      "Epoch 28: val_loss improved from 900270.37500 to 879516.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 856464.8125 - mae: 915.3295 - val_loss: 879516.1250 - val_mae: 928.7373\n",
      "Epoch 29/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 835595.0000 - mae: 903.2403\n",
      "Epoch 29: val_loss improved from 879516.12500 to 858787.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 834972.8750 - mae: 902.7327 - val_loss: 858787.8750 - val_mae: 916.7698\n",
      "Epoch 30/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 812747.1250 - mae: 889.3898\n",
      "Epoch 30: val_loss improved from 858787.87500 to 838059.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 813594.8125 - mae: 889.9318 - val_loss: 838059.7500 - val_mae: 904.6118\n",
      "Epoch 31/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 797214.2500 - mae: 879.7482\n",
      "Epoch 31: val_loss improved from 838059.75000 to 817458.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 792234.0000 - mae: 876.9905 - val_loss: 817458.8125 - val_mae: 892.3456\n",
      "Epoch 32/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 772789.6250 - mae: 864.8122\n",
      "Epoch 32: val_loss improved from 817458.81250 to 796843.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 771002.9375 - mae: 863.8972 - val_loss: 796843.1250 - val_mae: 879.8232\n",
      "Epoch 33/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 751555.6875 - mae: 851.5663\n",
      "Epoch 33: val_loss improved from 796843.12500 to 776371.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 749914.3750 - mae: 850.5696 - val_loss: 776371.2500 - val_mae: 867.1871\n",
      "Epoch 34/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 727698.5000 - mae: 836.5110\n",
      "Epoch 34: val_loss improved from 776371.25000 to 756097.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 729076.3750 - mae: 837.2520 - val_loss: 756097.7500 - val_mae: 854.4477\n",
      "Epoch 35/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 709966.4375 - mae: 824.6143\n",
      "Epoch 35: val_loss improved from 756097.75000 to 736045.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 708499.7500 - mae: 823.8126 - val_loss: 736045.4375 - val_mae: 841.6263\n",
      "Epoch 36/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 687745.3125 - mae: 810.1980\n",
      "Epoch 36: val_loss improved from 736045.43750 to 716154.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 688142.8125 - mae: 810.2195 - val_loss: 716154.5625 - val_mae: 828.6449\n",
      "Epoch 37/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 669143.3125 - mae: 797.2441\n",
      "Epoch 37: val_loss improved from 716154.56250 to 696492.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 668049.1875 - mae: 796.6118 - val_loss: 696492.0625 - val_mae: 815.6389\n",
      "Epoch 38/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 649173.1250 - mae: 784.0201\n",
      "Epoch 38: val_loss improved from 696492.06250 to 676983.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 648230.1250 - mae: 782.8479 - val_loss: 676983.7500 - val_mae: 802.4291\n",
      "Epoch 39/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 629260.9375 - mae: 769.4333\n",
      "Epoch 39: val_loss improved from 676983.75000 to 657808.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 628724.1250 - mae: 769.0637 - val_loss: 657808.9375 - val_mae: 789.2195\n",
      "Epoch 40/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 613960.2500 - mae: 758.3351\n",
      "Epoch 40: val_loss improved from 657808.93750 to 638837.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 609569.5625 - mae: 755.3720 - val_loss: 638837.3750 - val_mae: 775.9209\n",
      "Epoch 41/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 593810.3750 - mae: 743.6504\n",
      "Epoch 41: val_loss improved from 638837.37500 to 620260.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 590733.4375 - mae: 741.6191 - val_loss: 620260.0000 - val_mae: 762.6157\n",
      "Epoch 42/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 563848.3125 - mae: 722.2694\n",
      "Epoch 42: val_loss improved from 620260.00000 to 601830.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 572264.2500 - mae: 727.7944 - val_loss: 601830.1875 - val_mae: 749.1392\n",
      "Epoch 43/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 557193.1250 - mae: 716.3517\n",
      "Epoch 43: val_loss improved from 601830.18750 to 583830.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 554119.6875 - mae: 713.9746 - val_loss: 583830.6875 - val_mae: 735.7741\n",
      "Epoch 44/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 533002.5000 - mae: 697.7581\n",
      "Epoch 44: val_loss improved from 583830.68750 to 566032.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 536360.5625 - mae: 700.1144 - val_loss: 566032.4375 - val_mae: 722.2727\n",
      "Epoch 45/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 516259.3438 - mae: 684.1668\n",
      "Epoch 45: val_loss improved from 566032.43750 to 548591.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 518966.1250 - mae: 686.2364 - val_loss: 548591.4375 - val_mae: 708.7998\n",
      "Epoch 46/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 504216.4375 - mae: 673.9151\n",
      "Epoch 46: val_loss improved from 548591.43750 to 531401.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 501949.2812 - mae: 672.4187 - val_loss: 531401.8125 - val_mae: 695.2444\n",
      "Epoch 47/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 485745.2812 - mae: 658.9716\n",
      "Epoch 47: val_loss improved from 531401.81250 to 514595.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 485321.6250 - mae: 658.5297 - val_loss: 514595.6875 - val_mae: 681.7236\n",
      "Epoch 48/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 469062.5000 - mae: 644.6376\n",
      "Epoch 48: val_loss improved from 514595.68750 to 498001.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 469062.5000 - mae: 644.6376 - val_loss: 498001.7188 - val_mae: 668.4259\n",
      "Epoch 49/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 453586.0000 - mae: 630.8607\n",
      "Epoch 49: val_loss improved from 498001.71875 to 481793.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 453207.1562 - mae: 630.9235 - val_loss: 481793.1875 - val_mae: 655.1231\n",
      "Epoch 50/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 440378.1250 - mae: 618.8000\n",
      "Epoch 50: val_loss improved from 481793.18750 to 465968.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 437736.8750 - mae: 617.2045 - val_loss: 465968.6875 - val_mae: 642.0705\n",
      "Epoch 51/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 422383.6250 - mae: 603.7183\n",
      "Epoch 51: val_loss improved from 465968.68750 to 450317.40625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 422635.2188 - mae: 603.6198 - val_loss: 450317.4062 - val_mae: 628.9052\n",
      "Epoch 52/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 409368.7812 - mae: 591.0034\n",
      "Epoch 52: val_loss improved from 450317.40625 to 435132.78125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 407914.2188 - mae: 590.1603 - val_loss: 435132.7812 - val_mae: 616.0266\n",
      "Epoch 53/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 392569.5938 - mae: 576.1194\n",
      "Epoch 53: val_loss improved from 435132.78125 to 420171.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 393536.6250 - mae: 576.8046 - val_loss: 420171.0938 - val_mae: 603.0873\n",
      "Epoch 54/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 379747.3125 - mae: 563.8759\n",
      "Epoch 54: val_loss improved from 420171.09375 to 405440.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 379552.1562 - mae: 563.6578 - val_loss: 405440.6875 - val_mae: 590.3550\n",
      "Epoch 55/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 364564.8125 - mae: 549.9902\n",
      "Epoch 55: val_loss improved from 405440.68750 to 391117.15625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 365900.1875 - mae: 550.8290 - val_loss: 391117.1562 - val_mae: 577.6805\n",
      "Epoch 56/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 354899.3125 - mae: 541.3371\n",
      "Epoch 56: val_loss improved from 391117.15625 to 377032.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 352620.0000 - mae: 538.2181 - val_loss: 377032.8125 - val_mae: 565.0199\n",
      "Epoch 57/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 338681.8125 - mae: 525.4398\n",
      "Epoch 57: val_loss improved from 377032.81250 to 363246.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 339627.1875 - mae: 525.8951 - val_loss: 363246.4688 - val_mae: 552.4550\n",
      "Epoch 58/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 327951.2812 - mae: 513.8793\n",
      "Epoch 58: val_loss improved from 363246.46875 to 349842.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 326988.8125 - mae: 513.8321 - val_loss: 349842.8125 - val_mae: 540.1010\n",
      "Epoch 59/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 314094.2500 - mae: 502.5338\n",
      "Epoch 59: val_loss improved from 349842.81250 to 336560.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 314633.5625 - mae: 501.8824 - val_loss: 336560.6250 - val_mae: 527.7948\n",
      "Epoch 60/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 308417.5000 - mae: 495.0275\n",
      "Epoch 60: val_loss improved from 336560.62500 to 323598.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 302584.4375 - mae: 490.3090 - val_loss: 323598.4375 - val_mae: 515.9283\n",
      "Epoch 61/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 293030.4688 - mae: 480.4980\n",
      "Epoch 61: val_loss improved from 323598.43750 to 310915.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 290839.2812 - mae: 478.7468 - val_loss: 310915.9375 - val_mae: 504.0085\n",
      "Epoch 62/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 281222.3438 - mae: 468.5493\n",
      "Epoch 62: val_loss improved from 310915.93750 to 298542.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 67ms/step - loss: 279381.1250 - mae: 467.3102 - val_loss: 298542.8125 - val_mae: 492.3754\n",
      "Epoch 63/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 267551.3125 - mae: 455.4379\n",
      "Epoch 63: val_loss improved from 298542.81250 to 286330.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 268139.3438 - mae: 455.9386 - val_loss: 286330.7188 - val_mae: 480.7284\n",
      "Epoch 64/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 256823.0625 - mae: 445.3061\n",
      "Epoch 64: val_loss improved from 286330.71875 to 274198.40625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 257169.0469 - mae: 444.7099 - val_loss: 274198.4062 - val_mae: 469.0767\n",
      "Epoch 65/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 249044.0000 - mae: 436.4563\n",
      "Epoch 65: val_loss improved from 274198.40625 to 262597.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 246453.7969 - mae: 433.7765 - val_loss: 262597.9375 - val_mae: 457.7300\n",
      "Epoch 66/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 237270.4062 - mae: 424.5061\n",
      "Epoch 66: val_loss improved from 262597.93750 to 251161.35938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 236000.6562 - mae: 422.7176 - val_loss: 251161.3594 - val_mae: 446.4456\n",
      "Epoch 67/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 225987.7031 - mae: 411.7994\n",
      "Epoch 67: val_loss improved from 251161.35938 to 239917.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 225800.4219 - mae: 411.8792 - val_loss: 239917.3125 - val_mae: 435.1895\n",
      "Epoch 68/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 215861.5156 - mae: 401.7936\n",
      "Epoch 68: val_loss improved from 239917.31250 to 228834.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 215811.5156 - mae: 401.1822 - val_loss: 228834.1250 - val_mae: 423.8974\n",
      "Epoch 69/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 207746.2812 - mae: 391.0057\n",
      "Epoch 69: val_loss improved from 228834.12500 to 218219.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 206100.9531 - mae: 390.7957 - val_loss: 218219.1250 - val_mae: 412.9474\n",
      "Epoch 70/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 196594.8750 - mae: 380.4125\n",
      "Epoch 70: val_loss improved from 218219.12500 to 207769.79688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 196594.8750 - mae: 380.4125 - val_loss: 207769.7969 - val_mae: 401.9492\n",
      "Epoch 71/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 188807.2500 - mae: 370.5912\n",
      "Epoch 71: val_loss improved from 207769.79688 to 197527.92188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 187368.0469 - mae: 370.0171 - val_loss: 197527.9219 - val_mae: 391.0894\n",
      "Epoch 72/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 180929.9375 - mae: 363.2575\n",
      "Epoch 72: val_loss improved from 197527.92188 to 187595.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 178425.8906 - mae: 359.9381 - val_loss: 187595.5000 - val_mae: 380.3315\n",
      "Epoch 73/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 173479.1875 - mae: 353.9247\n",
      "Epoch 73: val_loss improved from 187595.50000 to 178009.96875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 169752.5000 - mae: 349.9443 - val_loss: 178009.9688 - val_mae: 369.7278\n",
      "Epoch 74/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 161488.0781 - mae: 340.1568\n",
      "Epoch 74: val_loss improved from 178009.96875 to 168530.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 161293.6719 - mae: 339.8882 - val_loss: 168530.5312 - val_mae: 359.0636\n",
      "Epoch 75/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 153001.6094 - mae: 329.6299\n",
      "Epoch 75: val_loss improved from 168530.53125 to 159615.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 153115.1562 - mae: 330.0605 - val_loss: 159615.7188 - val_mae: 348.7584\n",
      "Epoch 76/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 145955.2656 - mae: 320.6634\n",
      "Epoch 76: val_loss improved from 159615.71875 to 150796.79688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 145196.9531 - mae: 320.1934 - val_loss: 150796.7969 - val_mae: 338.3327\n",
      "Epoch 77/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 138939.0469 - mae: 311.6746\n",
      "Epoch 77: val_loss improved from 150796.79688 to 142359.20312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 137576.9844 - mae: 310.5323 - val_loss: 142359.2031 - val_mae: 328.0784\n",
      "Epoch 78/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 130159.1328 - mae: 300.9441\n",
      "Epoch 78: val_loss improved from 142359.20312 to 134053.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 130159.1328 - mae: 300.9441 - val_loss: 134053.2188 - val_mae: 317.6920\n",
      "Epoch 79/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 122886.8047 - mae: 291.3285\n",
      "Epoch 79: val_loss improved from 134053.21875 to 126190.46094, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 123032.9688 - mae: 291.5299 - val_loss: 126190.4609 - val_mae: 307.5311\n",
      "Epoch 80/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 118025.3828 - mae: 284.1367\n",
      "Epoch 80: val_loss improved from 126190.46094 to 118650.99219, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 116213.4922 - mae: 282.2312 - val_loss: 118650.9922 - val_mae: 297.5014\n",
      "Epoch 81/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 109787.3750 - mae: 273.2259\n",
      "Epoch 81: val_loss improved from 118650.99219 to 111397.91406, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 109679.2109 - mae: 273.1284 - val_loss: 111397.9141 - val_mae: 287.6443\n",
      "Epoch 82/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 105183.5859 - mae: 266.5493\n",
      "Epoch 82: val_loss improved from 111397.91406 to 104474.41406, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 103404.7812 - mae: 264.0966 - val_loss: 104474.4141 - val_mae: 277.8864\n",
      "Epoch 83/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 96608.4062 - mae: 254.1467 \n",
      "Epoch 83: val_loss improved from 104474.41406 to 97902.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 97358.3203 - mae: 255.2805 - val_loss: 97902.4375 - val_mae: 268.4323\n",
      "Epoch 84/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 91439.3750 - mae: 246.9369\n",
      "Epoch 84: val_loss improved from 97902.43750 to 91523.50781, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 91578.3438 - mae: 246.4534 - val_loss: 91523.5078 - val_mae: 258.9297\n",
      "Epoch 85/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 86986.8438 - mae: 237.3753\n",
      "Epoch 85: val_loss improved from 91523.50781 to 85627.67969, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 86104.4453 - mae: 237.9961 - val_loss: 85627.6797 - val_mae: 249.8111\n",
      "Epoch 86/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 80917.7969 - mae: 229.5837\n",
      "Epoch 86: val_loss improved from 85627.67969 to 79922.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 80827.8516 - mae: 229.4444 - val_loss: 79922.0938 - val_mae: 240.5878\n",
      "Epoch 87/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 75605.9609 - mae: 220.6256\n",
      "Epoch 87: val_loss improved from 79922.09375 to 74653.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 109ms/step - loss: 75798.6641 - mae: 221.3104 - val_loss: 74653.0625 - val_mae: 231.7135\n",
      "Epoch 88/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 70961.2500 - mae: 212.7686\n",
      "Epoch 88: val_loss improved from 74653.06250 to 69600.90625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 70975.7031 - mae: 213.2252 - val_loss: 69600.9062 - val_mae: 222.9823\n",
      "Epoch 89/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 67117.6172 - mae: 206.0875\n",
      "Epoch 89: val_loss improved from 69600.90625 to 64758.29688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 66320.7266 - mae: 205.2139 - val_loss: 64758.2969 - val_mae: 214.3347\n",
      "Epoch 90/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 62145.4961 - mae: 197.6388\n",
      "Epoch 90: val_loss improved from 64758.29688 to 60257.35547, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 61844.7930 - mae: 197.1885 - val_loss: 60257.3555 - val_mae: 205.9381\n",
      "Epoch 91/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 57828.1406 - mae: 189.6650\n",
      "Epoch 91: val_loss improved from 60257.35547 to 56070.35938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 57828.1406 - mae: 189.6650 - val_loss: 56070.3594 - val_mae: 197.7410\n",
      "Epoch 92/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 53522.6602 - mae: 181.1906\n",
      "Epoch 92: val_loss improved from 56070.35938 to 52107.05859, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 53945.7969 - mae: 182.0331 - val_loss: 52107.0586 - val_mae: 189.6167\n",
      "Epoch 93/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 51992.3320 - mae: 176.1174\n",
      "Epoch 93: val_loss improved from 52107.05859 to 48387.24609, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 50407.6484 - mae: 174.8673 - val_loss: 48387.2461 - val_mae: 181.6873\n",
      "Epoch 94/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 47230.0039 - mae: 167.9478\n",
      "Epoch 94: val_loss improved from 48387.24609 to 44951.10547, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 47118.9023 - mae: 167.8904 - val_loss: 44951.1055 - val_mae: 173.9191\n",
      "Epoch 95/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 44226.7383 - mae: 161.3084\n",
      "Epoch 95: val_loss improved from 44951.10547 to 41750.67188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 44095.8828 - mae: 161.0880 - val_loss: 41750.6719 - val_mae: 166.4470\n",
      "Epoch 96/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 41461.4297 - mae: 154.4694\n",
      "Epoch 96: val_loss improved from 41750.67188 to 38787.21484, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 41271.7930 - mae: 154.4354 - val_loss: 38787.2148 - val_mae: 159.2518\n",
      "Epoch 97/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 35246.4219 - mae: 145.6644\n",
      "Epoch 97: val_loss improved from 38787.21484 to 35975.88672, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 38636.5039 - mae: 147.7466 - val_loss: 35975.8867 - val_mae: 152.0205\n",
      "Epoch 98/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 36528.9961 - mae: 141.8721\n",
      "Epoch 98: val_loss improved from 35975.88672 to 33418.29688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 36151.4531 - mae: 141.4949 - val_loss: 33418.2969 - val_mae: 145.1829\n",
      "Epoch 99/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 31838.4355 - mae: 135.2676\n",
      "Epoch 99: val_loss improved from 33418.29688 to 30918.56641, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 33818.8750 - mae: 135.2474 - val_loss: 30918.5664 - val_mae: 138.1682\n",
      "Epoch 100/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 30794.3418 - mae: 128.3270\n",
      "Epoch 100: val_loss improved from 30918.56641 to 28666.64258, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 31602.9941 - mae: 129.0937 - val_loss: 28666.6426 - val_mae: 131.6079\n",
      "Epoch 101/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 30068.8770 - mae: 123.3286\n",
      "Epoch 101: val_loss improved from 28666.64258 to 26526.13672, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 29467.9160 - mae: 123.1085 - val_loss: 26526.1367 - val_mae: 125.0624\n",
      "Epoch 102/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 27972.5820 - mae: 117.7033\n",
      "Epoch 102: val_loss improved from 26526.13672 to 24533.24609, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 27232.8613 - mae: 116.9027 - val_loss: 24533.2461 - val_mae: 118.4877\n",
      "Epoch 103/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 25490.6406 - mae: 111.6453\n",
      "Epoch 103: val_loss improved from 24533.24609 to 22778.51758, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 24987.9180 - mae: 110.8067 - val_loss: 22778.5176 - val_mae: 112.3232\n",
      "Epoch 104/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 23166.4785 - mae: 104.9726\n",
      "Epoch 104: val_loss improved from 22778.51758 to 21187.09180, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 22902.8477 - mae: 104.8460 - val_loss: 21187.0918 - val_mae: 106.3606\n",
      "Epoch 105/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 20977.1562 - mae: 98.8130 \n",
      "Epoch 105: val_loss improved from 21187.09180 to 19767.93359, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 21125.0566 - mae: 99.3953 - val_loss: 19767.9336 - val_mae: 100.6513\n",
      "Epoch 106/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 19735.4062 - mae: 94.1185\n",
      "Epoch 106: val_loss improved from 19767.93359 to 18452.20898, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 19532.5176 - mae: 93.9135 - val_loss: 18452.2090 - val_mae: 95.3707\n",
      "Epoch 107/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 18060.9375 - mae: 88.6366\n",
      "Epoch 107: val_loss improved from 18452.20898 to 17261.13672, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 18014.3906 - mae: 88.6915 - val_loss: 17261.1367 - val_mae: 90.4683\n",
      "Epoch 108/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 16777.8535 - mae: 84.5948\n",
      "Epoch 108: val_loss improved from 17261.13672 to 16181.10449, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 16544.5527 - mae: 84.0918 - val_loss: 16181.1045 - val_mae: 85.9343\n",
      "Epoch 109/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 15422.6396 - mae: 79.5022\n",
      "Epoch 109: val_loss improved from 16181.10449 to 15207.85254, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 15160.7305 - mae: 79.6647 - val_loss: 15207.8525 - val_mae: 81.4977\n",
      "Epoch 110/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 14474.5537 - mae: 76.0903\n",
      "Epoch 110: val_loss improved from 15207.85254 to 14326.80176, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 13913.0195 - mae: 75.3357 - val_loss: 14326.8018 - val_mae: 77.2043\n",
      "Epoch 111/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 12906.4102 - mae: 71.5279\n",
      "Epoch 111: val_loss improved from 14326.80176 to 13540.31445, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 12810.8291 - mae: 71.4160 - val_loss: 13540.3145 - val_mae: 73.3207\n",
      "Epoch 112/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 11870.8887 - mae: 67.6572\n",
      "Epoch 112: val_loss improved from 13540.31445 to 12842.00684, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 11870.8887 - mae: 67.6572 - val_loss: 12842.0068 - val_mae: 69.6154\n",
      "Epoch 113/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 11206.0186 - mae: 64.1813\n",
      "Epoch 113: val_loss improved from 12842.00684 to 12229.70703, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 11076.1162 - mae: 64.0838 - val_loss: 12229.7070 - val_mae: 66.0988\n",
      "Epoch 114/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 10357.0264 - mae: 62.0515\n",
      "Epoch 114: val_loss improved from 12229.70703 to 11689.57422, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 10390.5098 - mae: 60.9600 - val_loss: 11689.5742 - val_mae: 63.1191\n",
      "Epoch 115/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 10101.1895 - mae: 58.0898\n",
      "Epoch 115: val_loss improved from 11689.57422 to 11214.98633, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 9815.6035 - mae: 57.9645 - val_loss: 11214.9863 - val_mae: 60.3053\n",
      "Epoch 116/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 9379.9883 - mae: 55.3493\n",
      "Epoch 116: val_loss improved from 11214.98633 to 10782.85059, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 9263.0283 - mae: 55.2710 - val_loss: 10782.8506 - val_mae: 57.6677\n",
      "Epoch 117/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 8936.5381 - mae: 52.8171\n",
      "Epoch 117: val_loss improved from 10782.85059 to 10390.88672, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 8759.3662 - mae: 52.7957 - val_loss: 10390.8867 - val_mae: 55.2583\n",
      "Epoch 118/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 8396.7568 - mae: 50.1563\n",
      "Epoch 118: val_loss improved from 10390.88672 to 10048.73926, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 8293.1270 - mae: 50.1189 - val_loss: 10048.7393 - val_mae: 52.9854\n",
      "Epoch 119/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 8263.1162 - mae: 48.3750\n",
      "Epoch 119: val_loss improved from 10048.73926 to 9713.36816, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 7840.0547 - mae: 48.2050 - val_loss: 9713.3682 - val_mae: 51.0547\n",
      "Epoch 120/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 7149.3286 - mae: 46.1379\n",
      "Epoch 120: val_loss improved from 9713.36816 to 9401.20117, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 7419.6030 - mae: 46.1107 - val_loss: 9401.2012 - val_mae: 49.0941\n",
      "Epoch 121/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 7010.1841 - mae: 44.5272\n",
      "Epoch 121: val_loss improved from 9401.20117 to 9140.95996, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 7010.1841 - mae: 44.5272 - val_loss: 9140.9600 - val_mae: 47.6682\n",
      "Epoch 122/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 7110.1504 - mae: 43.4995\n",
      "Epoch 122: val_loss improved from 9140.95996 to 8878.29199, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 6661.7236 - mae: 42.5877 - val_loss: 8878.2920 - val_mae: 45.9312\n",
      "Epoch 123/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 6145.8521 - mae: 40.7307\n",
      "Epoch 123: val_loss improved from 8878.29199 to 8641.46484, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 6349.1235 - mae: 40.8355 - val_loss: 8641.4648 - val_mae: 44.2436\n",
      "Epoch 124/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 6067.4995 - mae: 39.3920\n",
      "Epoch 124: val_loss improved from 8641.46484 to 8409.47656, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 6056.4395 - mae: 39.3459 - val_loss: 8409.4766 - val_mae: 42.8230\n",
      "Epoch 125/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 5900.8867 - mae: 37.9843\n",
      "Epoch 125: val_loss improved from 8409.47656 to 8198.49707, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 98ms/step - loss: 5786.4463 - mae: 37.8403 - val_loss: 8198.4971 - val_mae: 41.6894\n",
      "Epoch 126/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 5710.1431 - mae: 37.0060\n",
      "Epoch 126: val_loss improved from 8198.49707 to 7994.36035, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 93ms/step - loss: 5536.9048 - mae: 36.7351 - val_loss: 7994.3604 - val_mae: 40.5664\n",
      "Epoch 127/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 5024.9370 - mae: 34.8136\n",
      "Epoch 127: val_loss improved from 7994.36035 to 7796.02051, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 5304.1943 - mae: 35.3255 - val_loss: 7796.0205 - val_mae: 39.2870\n",
      "Epoch 128/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 5170.6753 - mae: 34.5313\n",
      "Epoch 128: val_loss improved from 7796.02051 to 7602.73340, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 5071.0596 - mae: 34.3523 - val_loss: 7602.7334 - val_mae: 38.3485\n",
      "Epoch 129/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 4896.7827 - mae: 33.5103\n",
      "Epoch 129: val_loss improved from 7602.73340 to 7408.49756, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 4840.1240 - mae: 33.2983 - val_loss: 7408.4976 - val_mae: 37.2628\n",
      "Epoch 130/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 3832.4998 - mae: 31.5257\n",
      "Epoch 130: val_loss improved from 7408.49756 to 7233.19141, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 4615.4253 - mae: 32.1784 - val_loss: 7233.1914 - val_mae: 36.3253\n",
      "Epoch 131/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 3712.1853 - mae: 30.5313\n",
      "Epoch 131: val_loss improved from 7233.19141 to 7065.35547, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 4377.5664 - mae: 31.4229 - val_loss: 7065.3555 - val_mae: 35.7294\n",
      "Epoch 132/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 4494.6162 - mae: 31.4737\n",
      "Epoch 132: val_loss improved from 7065.35547 to 6880.68262, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 4180.2217 - mae: 30.5708 - val_loss: 6880.6826 - val_mae: 34.9783\n",
      "Epoch 133/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 3541.4485 - mae: 29.1910\n",
      "Epoch 133: val_loss improved from 6880.68262 to 6716.34961, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 4003.2852 - mae: 29.8012 - val_loss: 6716.3496 - val_mae: 34.0962\n",
      "Epoch 134/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 3872.0398 - mae: 29.1093\n",
      "Epoch 134: val_loss improved from 6716.34961 to 6538.68555, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 3833.4780 - mae: 29.0005 - val_loss: 6538.6855 - val_mae: 33.3904\n",
      "Epoch 135/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 3736.6697 - mae: 28.2563\n",
      "Epoch 135: val_loss improved from 6538.68555 to 6382.50244, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 3670.9512 - mae: 28.1424 - val_loss: 6382.5024 - val_mae: 32.7593\n",
      "Epoch 136/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 3494.6250 - mae: 27.6724\n",
      "Epoch 136: val_loss improved from 6382.50244 to 6217.66943, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 3521.4297 - mae: 27.7571 - val_loss: 6217.6694 - val_mae: 32.1872\n",
      "Epoch 137/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 3444.2720 - mae: 27.3094\n",
      "Epoch 137: val_loss improved from 6217.66943 to 6055.32178, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 3373.6655 - mae: 27.0876 - val_loss: 6055.3218 - val_mae: 31.6712\n",
      "Epoch 138/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 3239.7532 - mae: 26.6194\n",
      "Epoch 138: val_loss improved from 6055.32178 to 5888.59961, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 3243.3147 - mae: 26.6750 - val_loss: 5888.5996 - val_mae: 31.0705\n",
      "Epoch 139/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 3098.0681 - mae: 25.6418\n",
      "Epoch 139: val_loss improved from 5888.59961 to 5732.72314, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 3127.3174 - mae: 25.9262 - val_loss: 5732.7231 - val_mae: 30.4919\n",
      "Epoch 140/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 2953.3174 - mae: 25.0319\n",
      "Epoch 140: val_loss improved from 5732.72314 to 5599.15918, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 3024.8396 - mae: 25.4087 - val_loss: 5599.1592 - val_mae: 30.0400\n",
      "Epoch 141/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 3076.3953 - mae: 25.4659\n",
      "Epoch 141: val_loss improved from 5599.15918 to 5420.25586, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 2925.3235 - mae: 25.0104 - val_loss: 5420.2559 - val_mae: 29.5579\n",
      "Epoch 142/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 2891.7632 - mae: 24.1817\n",
      "Epoch 142: val_loss improved from 5420.25586 to 5296.54639, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 2838.4714 - mae: 24.2880 - val_loss: 5296.5464 - val_mae: 29.0885\n",
      "Epoch 143/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 2817.2034 - mae: 24.2687\n",
      "Epoch 143: val_loss improved from 5296.54639 to 5139.45801, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 2745.4907 - mae: 24.2249 - val_loss: 5139.4580 - val_mae: 28.8239\n",
      "Epoch 144/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 2712.1299 - mae: 23.9391\n",
      "Epoch 144: val_loss improved from 5139.45801 to 5009.30322, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 2664.7183 - mae: 23.7161 - val_loss: 5009.3032 - val_mae: 28.3221\n",
      "Epoch 145/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 2227.7007 - mae: 22.9369\n",
      "Epoch 145: val_loss improved from 5009.30322 to 4876.26660, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 2582.4802 - mae: 23.3429 - val_loss: 4876.2666 - val_mae: 27.8384\n",
      "Epoch 146/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 2626.8430 - mae: 23.9898\n",
      "Epoch 146: val_loss improved from 4876.26660 to 4763.18652, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 2474.8198 - mae: 23.5697 - val_loss: 4763.1865 - val_mae: 28.1846\n",
      "Epoch 147/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 2450.7649 - mae: 23.3032\n",
      "Epoch 147: val_loss improved from 4763.18652 to 4621.44043, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 2405.4695 - mae: 23.1532 - val_loss: 4621.4404 - val_mae: 27.4783\n",
      "Epoch 148/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 2363.3262 - mae: 22.7155\n",
      "Epoch 148: val_loss improved from 4621.44043 to 4477.46484, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 2337.2769 - mae: 22.6844 - val_loss: 4477.4648 - val_mae: 27.0093\n",
      "Epoch 149/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 2407.6169 - mae: 22.9270\n",
      "Epoch 149: val_loss improved from 4477.46484 to 4350.99316, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 2270.7754 - mae: 22.3120 - val_loss: 4350.9932 - val_mae: 26.5010\n",
      "Epoch 150/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 2246.5984 - mae: 21.6033\n",
      "Epoch 150: val_loss improved from 4350.99316 to 4213.44727, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 2205.9912 - mae: 21.8731 - val_loss: 4213.4473 - val_mae: 26.0382\n",
      "Epoch 151/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 2156.5110 - mae: 21.6015\n",
      "Epoch 151: val_loss improved from 4213.44727 to 4055.12720, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 2140.2668 - mae: 21.5825 - val_loss: 4055.1272 - val_mae: 25.6777\n",
      "Epoch 152/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1986.8234 - mae: 21.0421\n",
      "Epoch 152: val_loss improved from 4055.12720 to 3896.35181, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 2076.0923 - mae: 21.2171 - val_loss: 3896.3518 - val_mae: 25.1277\n",
      "Epoch 153/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 2088.6038 - mae: 21.1949\n",
      "Epoch 153: val_loss improved from 3896.35181 to 3751.59473, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 2013.4198 - mae: 20.9759 - val_loss: 3751.5947 - val_mae: 24.8194\n",
      "Epoch 154/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1954.8563 - mae: 20.7548\n",
      "Epoch 154: val_loss improved from 3751.59473 to 3596.64453, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1951.3143 - mae: 20.7347 - val_loss: 3596.6445 - val_mae: 24.4112\n",
      "Epoch 155/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1588.7687 - mae: 19.6730\n",
      "Epoch 155: val_loss improved from 3596.64453 to 3446.41895, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1890.6450 - mae: 20.3719 - val_loss: 3446.4189 - val_mae: 23.8686\n",
      "Epoch 156/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1769.0477 - mae: 19.6205\n",
      "Epoch 156: val_loss improved from 3446.41895 to 3308.64429, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1831.3068 - mae: 20.1530 - val_loss: 3308.6443 - val_mae: 23.5457\n",
      "Epoch 157/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1722.7581 - mae: 19.7137\n",
      "Epoch 157: val_loss improved from 3308.64429 to 3163.13550, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1772.2120 - mae: 19.9387 - val_loss: 3163.1355 - val_mae: 23.1149\n",
      "Epoch 158/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1744.8060 - mae: 19.6747\n",
      "Epoch 158: val_loss improved from 3163.13550 to 3026.38403, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1712.0115 - mae: 19.6009 - val_loss: 3026.3840 - val_mae: 22.7906\n",
      "Epoch 159/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1460.0913 - mae: 18.9433\n",
      "Epoch 159: val_loss improved from 3026.38403 to 2883.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1657.4838 - mae: 19.3154 - val_loss: 2883.8750 - val_mae: 22.2997\n",
      "Epoch 160/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1620.8824 - mae: 19.1473\n",
      "Epoch 160: val_loss improved from 2883.87500 to 2765.19849, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1604.4312 - mae: 19.0762 - val_loss: 2765.1985 - val_mae: 21.9320\n",
      "Epoch 161/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1554.7417 - mae: 18.9454\n",
      "Epoch 161: val_loss improved from 2765.19849 to 2653.04858, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 1554.7417 - mae: 18.9454 - val_loss: 2653.0486 - val_mae: 21.5840\n",
      "Epoch 162/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1531.9916 - mae: 18.6321\n",
      "Epoch 162: val_loss improved from 2653.04858 to 2519.96924, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1501.7628 - mae: 18.5738 - val_loss: 2519.9692 - val_mae: 21.1187\n",
      "Epoch 163/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 1525.4543 - mae: 18.2702\n",
      "Epoch 163: val_loss improved from 2519.96924 to 2418.44629, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1455.0189 - mae: 18.3055 - val_loss: 2418.4463 - val_mae: 20.7655\n",
      "Epoch 164/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1407.4250 - mae: 18.0538\n",
      "Epoch 164: val_loss improved from 2418.44629 to 2320.97241, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 1412.0865 - mae: 18.1351 - val_loss: 2320.9724 - val_mae: 20.3568\n",
      "Epoch 165/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1443.2124 - mae: 18.0491\n",
      "Epoch 165: val_loss improved from 2320.97241 to 2228.08325, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 1365.7405 - mae: 17.7856 - val_loss: 2228.0833 - val_mae: 19.9972\n",
      "Epoch 166/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1118.7090 - mae: 17.0909\n",
      "Epoch 166: val_loss improved from 2228.08325 to 2136.91724, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1324.4987 - mae: 17.4886 - val_loss: 2136.9172 - val_mae: 19.6125\n",
      "Epoch 167/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1221.5504 - mae: 16.9976\n",
      "Epoch 167: val_loss improved from 2136.91724 to 2044.42542, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 1279.9856 - mae: 17.2872 - val_loss: 2044.4254 - val_mae: 19.3064\n",
      "Epoch 168/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1266.3947 - mae: 17.1257\n",
      "Epoch 168: val_loss improved from 2044.42542 to 1968.32727, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1240.8384 - mae: 17.0941 - val_loss: 1968.3273 - val_mae: 19.0151\n",
      "Epoch 169/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1054.7159 - mae: 16.3220\n",
      "Epoch 169: val_loss improved from 1968.32727 to 1889.80835, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1199.5721 - mae: 16.7420 - val_loss: 1889.8083 - val_mae: 18.6063\n",
      "Epoch 170/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1178.0997 - mae: 16.5317\n",
      "Epoch 170: val_loss improved from 1889.80835 to 1807.43713, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 1161.6786 - mae: 16.4630 - val_loss: 1807.4371 - val_mae: 18.3504\n",
      "Epoch 171/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1180.4794 - mae: 16.4250\n",
      "Epoch 171: val_loss improved from 1807.43713 to 1739.56335, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1121.5759 - mae: 16.2012 - val_loss: 1739.5634 - val_mae: 18.0705\n",
      "Epoch 172/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1104.4774 - mae: 15.9824\n",
      "Epoch 172: val_loss improved from 1739.56335 to 1673.13855, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1084.8649 - mae: 16.0076 - val_loss: 1673.1385 - val_mae: 17.7960\n",
      "Epoch 173/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1006.2943 - mae: 15.6210\n",
      "Epoch 173: val_loss improved from 1673.13855 to 1604.45898, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1048.3145 - mae: 15.7313 - val_loss: 1604.4590 - val_mae: 17.4907\n",
      "Epoch 174/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 929.5015 - mae: 15.1982 \n",
      "Epoch 174: val_loss improved from 1604.45898 to 1538.71631, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1014.7438 - mae: 15.4478 - val_loss: 1538.7163 - val_mae: 17.2155\n",
      "Epoch 175/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1016.5341 - mae: 15.3404\n",
      "Epoch 175: val_loss improved from 1538.71631 to 1483.42114, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 983.1469 - mae: 15.2434 - val_loss: 1483.4211 - val_mae: 17.0314\n",
      "Epoch 176/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 945.4551 - mae: 15.1483 \n",
      "Epoch 176: val_loss improved from 1483.42114 to 1422.81812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 945.4551 - mae: 15.1483 - val_loss: 1422.8181 - val_mae: 16.7777\n",
      "Epoch 177/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 914.5677 - mae: 14.8639\n",
      "Epoch 177: val_loss improved from 1422.81812 to 1368.93835, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 914.5677 - mae: 14.8639 - val_loss: 1368.9384 - val_mae: 16.5204\n",
      "Epoch 178/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 883.1859 - mae: 14.6288\n",
      "Epoch 178: val_loss improved from 1368.93835 to 1316.17615, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 883.1859 - mae: 14.6288 - val_loss: 1316.1761 - val_mae: 16.2755\n",
      "Epoch 179/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 876.2055 - mae: 14.4102\n",
      "Epoch 179: val_loss improved from 1316.17615 to 1274.01147, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 853.1171 - mae: 14.3613 - val_loss: 1274.0115 - val_mae: 16.0731\n",
      "Epoch 180/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 826.0097 - mae: 14.3033\n",
      "Epoch 180: val_loss improved from 1274.01147 to 1222.12280, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 826.0097 - mae: 14.3033 - val_loss: 1222.1228 - val_mae: 15.8574\n",
      "Epoch 181/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 776.0367 - mae: 14.0289\n",
      "Epoch 181: val_loss improved from 1222.12280 to 1180.35388, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 796.0837 - mae: 14.0077 - val_loss: 1180.3539 - val_mae: 15.6235\n",
      "Epoch 182/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 554.0931 - mae: 13.1083\n",
      "Epoch 182: val_loss improved from 1180.35388 to 1131.82581, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 771.6757 - mae: 13.7382 - val_loss: 1131.8258 - val_mae: 15.3884\n",
      "Epoch 183/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 765.3546 - mae: 13.6728\n",
      "Epoch 183: val_loss improved from 1131.82581 to 1096.06763, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 742.3367 - mae: 13.6230 - val_loss: 1096.0676 - val_mae: 15.2271\n",
      "Epoch 184/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 747.0631 - mae: 13.6512\n",
      "Epoch 184: val_loss improved from 1096.06763 to 1056.15674, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 717.4429 - mae: 13.5059 - val_loss: 1056.1567 - val_mae: 15.0313\n",
      "Epoch 185/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 693.2869 - mae: 13.2820\n",
      "Epoch 185: val_loss improved from 1056.15674 to 1021.48529, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 693.2869 - mae: 13.2820 - val_loss: 1021.4853 - val_mae: 14.8574\n",
      "Epoch 186/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 727.5409 - mae: 13.4093\n",
      "Epoch 186: val_loss improved from 1021.48529 to 983.67090, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 666.9766 - mae: 13.1389 - val_loss: 983.6709 - val_mae: 14.6441\n",
      "Epoch 187/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 680.6830 - mae: 13.1597\n",
      "Epoch 187: val_loss improved from 983.67090 to 952.27039, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 643.1782 - mae: 13.0040 - val_loss: 952.2704 - val_mae: 14.4486\n",
      "Epoch 188/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 623.5446 - mae: 12.7534\n",
      "Epoch 188: val_loss improved from 952.27039 to 921.97491, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 620.3864 - mae: 12.7462 - val_loss: 921.9749 - val_mae: 14.2619\n",
      "Epoch 189/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 631.6077 - mae: 12.8229\n",
      "Epoch 189: val_loss improved from 921.97491 to 892.20422, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 598.7497 - mae: 12.6458 - val_loss: 892.2042 - val_mae: 14.1030\n",
      "Epoch 190/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 599.3954 - mae: 12.5816\n",
      "Epoch 190: val_loss improved from 892.20422 to 858.68671, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 576.5284 - mae: 12.4851 - val_loss: 858.6867 - val_mae: 13.8721\n",
      "Epoch 191/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 531.2785 - mae: 12.1348\n",
      "Epoch 191: val_loss improved from 858.68671 to 832.51318, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 556.8671 - mae: 12.2398 - val_loss: 832.5132 - val_mae: 13.6725\n",
      "Epoch 192/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 538.7039 - mae: 12.1282\n",
      "Epoch 192: val_loss improved from 832.51318 to 806.18005, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 534.5627 - mae: 12.1067 - val_loss: 806.1801 - val_mae: 13.5465\n",
      "Epoch 193/200\n",
      "81/98 [=======================>......] - ETA: 0s - loss: 576.7845 - mae: 12.2839\n",
      "Epoch 193: val_loss improved from 806.18005 to 776.49524, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 516.0921 - mae: 12.0074 - val_loss: 776.4952 - val_mae: 13.3399\n",
      "Epoch 194/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 502.9461 - mae: 11.8423\n",
      "Epoch 194: val_loss improved from 776.49524 to 754.06903, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 498.6323 - mae: 11.8095 - val_loss: 754.0690 - val_mae: 13.1773\n",
      "Epoch 195/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 391.6849 - mae: 11.3091\n",
      "Epoch 195: val_loss improved from 754.06903 to 731.01501, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 481.0146 - mae: 11.6317 - val_loss: 731.0150 - val_mae: 13.0331\n",
      "Epoch 196/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 487.0800 - mae: 11.7265\n",
      "Epoch 196: val_loss improved from 731.01501 to 702.65454, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 463.3496 - mae: 11.5677 - val_loss: 702.6545 - val_mae: 12.8810\n",
      "Epoch 197/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 451.4938 - mae: 11.3786\n",
      "Epoch 197: val_loss improved from 702.65454 to 676.65039, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 445.4693 - mae: 11.3506 - val_loss: 676.6504 - val_mae: 12.6570\n",
      "Epoch 198/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 430.0512 - mae: 11.1829\n",
      "Epoch 198: val_loss improved from 676.65039 to 655.90076, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 430.0512 - mae: 11.1829 - val_loss: 655.9008 - val_mae: 12.4973\n",
      "Epoch 199/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 454.6763 - mae: 11.2686\n",
      "Epoch 199: val_loss improved from 655.90076 to 639.83948, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 415.8453 - mae: 11.0633 - val_loss: 639.8395 - val_mae: 12.4157\n",
      "Epoch 200/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 403.9549 - mae: 10.9837\n",
      "Epoch 200: val_loss improved from 639.83948 to 621.23560, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 403.4169 - mae: 10.9838 - val_loss: 621.2356 - val_mae: 12.2702\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일: 모델학습을 위한 학습과정 설정단계\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "# print(\"\\n Accuracy: %.4f\" % model.evaluate(x_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a1f169ad30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_feature)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(test_label, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7adfe9f3775c954d01fc005199cfce03b4193bbc6ff21451e4d1eaf7785b4e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
