{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>종가_ex</th>\n",
       "      <th>1Y_대비_irs</th>\n",
       "      <th>10Y_대비_irs</th>\n",
       "      <th>1Y_대비_crs</th>\n",
       "      <th>대비_국고10년</th>\n",
       "      <th>대비_ndf</th>\n",
       "      <th>전일종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>1131.7</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1126.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>1134.8</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1131.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             종가_ex  1Y_대비_irs  10Y_대비_irs  1Y_대비_crs  대비_국고10년  대비_ndf  \\\n",
       "DateTime                                                                 \n",
       "2012-08-02  1131.7      -0.03       -0.05      -0.01     -0.04    1.75   \n",
       "2012-08-03  1134.8      -0.03       -0.02      -0.01     -0.07    4.00   \n",
       "\n",
       "            전일종가_ex  \n",
       "DateTime             \n",
       "2012-08-02   1126.5  \n",
       "2012-08-03   1131.7  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 파일 불러오기\n",
    "all = pd.read_excel(\"./xlsx/Join_data.xlsx\", index_col = 0)    \n",
    "\n",
    "# 컬럼 추출\n",
    "all2 = all[['DateTime', '종가_ex','1Y_전일비_irs', '2Y_전일비_irs', '3Y_전일비_irs',\n",
    "        '5Y_전일비_irs', '10Y_전일비_irs',  '1Y_전일비_crs', '2Y_전일비_crs','3Y_전일비_crs','5Y_전일비_crs','10Y_전일비_crs',\n",
    "        '국고1년', '국고3년', '국고5년','국고10년', '통안364일', '통안2년', 'Mid_ndf',\n",
    "        'M1_스왑포인트']]            # [[]] 대괄호 2개 사용 -> 데이터 프레임형태로 나옴\n",
    "\n",
    "all2 = all2.set_index(\"DateTime\")\n",
    "\n",
    "all2['대비_국고1년'] = all2['국고1년']-all2['국고1년'].shift(1)\n",
    "all2['대비_국고3년'] = all2['국고3년']-all2['국고3년'].shift(1)\n",
    "all2['대비_국고5년'] = all2['국고5년']-all2['국고5년'].shift(1)\n",
    "all2['대비_국고10년'] = all2['국고10년']-all2['국고10년'].shift(1)\n",
    "all2['대비_통안1년'] = all2['통안364일']-all2['통안364일'].shift(1)\n",
    "all2['대비_통안2년'] = all2['통안364일']-all2['통안364일'].shift(1)\n",
    "all2['대비_ndf'] = all2['Mid_ndf']-all2['Mid_ndf'].shift(1)\n",
    "all2['스왑포인트_1월물'] = all2[\"M1_스왑포인트\"]/100 \n",
    "all2['전일종가_ex'] = all2['종가_ex'].shift(1)\n",
    "\n",
    "all2.rename({\"1Y_전일비_irs\" : \"1Y_대비_irs\", \"2Y_전일비_irs\" : \"2Y_대비_irs\", \"3Y_전일비_irs\" : \"3Y_대비_irs\", \"5Y_전일비_irs\" : \"5Y_대비_irs\", \"10Y_전일비_irs\" : \"10Y_대비_irs\",\n",
    "            \"1Y_전일비_crs\" : \"1Y_대비_crs\", \"2Y_전일비_crs\" : \"2Y_대비_crs\", \"3Y_전일비_crs\" : \"3Y_대비_crs\", \"5Y_전일비_crs\" : \"5Y_대비_crs\", \"10Y_전일비_crs\" : \"10Y_대비_crs\"}, axis=1, inplace=True)\n",
    "\n",
    "all2 = all2.dropna()\n",
    "\n",
    "# 필요 칼럼만 남기기\n",
    "df = all2.copy()\n",
    "df = all2[[\"종가_ex\", \n",
    "            '1Y_대비_irs', '10Y_대비_irs', \n",
    "            '1Y_대비_crs',\n",
    "            \"대비_국고10년\",\n",
    "            \"대비_ndf\", \"전일종가_ex\"]]\n",
    "\n",
    "# DateTime을 인덱스로 바꿔주기\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1Y_대비_irs</th>\n",
       "      <th>10Y_대비_irs</th>\n",
       "      <th>1Y_대비_crs</th>\n",
       "      <th>대비_국고10년</th>\n",
       "      <th>대비_ndf</th>\n",
       "      <th>전일종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>-0.847862</td>\n",
       "      <td>-1.009126</td>\n",
       "      <td>-0.205336</td>\n",
       "      <td>-1.027569</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>-0.149841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-0.847862</td>\n",
       "      <td>-0.403231</td>\n",
       "      <td>-0.205336</td>\n",
       "      <td>-1.798403</td>\n",
       "      <td>0.184972</td>\n",
       "      <td>-0.056232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>0.560374</td>\n",
       "      <td>0.202663</td>\n",
       "      <td>-0.003044</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>-0.449862</td>\n",
       "      <td>-0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.278727</td>\n",
       "      <td>0.404628</td>\n",
       "      <td>0.401540</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>-0.104837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>0.278727</td>\n",
       "      <td>0.606592</td>\n",
       "      <td>0.199248</td>\n",
       "      <td>-0.513680</td>\n",
       "      <td>0.055654</td>\n",
       "      <td>-0.108437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-1.129510</td>\n",
       "      <td>-2.220914</td>\n",
       "      <td>-1.621379</td>\n",
       "      <td>-2.312292</td>\n",
       "      <td>0.154406</td>\n",
       "      <td>3.207485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>-0.284568</td>\n",
       "      <td>-0.605196</td>\n",
       "      <td>0.199248</td>\n",
       "      <td>-0.770625</td>\n",
       "      <td>0.050952</td>\n",
       "      <td>3.220086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>-0.284568</td>\n",
       "      <td>-1.211091</td>\n",
       "      <td>0.199248</td>\n",
       "      <td>-1.541458</td>\n",
       "      <td>-0.026639</td>\n",
       "      <td>3.110275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.842021</td>\n",
       "      <td>1.414452</td>\n",
       "      <td>0.806124</td>\n",
       "      <td>2.055766</td>\n",
       "      <td>-0.214738</td>\n",
       "      <td>3.212885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-1.974451</td>\n",
       "      <td>-2.422879</td>\n",
       "      <td>-2.228254</td>\n",
       "      <td>-2.055348</td>\n",
       "      <td>-0.485131</td>\n",
       "      <td>2.903255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1Y_대비_irs  10Y_대비_irs  1Y_대비_crs  대비_국고10년    대비_ndf   전일종가_ex\n",
       "DateTime                                                                  \n",
       "2012-08-02  -0.847862   -1.009126  -0.205336 -1.027569  0.079167 -0.149841\n",
       "2012-08-03  -0.847862   -0.403231  -0.205336 -1.798403  0.184972 -0.056232\n",
       "2012-08-06   0.560374    0.202663  -0.003044  0.000209 -0.449862 -0.000426\n",
       "2012-08-07   0.278727    0.404628   0.401540  0.000209  0.020386 -0.104837\n",
       "2012-08-08   0.278727    0.606592   0.199248 -0.513680  0.055654 -0.108437\n",
       "...               ...         ...        ...       ...       ...       ...\n",
       "2022-07-25  -1.129510   -2.220914  -1.621379 -2.312292  0.154406  3.207485\n",
       "2022-07-26  -0.284568   -0.605196   0.199248 -0.770625  0.050952  3.220086\n",
       "2022-07-27  -0.284568   -1.211091   0.199248 -1.541458 -0.026639  3.110275\n",
       "2022-07-28   0.842021    1.414452   0.806124  2.055766 -0.214738  3.212885\n",
       "2022-07-29  -1.974451   -2.422879  -2.228254 -2.055348 -0.485131  2.903255\n",
       "\n",
       "[2459 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 쓸 칼럼만 남기고 feature, target 분리해 각각 x,y 에 저장\n",
    "x = df[['1Y_대비_irs', '10Y_대비_irs',  \n",
    "            '1Y_대비_crs',\n",
    "            \"대비_국고10년\",\n",
    "            \"대비_ndf\", \"전일종가_ex\"]]\n",
    "y = df[[\"종가_ex\"]]\n",
    "\n",
    "# 이건 이렇게 해야 밑에 코드 8번째 줄 columns에 들어갈 수 있다고 하네요!\n",
    "x.feature = x.columns \n",
    "x.feature\n",
    "\n",
    "# scaling 진행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1Y_대비_irs</th>\n",
       "      <th>10Y_대비_irs</th>\n",
       "      <th>1Y_대비_crs</th>\n",
       "      <th>대비_국고10년</th>\n",
       "      <th>대비_ndf</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>-0.847862</td>\n",
       "      <td>-1.009126</td>\n",
       "      <td>-0.205336</td>\n",
       "      <td>-1.027569</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>1131.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-0.847862</td>\n",
       "      <td>-0.403231</td>\n",
       "      <td>-0.205336</td>\n",
       "      <td>-1.798403</td>\n",
       "      <td>0.184972</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>0.560374</td>\n",
       "      <td>0.202663</td>\n",
       "      <td>-0.003044</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>-0.449862</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.278727</td>\n",
       "      <td>0.404628</td>\n",
       "      <td>0.401540</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>0.278727</td>\n",
       "      <td>0.606592</td>\n",
       "      <td>0.199248</td>\n",
       "      <td>-0.513680</td>\n",
       "      <td>0.055654</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-1.129510</td>\n",
       "      <td>-2.220914</td>\n",
       "      <td>-1.621379</td>\n",
       "      <td>-2.312292</td>\n",
       "      <td>0.154406</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>-0.284568</td>\n",
       "      <td>-0.605196</td>\n",
       "      <td>0.199248</td>\n",
       "      <td>-0.770625</td>\n",
       "      <td>0.050952</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>-0.284568</td>\n",
       "      <td>-1.211091</td>\n",
       "      <td>0.199248</td>\n",
       "      <td>-1.541458</td>\n",
       "      <td>-0.026639</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.842021</td>\n",
       "      <td>1.414452</td>\n",
       "      <td>0.806124</td>\n",
       "      <td>2.055766</td>\n",
       "      <td>-0.214738</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-1.974451</td>\n",
       "      <td>-2.422879</td>\n",
       "      <td>-2.228254</td>\n",
       "      <td>-2.055348</td>\n",
       "      <td>-0.485131</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1Y_대비_irs  10Y_대비_irs  1Y_대비_crs  대비_국고10년    대비_ndf   전일종가_ex  \\\n",
       "DateTime                                                                     \n",
       "2012-08-02  -0.847862   -1.009126  -0.205336 -1.027569  0.079167 -0.149841   \n",
       "2012-08-03  -0.847862   -0.403231  -0.205336 -1.798403  0.184972 -0.056232   \n",
       "2012-08-06   0.560374    0.202663  -0.003044  0.000209 -0.449862 -0.000426   \n",
       "2012-08-07   0.278727    0.404628   0.401540  0.000209  0.020386 -0.104837   \n",
       "2012-08-08   0.278727    0.606592   0.199248 -0.513680  0.055654 -0.108437   \n",
       "...               ...         ...        ...       ...       ...       ...   \n",
       "2022-07-25  -1.129510   -2.220914  -1.621379 -2.312292  0.154406  3.207485   \n",
       "2022-07-26  -0.284568   -0.605196   0.199248 -0.770625  0.050952  3.220086   \n",
       "2022-07-27  -0.284568   -1.211091   0.199248 -1.541458 -0.026639  3.110275   \n",
       "2022-07-28   0.842021    1.414452   0.806124  2.055766 -0.214738  3.212885   \n",
       "2022-07-29  -1.974451   -2.422879  -2.228254 -2.055348 -0.485131  2.903255   \n",
       "\n",
       "             종가_ex  \n",
       "DateTime            \n",
       "2012-08-02  1131.7  \n",
       "2012-08-03  1134.8  \n",
       "2012-08-06  1129.0  \n",
       "2012-08-07  1128.8  \n",
       "2012-08-08  1128.3  \n",
       "...            ...  \n",
       "2022-07-25  1313.7  \n",
       "2022-07-26  1307.6  \n",
       "2022-07-27  1313.3  \n",
       "2022-07-28  1296.1  \n",
       "2022-07-29  1299.1  \n",
       "\n",
       "[2459 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = pd.concat([x_scaled,y], axis=1)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_scaled[0:1945]\n",
    "test = df_scaled[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=1):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['1Y_대비_irs', '10Y_대비_irs','1Y_대비_crs', '대비_국고10년', '대비_ndf', '전일종가_ex']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1555, 1, 6), (389, 1, 6))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.84567957e-01, -2.01266649e-01,  1.99248003e-01,\n",
       "         -5.13680169e-01, -1.20688685e-01, -1.35415782e+00]],\n",
       "\n",
       "       [[ 5.60373809e-01,  6.06592466e-01, -2.05335679e-01,\n",
       "          2.57153559e-01,  1.42650162e-01, -2.92054738e-01]],\n",
       "\n",
       "       [[-2.92070151e-03,  2.02662908e-01,  1.21070721e+00,\n",
       "          2.08982982e-04, -4.12242408e-01, -6.01684789e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.92070151e-03,  2.02662908e-01,  6.03831686e-01,\n",
       "         -2.56735593e-01, -1.81820917e-01,  1.09384223e-01]],\n",
       "\n",
       "       [[-8.47862468e-01, -8.07160985e-01, -6.09919362e-01,\n",
       "         -5.13680169e-01, -1.08234572e+00,  8.78058827e-01]],\n",
       "\n",
       "       [[-2.92070151e-03,  6.98129573e-04, -4.07627521e-01,\n",
       "         -2.56735593e-01, -7.75459699e-04, -8.66310473e-01]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((513, 1, 6), (513, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 1)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1264926.1250 - mae: 1123.5696\n",
      "Epoch 1: val_loss improved from inf to 1271041.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 13s 79ms/step - loss: 1265353.0000 - mae: 1123.7357 - val_loss: 1271041.6250 - val_mae: 1126.3025\n",
      "Epoch 2/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1263819.8750 - mae: 1123.0289\n",
      "Epoch 2: val_loss improved from 1271041.62500 to 1269542.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 1264187.0000 - mae: 1123.2151 - val_loss: 1269542.6250 - val_mae: 1125.6350\n",
      "Epoch 3/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1261277.0000 - mae: 1121.9202\n",
      "Epoch 3: val_loss improved from 1269542.62500 to 1266620.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1261802.1250 - mae: 1122.1483 - val_loss: 1266620.5000 - val_mae: 1124.3307\n",
      "Epoch 4/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1258360.1250 - mae: 1120.5989\n",
      "Epoch 4: val_loss improved from 1266620.50000 to 1261939.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1257645.8750 - mae: 1120.2794 - val_loss: 1261939.1250 - val_mae: 1122.2334\n",
      "Epoch 5/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1251363.1250 - mae: 1117.4454\n",
      "Epoch 5: val_loss improved from 1261939.12500 to 1254961.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1251363.1250 - mae: 1117.4454 - val_loss: 1254961.3750 - val_mae: 1119.0903\n",
      "Epoch 6/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1242569.0000 - mae: 1113.4547\n",
      "Epoch 6: val_loss improved from 1254961.37500 to 1245429.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1242475.3750 - mae: 1113.4037 - val_loss: 1245429.0000 - val_mae: 1114.7732\n",
      "Epoch 7/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1232724.3750 - mae: 1108.9427\n",
      "Epoch 7: val_loss improved from 1245429.00000 to 1233559.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1230959.5000 - mae: 1108.1348 - val_loss: 1233559.7500 - val_mae: 1109.3645\n",
      "Epoch 8/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1217530.6250 - mae: 1101.9478\n",
      "Epoch 8: val_loss improved from 1233559.75000 to 1219637.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1217264.6250 - mae: 1101.8336 - val_loss: 1219637.0000 - val_mae: 1102.9762\n",
      "Epoch 9/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1202098.6250 - mae: 1094.7964\n",
      "Epoch 9: val_loss improved from 1219637.00000 to 1203585.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1201419.7500 - mae: 1094.4836 - val_loss: 1203585.6250 - val_mae: 1095.5593\n",
      "Epoch 10/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 1187162.5000 - mae: 1087.7896\n",
      "Epoch 10: val_loss improved from 1203585.62500 to 1185718.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1183515.6250 - mae: 1086.1149 - val_loss: 1185718.5000 - val_mae: 1087.2372\n",
      "Epoch 11/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1164780.3750 - mae: 1077.1959\n",
      "Epoch 11: val_loss improved from 1185718.50000 to 1166255.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1163949.5000 - mae: 1076.8439 - val_loss: 1166255.3750 - val_mae: 1078.0774\n",
      "Epoch 12/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1144738.6250 - mae: 1067.7601\n",
      "Epoch 12: val_loss improved from 1166255.37500 to 1145650.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1142944.3750 - mae: 1066.7423 - val_loss: 1145650.3750 - val_mae: 1068.2793\n",
      "Epoch 13/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1120356.1250 - mae: 1055.8641\n",
      "Epoch 13: val_loss improved from 1145650.37500 to 1123737.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1120723.0000 - mae: 1056.0464 - val_loss: 1123737.7500 - val_mae: 1057.7310\n",
      "Epoch 14/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1097867.7500 - mae: 1044.7609\n",
      "Epoch 14: val_loss improved from 1123737.75000 to 1100881.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1097406.5000 - mae: 1044.5389 - val_loss: 1100881.5000 - val_mae: 1046.5924\n",
      "Epoch 15/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1073206.6250 - mae: 1032.5411\n",
      "Epoch 15: val_loss improved from 1100881.50000 to 1077188.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1073206.6250 - mae: 1032.5411 - val_loss: 1077188.1250 - val_mae: 1034.8840\n",
      "Epoch 16/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1049771.2500 - mae: 1020.8445\n",
      "Epoch 16: val_loss improved from 1077188.12500 to 1052816.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1048245.0000 - mae: 1020.1685 - val_loss: 1052816.0000 - val_mae: 1022.6765\n",
      "Epoch 17/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1022759.9375 - mae: 1007.1027\n",
      "Epoch 17: val_loss improved from 1052816.00000 to 1027760.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 1022711.2500 - mae: 1007.1879 - val_loss: 1027760.6250 - val_mae: 1009.9290\n",
      "Epoch 18/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 998959.8125 - mae: 995.0645 \n",
      "Epoch 18: val_loss improved from 1027760.62500 to 1002340.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 996713.6875 - mae: 993.9153 - val_loss: 1002340.0000 - val_mae: 996.7968\n",
      "Epoch 19/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 972748.6875 - mae: 981.3571\n",
      "Epoch 19: val_loss improved from 1002340.00000 to 976429.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 970398.5625 - mae: 980.2260 - val_loss: 976429.4375 - val_mae: 983.1873\n",
      "Epoch 20/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 945317.6875 - mae: 966.9177\n",
      "Epoch 20: val_loss improved from 976429.43750 to 950421.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 943793.8125 - mae: 966.1234 - val_loss: 950421.5000 - val_mae: 969.3046\n",
      "Epoch 21/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 914819.9375 - mae: 950.4208\n",
      "Epoch 21: val_loss improved from 950421.50000 to 923950.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 917157.6875 - mae: 951.6453 - val_loss: 923950.3750 - val_mae: 954.8920\n",
      "Epoch 22/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 890031.3125 - mae: 936.6607\n",
      "Epoch 22: val_loss improved from 923950.37500 to 897914.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 890425.3125 - mae: 936.8868 - val_loss: 897914.8125 - val_mae: 940.5017\n",
      "Epoch 23/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 864965.5625 - mae: 922.4556\n",
      "Epoch 23: val_loss improved from 897914.81250 to 871661.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 863832.8125 - mae: 921.8568 - val_loss: 871661.1875 - val_mae: 925.7054\n",
      "Epoch 24/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 837351.3125 - mae: 906.4767\n",
      "Epoch 24: val_loss improved from 871661.18750 to 845541.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 837351.3125 - mae: 906.4767 - val_loss: 845541.6875 - val_mae: 910.7008\n",
      "Epoch 25/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 811165.0000 - mae: 891.0760\n",
      "Epoch 25: val_loss improved from 845541.68750 to 819540.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 811117.0625 - mae: 891.0206 - val_loss: 819540.5000 - val_mae: 895.4495\n",
      "Epoch 26/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 785255.9375 - mae: 875.3115\n",
      "Epoch 26: val_loss improved from 819540.50000 to 793742.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 785255.9375 - mae: 875.3115 - val_loss: 793742.7500 - val_mae: 880.0242\n",
      "Epoch 27/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 761102.5000 - mae: 860.3402\n",
      "Epoch 27: val_loss improved from 793742.75000 to 768482.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 759708.1250 - mae: 859.5660 - val_loss: 768482.4375 - val_mae: 864.8572\n",
      "Epoch 28/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 736807.8125 - mae: 844.8867\n",
      "Epoch 28: val_loss improved from 768482.43750 to 743326.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 734570.0000 - mae: 843.5626 - val_loss: 743326.9375 - val_mae: 849.4491\n",
      "Epoch 29/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 710238.3750 - mae: 827.9012\n",
      "Epoch 29: val_loss improved from 743326.93750 to 718753.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 709849.1250 - mae: 827.7357 - val_loss: 718753.1250 - val_mae: 834.0732\n",
      "Epoch 30/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 687776.6250 - mae: 812.8761\n",
      "Epoch 30: val_loss improved from 718753.12500 to 694450.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 685560.4375 - mae: 811.5065 - val_loss: 694450.8125 - val_mae: 818.5137\n",
      "Epoch 31/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 660988.0625 - mae: 796.3130\n",
      "Epoch 31: val_loss improved from 694450.81250 to 670243.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 661813.8125 - mae: 795.3777 - val_loss: 670243.8750 - val_mae: 802.9237\n",
      "Epoch 32/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 642580.5625 - mae: 781.2833\n",
      "Epoch 32: val_loss improved from 670243.87500 to 647193.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 638456.6875 - mae: 779.1338 - val_loss: 647193.5625 - val_mae: 787.7355\n",
      "Epoch 33/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 615680.6875 - mae: 763.1523\n",
      "Epoch 33: val_loss improved from 647193.56250 to 624255.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 615757.6875 - mae: 763.1012 - val_loss: 624255.9375 - val_mae: 772.2617\n",
      "Epoch 34/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 594343.1875 - mae: 747.1220\n",
      "Epoch 34: val_loss improved from 624255.93750 to 601803.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 593585.0625 - mae: 746.5594 - val_loss: 601803.5000 - val_mae: 756.7528\n",
      "Epoch 35/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 574795.3750 - mae: 731.6402\n",
      "Epoch 35: val_loss improved from 601803.50000 to 579711.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 571973.0000 - mae: 730.1771 - val_loss: 579711.5625 - val_mae: 741.1239\n",
      "Epoch 36/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 546315.1875 - mae: 712.5266\n",
      "Epoch 36: val_loss improved from 579711.56250 to 558317.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 550906.3125 - mae: 713.8419 - val_loss: 558317.6250 - val_mae: 725.6281\n",
      "Epoch 37/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 530598.2500 - mae: 697.7281\n",
      "Epoch 37: val_loss improved from 558317.62500 to 537603.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 530467.2500 - mae: 697.6552 - val_loss: 537603.1250 - val_mae: 710.3743\n",
      "Epoch 38/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 512482.1875 - mae: 681.9950\n",
      "Epoch 38: val_loss improved from 537603.12500 to 517320.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 510610.0312 - mae: 681.5428 - val_loss: 517320.0938 - val_mae: 695.0847\n",
      "Epoch 39/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 493245.0625 - mae: 667.2720\n",
      "Epoch 39: val_loss improved from 517320.09375 to 497515.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 491379.3750 - mae: 665.8035 - val_loss: 497515.9375 - val_mae: 679.8414\n",
      "Epoch 40/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 470101.1875 - mae: 647.2709\n",
      "Epoch 40: val_loss improved from 497515.93750 to 478446.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 472720.0000 - mae: 649.9831 - val_loss: 478446.7188 - val_mae: 664.7932\n",
      "Epoch 41/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 454505.0000 - mae: 634.0250\n",
      "Epoch 41: val_loss improved from 478446.71875 to 459522.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 454707.8438 - mae: 634.2231 - val_loss: 459522.8750 - val_mae: 649.7839\n",
      "Epoch 42/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 444837.9062 - mae: 623.5888\n",
      "Epoch 42: val_loss improved from 459522.87500 to 441651.03125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 437164.2188 - mae: 618.7815 - val_loss: 441651.0312 - val_mae: 635.3118\n",
      "Epoch 43/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 422641.1875 - mae: 604.2551\n",
      "Epoch 43: val_loss improved from 441651.03125 to 424157.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 420305.8125 - mae: 603.7302 - val_loss: 424157.9375 - val_mae: 620.7699\n",
      "Epoch 44/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 407878.9688 - mae: 590.9284\n",
      "Epoch 44: val_loss improved from 424157.93750 to 407123.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 404004.2500 - mae: 588.7768 - val_loss: 407123.2188 - val_mae: 606.2628\n",
      "Epoch 45/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 392847.9062 - mae: 576.9582\n",
      "Epoch 45: val_loss improved from 407123.21875 to 390593.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 388301.7812 - mae: 573.7789 - val_loss: 390593.5312 - val_mae: 591.8497\n",
      "Epoch 46/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 368120.9375 - mae: 559.4274\n",
      "Epoch 46: val_loss improved from 390593.53125 to 374465.59375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 373165.1875 - mae: 559.0767 - val_loss: 374465.5938 - val_mae: 577.4164\n",
      "Epoch 47/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 358442.8125 - mae: 544.4949\n",
      "Epoch 47: val_loss improved from 374465.59375 to 359043.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 358442.8125 - mae: 544.4949 - val_loss: 359043.5000 - val_mae: 563.3174\n",
      "Epoch 48/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 347289.9688 - mae: 532.7455\n",
      "Epoch 48: val_loss improved from 359043.50000 to 344277.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 344311.0938 - mae: 530.2358 - val_loss: 344277.4375 - val_mae: 549.5501\n",
      "Epoch 49/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 331318.7188 - mae: 517.1660\n",
      "Epoch 49: val_loss improved from 344277.43750 to 329743.28125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 330678.6562 - mae: 516.4095 - val_loss: 329743.2812 - val_mae: 535.9336\n",
      "Epoch 50/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 322793.9688 - mae: 506.7184\n",
      "Epoch 50: val_loss improved from 329743.28125 to 315865.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 317540.1562 - mae: 503.2310 - val_loss: 315865.5312 - val_mae: 522.6485\n",
      "Epoch 51/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 308635.3125 - mae: 492.5744\n",
      "Epoch 51: val_loss improved from 315865.53125 to 302286.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 304905.0000 - mae: 489.9508 - val_loss: 302286.9375 - val_mae: 509.2247\n",
      "Epoch 52/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 297304.4062 - mae: 479.8859\n",
      "Epoch 52: val_loss improved from 302286.93750 to 289250.03125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 292713.1250 - mae: 477.4264 - val_loss: 289250.0312 - val_mae: 496.3680\n",
      "Epoch 53/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 281899.5938 - mae: 463.8177\n",
      "Epoch 53: val_loss improved from 289250.03125 to 276613.03125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 280982.7500 - mae: 464.7522 - val_loss: 276613.0312 - val_mae: 483.5635\n",
      "Epoch 54/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 271323.1250 - mae: 452.2279\n",
      "Epoch 54: val_loss improved from 276613.03125 to 264387.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 269624.5625 - mae: 452.5956 - val_loss: 264387.8125 - val_mae: 471.0764\n",
      "Epoch 55/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 258683.1094 - mae: 441.0845\n",
      "Epoch 55: val_loss improved from 264387.81250 to 252580.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 258683.1094 - mae: 441.0845 - val_loss: 252580.6250 - val_mae: 458.7394\n",
      "Epoch 56/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 253542.2969 - mae: 431.9644\n",
      "Epoch 56: val_loss improved from 252580.62500 to 240979.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 248181.5938 - mae: 429.2532 - val_loss: 240979.0000 - val_mae: 446.4309\n",
      "Epoch 57/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 227211.9219 - mae: 415.0984\n",
      "Epoch 57: val_loss improved from 240979.00000 to 229809.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 237995.9531 - mae: 417.9944 - val_loss: 229809.5000 - val_mae: 434.1898\n",
      "Epoch 58/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 230164.4531 - mae: 408.9510\n",
      "Epoch 58: val_loss improved from 229809.50000 to 219190.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 228114.0469 - mae: 407.3683 - val_loss: 219190.8750 - val_mae: 422.7658\n",
      "Epoch 59/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 218289.6562 - mae: 395.3659\n",
      "Epoch 59: val_loss improved from 219190.87500 to 208906.42188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 218617.8125 - mae: 396.9008 - val_loss: 208906.4219 - val_mae: 411.2671\n",
      "Epoch 60/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 212058.4844 - mae: 388.5087\n",
      "Epoch 60: val_loss improved from 208906.42188 to 198657.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 209505.7031 - mae: 386.5400 - val_loss: 198657.3438 - val_mae: 399.6422\n",
      "Epoch 61/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 202026.6875 - mae: 377.6894\n",
      "Epoch 61: val_loss improved from 198657.34375 to 189165.03125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 200639.3438 - mae: 376.5638 - val_loss: 189165.0312 - val_mae: 388.7519\n",
      "Epoch 62/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 192565.4062 - mae: 366.9221\n",
      "Epoch 62: val_loss improved from 189165.03125 to 179847.20312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 192133.7656 - mae: 366.6837 - val_loss: 179847.2031 - val_mae: 377.7917\n",
      "Epoch 63/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 184900.0469 - mae: 356.4123\n",
      "Epoch 63: val_loss improved from 179847.20312 to 170917.28125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 183875.9688 - mae: 356.8348 - val_loss: 170917.2812 - val_mae: 367.0819\n",
      "Epoch 64/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 180138.5156 - mae: 350.5380\n",
      "Epoch 64: val_loss improved from 170917.28125 to 162178.96875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 176000.2812 - mae: 347.0409 - val_loss: 162178.9688 - val_mae: 356.3080\n",
      "Epoch 65/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 168413.1562 - mae: 337.8779\n",
      "Epoch 65: val_loss improved from 162178.96875 to 153797.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 168321.6719 - mae: 337.7681 - val_loss: 153797.9375 - val_mae: 345.8419\n",
      "Epoch 66/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 163801.4219 - mae: 330.1026\n",
      "Epoch 66: val_loss improved from 153797.93750 to 145922.51562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 160971.7812 - mae: 328.7624 - val_loss: 145922.5156 - val_mae: 335.9272\n",
      "Epoch 67/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 141792.7656 - mae: 314.8019\n",
      "Epoch 67: val_loss improved from 145922.51562 to 137971.10938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 153801.0469 - mae: 319.5012 - val_loss: 137971.1094 - val_mae: 325.6603\n",
      "Epoch 68/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 147080.3438 - mae: 310.2420\n",
      "Epoch 68: val_loss improved from 137971.10938 to 130733.48438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 146887.1250 - mae: 310.7987 - val_loss: 130733.4844 - val_mae: 316.1424\n",
      "Epoch 69/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 140533.4844 - mae: 302.0849\n",
      "Epoch 69: val_loss improved from 130733.48438 to 123474.73438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 100ms/step - loss: 140312.0000 - mae: 301.7915 - val_loss: 123474.7344 - val_mae: 306.2484\n",
      "Epoch 70/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 136871.7031 - mae: 295.7017\n",
      "Epoch 70: val_loss improved from 123474.73438 to 116719.76562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 107ms/step - loss: 134007.1719 - mae: 293.1143 - val_loss: 116719.7656 - val_mae: 296.8456\n",
      "Epoch 71/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 129575.9219 - mae: 283.4168\n",
      "Epoch 71: val_loss improved from 116719.76562 to 110230.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 111ms/step - loss: 127875.2734 - mae: 284.3245 - val_loss: 110230.1250 - val_mae: 287.4448\n",
      "Epoch 72/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 123283.5859 - mae: 275.5223\n",
      "Epoch 72: val_loss improved from 110230.12500 to 104001.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 107ms/step - loss: 121996.8828 - mae: 275.9641 - val_loss: 104001.4688 - val_mae: 278.2044\n",
      "Epoch 73/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 117461.3906 - mae: 268.3409\n",
      "Epoch 73: val_loss improved from 104001.46875 to 97962.46094, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 116361.3203 - mae: 267.6992 - val_loss: 97962.4609 - val_mae: 269.0249\n",
      "Epoch 74/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 112721.7812 - mae: 259.6873\n",
      "Epoch 74: val_loss improved from 97962.46094 to 92209.49219, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 114ms/step - loss: 110965.2578 - mae: 259.0046 - val_loss: 92209.4922 - val_mae: 259.9172\n",
      "Epoch 75/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 108398.5625 - mae: 253.1302\n",
      "Epoch 75: val_loss improved from 92209.49219 to 86775.08594, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 105774.9062 - mae: 251.0149 - val_loss: 86775.0859 - val_mae: 251.0762\n",
      "Epoch 76/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 100828.6172 - mae: 242.8358\n",
      "Epoch 76: val_loss improved from 86775.08594 to 81499.78125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 12s 120ms/step - loss: 100828.6172 - mae: 242.8358 - val_loss: 81499.7812 - val_mae: 242.0444\n",
      "Epoch 77/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 86070.7344 - mae: 232.0237\n",
      "Epoch 77: val_loss improved from 81499.78125 to 76575.28906, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 96066.2422 - mae: 234.9215 - val_loss: 76575.2891 - val_mae: 233.4553\n",
      "Epoch 78/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 93838.4922 - mae: 227.4873\n",
      "Epoch 78: val_loss improved from 76575.28906 to 71990.32031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 91542.7109 - mae: 227.5238 - val_loss: 71990.3203 - val_mae: 225.2340\n",
      "Epoch 79/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 87714.6953 - mae: 220.0195\n",
      "Epoch 79: val_loss improved from 71990.32031 to 67601.61719, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 87272.4609 - mae: 219.8110 - val_loss: 67601.6172 - val_mae: 217.1322\n",
      "Epoch 80/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 83665.2969 - mae: 212.7781\n",
      "Epoch 80: val_loss improved from 67601.61719 to 63392.14062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 83194.6406 - mae: 212.5207 - val_loss: 63392.1406 - val_mae: 208.9467\n",
      "Epoch 81/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 81171.7344 - mae: 206.4923\n",
      "Epoch 81: val_loss improved from 63392.14062 to 59430.17578, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 79349.0625 - mae: 205.0074 - val_loss: 59430.1758 - val_mae: 200.9551\n",
      "Epoch 82/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 63440.0586 - mae: 192.1087\n",
      "Epoch 82: val_loss improved from 59430.17578 to 55725.49219, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 75696.7734 - mae: 197.7218 - val_loss: 55725.4922 - val_mae: 193.0957\n",
      "Epoch 83/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 73915.5391 - mae: 191.2405\n",
      "Epoch 83: val_loss improved from 55725.49219 to 52248.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 72216.3516 - mae: 190.8470 - val_loss: 52248.1250 - val_mae: 185.7550\n",
      "Epoch 84/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 71847.6484 - mae: 183.9924\n",
      "Epoch 84: val_loss improved from 52248.12500 to 49006.26562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 68919.6797 - mae: 183.7051 - val_loss: 49006.2656 - val_mae: 178.4635\n",
      "Epoch 85/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 65612.7656 - mae: 176.8185\n",
      "Epoch 85: val_loss improved from 49006.26562 to 45875.93359, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 65776.9375 - mae: 177.0565 - val_loss: 45875.9336 - val_mae: 171.1723\n",
      "Epoch 86/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 63096.8320 - mae: 170.1423\n",
      "Epoch 86: val_loss improved from 45875.93359 to 43045.79688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 62850.2695 - mae: 170.6223 - val_loss: 43045.7969 - val_mae: 164.2451\n",
      "Epoch 87/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 51424.8633 - mae: 161.5457\n",
      "Epoch 87: val_loss improved from 43045.79688 to 40328.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 60040.1797 - mae: 163.8919 - val_loss: 40328.0938 - val_mae: 157.1497\n",
      "Epoch 88/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 61317.4844 - mae: 160.0957\n",
      "Epoch 88: val_loss improved from 40328.09375 to 37901.72656, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 57433.5078 - mae: 157.7841 - val_loss: 37901.7266 - val_mae: 150.8400\n",
      "Epoch 89/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 56677.2461 - mae: 151.7805\n",
      "Epoch 89: val_loss improved from 37901.72656 to 35580.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 54972.4766 - mae: 151.6996 - val_loss: 35580.4688 - val_mae: 144.3152\n",
      "Epoch 90/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 55231.1992 - mae: 147.5770\n",
      "Epoch 90: val_loss improved from 35580.46875 to 33449.76562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 52653.3164 - mae: 145.9384 - val_loss: 33449.7656 - val_mae: 138.1177\n",
      "Epoch 91/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 39550.0117 - mae: 136.2514\n",
      "Epoch 91: val_loss improved from 33449.76562 to 31463.20508, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 50490.6797 - mae: 139.8172 - val_loss: 31463.2051 - val_mae: 131.8593\n",
      "Epoch 92/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 51801.8125 - mae: 136.6110\n",
      "Epoch 92: val_loss improved from 31463.20508 to 29681.80469, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 48520.1172 - mae: 135.0394 - val_loss: 29681.8047 - val_mae: 126.4490\n",
      "Epoch 93/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 46507.0625 - mae: 129.3132\n",
      "Epoch 93: val_loss improved from 29681.80469 to 28012.78125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 46507.0625 - mae: 129.3132 - val_loss: 28012.7812 - val_mae: 120.9979\n",
      "Epoch 94/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 45542.7148 - mae: 124.7824\n",
      "Epoch 94: val_loss improved from 28012.78125 to 26416.69141, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 44655.4102 - mae: 124.3214 - val_loss: 26416.6914 - val_mae: 115.4390\n",
      "Epoch 95/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 34380.5430 - mae: 117.1356\n",
      "Epoch 95: val_loss improved from 26416.69141 to 24985.76172, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 42821.6055 - mae: 118.7104 - val_loss: 24985.7617 - val_mae: 110.2344\n",
      "Epoch 96/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 33446.7422 - mae: 113.2662\n",
      "Epoch 96: val_loss improved from 24985.76172 to 23700.62305, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 40669.7734 - mae: 113.9351 - val_loss: 23700.6230 - val_mae: 105.2223\n",
      "Epoch 97/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 38225.0273 - mae: 109.4212\n",
      "Epoch 97: val_loss improved from 23700.62305 to 22441.64258, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 38163.6172 - mae: 109.3611 - val_loss: 22441.6426 - val_mae: 100.6107\n",
      "Epoch 98/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 36455.3789 - mae: 105.7276\n",
      "Epoch 98: val_loss improved from 22441.64258 to 21336.40820, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 34338.0469 - mae: 104.4132 - val_loss: 21336.4082 - val_mae: 96.1917\n",
      "Epoch 99/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 33060.3320 - mae: 99.4530 \n",
      "Epoch 99: val_loss improved from 21336.40820 to 20315.44727, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 31617.8262 - mae: 99.9292 - val_loss: 20315.4473 - val_mae: 91.8847\n",
      "Epoch 100/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 29344.8496 - mae: 95.1883\n",
      "Epoch 100: val_loss improved from 20315.44727 to 19382.37695, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 29344.8496 - mae: 95.1883 - val_loss: 19382.3770 - val_mae: 87.4613\n",
      "Epoch 101/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 24900.7324 - mae: 90.4746\n",
      "Epoch 101: val_loss improved from 19382.37695 to 18509.81445, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 27317.6875 - mae: 90.8291 - val_loss: 18509.8145 - val_mae: 83.4153\n",
      "Epoch 102/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 25668.4648 - mae: 86.9651\n",
      "Epoch 102: val_loss improved from 18509.81445 to 17693.79883, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 25668.4648 - mae: 86.9651 - val_loss: 17693.7988 - val_mae: 79.5116\n",
      "Epoch 103/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 23619.6328 - mae: 82.4878\n",
      "Epoch 103: val_loss improved from 17693.79883 to 17003.66992, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 23619.6328 - mae: 82.4878 - val_loss: 17003.6699 - val_mae: 75.5464\n",
      "Epoch 104/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 21910.5977 - mae: 78.7544\n",
      "Epoch 104: val_loss improved from 17003.66992 to 16305.72852, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 21455.2695 - mae: 78.5449 - val_loss: 16305.7285 - val_mae: 71.9042\n",
      "Epoch 105/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 19661.3340 - mae: 74.9907\n",
      "Epoch 105: val_loss improved from 16305.72852 to 15684.85449, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 19200.4980 - mae: 74.6440 - val_loss: 15684.8545 - val_mae: 68.3709\n",
      "Epoch 106/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 17935.7480 - mae: 71.4949\n",
      "Epoch 106: val_loss improved from 15684.85449 to 15153.70996, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 102ms/step - loss: 17418.8184 - mae: 71.0244 - val_loss: 15153.7100 - val_mae: 65.1679\n",
      "Epoch 107/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 17521.6250 - mae: 68.4213\n",
      "Epoch 107: val_loss improved from 15153.70996 to 14654.85742, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 16447.4316 - mae: 67.5604 - val_loss: 14654.8574 - val_mae: 61.9519\n",
      "Epoch 108/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 16360.6836 - mae: 64.3418\n",
      "Epoch 108: val_loss improved from 14654.85742 to 14194.37012, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 15722.5449 - mae: 64.3012 - val_loss: 14194.3701 - val_mae: 58.9168\n",
      "Epoch 109/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 16050.8193 - mae: 62.0423\n",
      "Epoch 109: val_loss improved from 14194.37012 to 13777.00098, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 15080.7861 - mae: 61.6642 - val_loss: 13777.0010 - val_mae: 56.4002\n",
      "Epoch 110/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 15522.6562 - mae: 60.0848\n",
      "Epoch 110: val_loss improved from 13777.00098 to 13374.76562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 14489.9463 - mae: 59.0134 - val_loss: 13374.7656 - val_mae: 53.8832\n",
      "Epoch 111/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 14289.8184 - mae: 56.8268\n",
      "Epoch 111: val_loss improved from 13374.76562 to 13008.16602, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 13917.5576 - mae: 56.4114 - val_loss: 13008.1660 - val_mae: 51.4273\n",
      "Epoch 112/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 13460.7539 - mae: 53.8572\n",
      "Epoch 112: val_loss improved from 13008.16602 to 12651.78125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 13379.8203 - mae: 53.8614 - val_loss: 12651.7812 - val_mae: 49.3085\n",
      "Epoch 113/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 12761.1270 - mae: 51.3859\n",
      "Epoch 113: val_loss improved from 12651.78125 to 12315.65137, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 12866.7842 - mae: 51.7436 - val_loss: 12315.6514 - val_mae: 47.3038\n",
      "Epoch 114/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 12393.9326 - mae: 49.6344\n",
      "Epoch 114: val_loss improved from 12315.65137 to 11986.95996, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 12371.7568 - mae: 49.5892 - val_loss: 11986.9600 - val_mae: 45.2769\n",
      "Epoch 115/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 12369.4131 - mae: 48.1122\n",
      "Epoch 115: val_loss improved from 11986.95996 to 11646.09863, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 11883.9639 - mae: 47.4456 - val_loss: 11646.0986 - val_mae: 43.6656\n",
      "Epoch 116/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 12170.4326 - mae: 46.0288\n",
      "Epoch 116: val_loss improved from 11646.09863 to 11306.40918, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 11411.3076 - mae: 45.7820 - val_loss: 11306.4092 - val_mae: 41.8850\n",
      "Epoch 117/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 10536.3154 - mae: 43.5612\n",
      "Epoch 117: val_loss improved from 11306.40918 to 11015.48047, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 10960.7207 - mae: 43.7919 - val_loss: 11015.4805 - val_mae: 40.2419\n",
      "Epoch 118/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 10685.4805 - mae: 42.1916\n",
      "Epoch 118: val_loss improved from 11015.48047 to 10667.13281, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 115ms/step - loss: 10536.4512 - mae: 42.0988 - val_loss: 10667.1328 - val_mae: 38.7946\n",
      "Epoch 119/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 10493.6387 - mae: 40.6232\n",
      "Epoch 119: val_loss improved from 10667.13281 to 10363.49414, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 10114.6016 - mae: 40.5100 - val_loss: 10363.4941 - val_mae: 37.1818\n",
      "Epoch 120/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 10213.1611 - mae: 39.1855\n",
      "Epoch 120: val_loss improved from 10363.49414 to 10052.83203, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 9720.4219 - mae: 38.9166 - val_loss: 10052.8320 - val_mae: 35.7389\n",
      "Epoch 121/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 8940.2852 - mae: 37.6358 \n",
      "Epoch 121: val_loss improved from 10052.83203 to 9735.08203, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 94ms/step - loss: 9331.9492 - mae: 37.6160 - val_loss: 9735.0820 - val_mae: 34.4954\n",
      "Epoch 122/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 9572.5742 - mae: 36.2851\n",
      "Epoch 122: val_loss improved from 9735.08203 to 9440.11230, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 8956.4180 - mae: 36.0391 - val_loss: 9440.1123 - val_mae: 33.2160\n",
      "Epoch 123/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 8677.3770 - mae: 34.8079\n",
      "Epoch 123: val_loss improved from 9440.11230 to 9147.34082, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 8601.8564 - mae: 34.7475 - val_loss: 9147.3408 - val_mae: 32.0350\n",
      "Epoch 124/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 8246.9043 - mae: 33.4899\n",
      "Epoch 124: val_loss improved from 9147.34082 to 8879.67578, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 8246.9043 - mae: 33.4899 - val_loss: 8879.6758 - val_mae: 30.8316\n",
      "Epoch 125/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 7257.8735 - mae: 31.4941\n",
      "Epoch 125: val_loss improved from 8879.67578 to 8613.90332, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 104ms/step - loss: 7911.1973 - mae: 31.9954 - val_loss: 8613.9033 - val_mae: 29.7550\n",
      "Epoch 126/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 7686.5039 - mae: 31.2751\n",
      "Epoch 126: val_loss improved from 8613.90332 to 8332.33887, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 7578.7915 - mae: 31.2181 - val_loss: 8332.3389 - val_mae: 28.9616\n",
      "Epoch 127/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 7434.4575 - mae: 30.2070\n",
      "Epoch 127: val_loss improved from 8332.33887 to 8071.01465, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 7223.3286 - mae: 30.1115 - val_loss: 8071.0146 - val_mae: 27.9076\n",
      "Epoch 128/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 7462.2559 - mae: 29.5199\n",
      "Epoch 128: val_loss improved from 8071.01465 to 7819.04053, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 6821.0830 - mae: 28.9652 - val_loss: 7819.0405 - val_mae: 27.0024\n",
      "Epoch 129/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 6470.9771 - mae: 27.7898\n",
      "Epoch 129: val_loss improved from 7819.04053 to 7585.82178, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 6341.9434 - mae: 27.6760 - val_loss: 7585.8218 - val_mae: 26.2105\n",
      "Epoch 130/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 6394.4375 - mae: 27.3399\n",
      "Epoch 130: val_loss improved from 7585.82178 to 7344.38623, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 5921.6655 - mae: 26.8453 - val_loss: 7344.3862 - val_mae: 25.2964\n",
      "Epoch 131/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 5504.5684 - mae: 25.5785\n",
      "Epoch 131: val_loss improved from 7344.38623 to 7116.91309, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 5612.1665 - mae: 25.8947 - val_loss: 7116.9131 - val_mae: 24.3250\n",
      "Epoch 132/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 5637.5195 - mae: 25.1965\n",
      "Epoch 132: val_loss improved from 7116.91309 to 6902.53320, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 5340.7944 - mae: 24.9343 - val_loss: 6902.5332 - val_mae: 23.5578\n",
      "Epoch 133/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 5353.6079 - mae: 24.0683\n",
      "Epoch 133: val_loss improved from 6902.53320 to 6671.17871, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 5082.1831 - mae: 23.8714 - val_loss: 6671.1787 - val_mae: 22.8814\n",
      "Epoch 134/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 4908.4248 - mae: 23.1747\n",
      "Epoch 134: val_loss improved from 6671.17871 to 6461.33740, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 93ms/step - loss: 4828.3037 - mae: 23.1606 - val_loss: 6461.3374 - val_mae: 22.0666\n",
      "Epoch 135/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 4667.1118 - mae: 22.3615\n",
      "Epoch 135: val_loss improved from 6461.33740 to 6225.93262, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 117ms/step - loss: 4584.9761 - mae: 22.2434 - val_loss: 6225.9326 - val_mae: 21.3679\n",
      "Epoch 136/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 4433.6533 - mae: 21.6095\n",
      "Epoch 136: val_loss improved from 6225.93262 to 6001.29102, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 4354.1182 - mae: 21.5595 - val_loss: 6001.2910 - val_mae: 20.7513\n",
      "Epoch 137/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 4236.8057 - mae: 20.8712\n",
      "Epoch 137: val_loss improved from 6001.29102 to 5777.74609, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 4107.7393 - mae: 20.7600 - val_loss: 5777.7461 - val_mae: 20.0961\n",
      "Epoch 138/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 3902.1775 - mae: 20.2071\n",
      "Epoch 138: val_loss improved from 5777.74609 to 5563.10107, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 94ms/step - loss: 3785.1257 - mae: 20.0296 - val_loss: 5563.1011 - val_mae: 19.4546\n",
      "Epoch 139/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 3287.3694 - mae: 19.3827\n",
      "Epoch 139: val_loss improved from 5563.10107 to 5322.72754, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 3287.3694 - mae: 19.3827 - val_loss: 5322.7275 - val_mae: 18.7639\n",
      "Epoch 140/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 2753.6216 - mae: 18.4278\n",
      "Epoch 140: val_loss improved from 5322.72754 to 5078.14893, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 94ms/step - loss: 2753.6216 - mae: 18.4278 - val_loss: 5078.1489 - val_mae: 18.2190\n",
      "Epoch 141/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 2357.6345 - mae: 17.2683\n",
      "Epoch 141: val_loss improved from 5078.14893 to 4861.71631, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 2418.3359 - mae: 17.7955 - val_loss: 4861.7163 - val_mae: 17.5026\n",
      "Epoch 142/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 2082.1562 - mae: 16.8791\n",
      "Epoch 142: val_loss improved from 4861.71631 to 4606.52295, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 2070.4922 - mae: 16.9346 - val_loss: 4606.5229 - val_mae: 17.0357\n",
      "Epoch 143/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1740.6068 - mae: 16.2003\n",
      "Epoch 143: val_loss improved from 4606.52295 to 4378.76758, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1740.9091 - mae: 16.2349 - val_loss: 4378.7676 - val_mae: 16.4674\n",
      "Epoch 144/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1582.5006 - mae: 15.6643\n",
      "Epoch 144: val_loss improved from 4378.76758 to 4169.67578, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1565.3325 - mae: 15.6045 - val_loss: 4169.6758 - val_mae: 16.0077\n",
      "Epoch 145/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1494.9395 - mae: 15.0871\n",
      "Epoch 145: val_loss improved from 4169.67578 to 3981.72144, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 93ms/step - loss: 1431.8838 - mae: 15.1391 - val_loss: 3981.7214 - val_mae: 15.5453\n",
      "Epoch 146/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1405.2065 - mae: 14.9966\n",
      "Epoch 146: val_loss improved from 3981.72144 to 3810.55908, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1325.9489 - mae: 14.6882 - val_loss: 3810.5591 - val_mae: 15.1398\n",
      "Epoch 147/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1228.7954 - mae: 14.0237\n",
      "Epoch 147: val_loss improved from 3810.55908 to 3662.41235, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 1237.4796 - mae: 14.2828 - val_loss: 3662.4124 - val_mae: 14.7897\n",
      "Epoch 148/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1265.0950 - mae: 14.2826\n",
      "Epoch 148: val_loss improved from 3662.41235 to 3505.54102, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1158.5090 - mae: 13.9746 - val_loss: 3505.5410 - val_mae: 14.4572\n",
      "Epoch 149/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1127.2363 - mae: 13.7112\n",
      "Epoch 149: val_loss improved from 3505.54102 to 3380.17969, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 1088.0623 - mae: 13.6058 - val_loss: 3380.1797 - val_mae: 14.1388\n",
      "Epoch 150/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 971.4074 - mae: 13.0886\n",
      "Epoch 150: val_loss improved from 3380.17969 to 3255.07446, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1028.2509 - mae: 13.2087 - val_loss: 3255.0745 - val_mae: 13.8206\n",
      "Epoch 151/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 976.6542 - mae: 12.9792 \n",
      "Epoch 151: val_loss improved from 3255.07446 to 3132.87012, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 976.6542 - mae: 12.9792 - val_loss: 3132.8701 - val_mae: 13.6168\n",
      "Epoch 152/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 969.5486 - mae: 12.7448\n",
      "Epoch 152: val_loss improved from 3132.87012 to 3026.75024, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 927.8531 - mae: 12.7813 - val_loss: 3026.7502 - val_mae: 13.4056\n",
      "Epoch 153/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 920.3608 - mae: 12.7755\n",
      "Epoch 153: val_loss improved from 3026.75024 to 2919.11890, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 885.2380 - mae: 12.5448 - val_loss: 2919.1189 - val_mae: 13.2218\n",
      "Epoch 154/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 674.6030 - mae: 12.0605\n",
      "Epoch 154: val_loss improved from 2919.11890 to 2819.52344, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 842.9769 - mae: 12.3408 - val_loss: 2819.5234 - val_mae: 13.0345\n",
      "Epoch 155/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 796.2925 - mae: 12.0976\n",
      "Epoch 155: val_loss improved from 2819.52344 to 2747.72485, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 796.2925 - mae: 12.0976 - val_loss: 2747.7249 - val_mae: 12.9319\n",
      "Epoch 156/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 604.2740 - mae: 11.6087\n",
      "Epoch 156: val_loss improved from 2747.72485 to 2644.16577, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 746.0423 - mae: 11.8861 - val_loss: 2644.1658 - val_mae: 12.7905\n",
      "Epoch 157/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 728.3142 - mae: 11.7724\n",
      "Epoch 157: val_loss improved from 2644.16577 to 2563.47900, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 714.3405 - mae: 11.7388 - val_loss: 2563.4790 - val_mae: 12.6896\n",
      "Epoch 158/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 687.5479 - mae: 11.6393\n",
      "Epoch 158: val_loss improved from 2563.47900 to 2485.04492, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 686.3503 - mae: 11.6295 - val_loss: 2485.0449 - val_mae: 12.5821\n",
      "Epoch 159/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 595.4507 - mae: 11.1245\n",
      "Epoch 159: val_loss improved from 2485.04492 to 2405.42920, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 657.5456 - mae: 11.4512 - val_loss: 2405.4292 - val_mae: 12.4499\n",
      "Epoch 160/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 640.5289 - mae: 11.4036\n",
      "Epoch 160: val_loss improved from 2405.42920 to 2314.71606, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 626.3257 - mae: 11.4807 - val_loss: 2314.7161 - val_mae: 12.3828\n",
      "Epoch 161/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 589.3887 - mae: 11.3200\n",
      "Epoch 161: val_loss improved from 2314.71606 to 2246.78076, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 588.3380 - mae: 11.3108 - val_loss: 2246.7808 - val_mae: 12.3259\n",
      "Epoch 162/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 593.3922 - mae: 11.1549\n",
      "Epoch 162: val_loss improved from 2246.78076 to 2163.70093, saving model to .\\\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일(모델학습을 위한 학습과정 설정단계)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "print(\"\\n Test Accuracy: %.4f\" % model.evaluate(x_valid, y_valid))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='Tesetset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='Traintset_loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7adfe9f3775c954d01fc005199cfce03b4193bbc6ff21451e4d1eaf7785b4e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
