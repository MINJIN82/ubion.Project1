{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>열1</th>\n",
       "      <th>종가_ex</th>\n",
       "      <th>대비_ex</th>\n",
       "      <th>증감률_ex</th>\n",
       "      <th>1Y_Mid_irs</th>\n",
       "      <th>1Y_전일비_irs</th>\n",
       "      <th>2Y_Mid_irs</th>\n",
       "      <th>2Y_전일비_irs</th>\n",
       "      <th>3Y_Mid_irs</th>\n",
       "      <th>3Y_전일비_irs</th>\n",
       "      <th>...</th>\n",
       "      <th>국고10년대비</th>\n",
       "      <th>통안1년대비</th>\n",
       "      <th>통안2년대비</th>\n",
       "      <th>전일비_1Y_베이시스</th>\n",
       "      <th>전일비_2Y_베이시스</th>\n",
       "      <th>전일비_3Y_베이시스</th>\n",
       "      <th>전일비_5Y_베이시스</th>\n",
       "      <th>전일비_10Y_베이시스</th>\n",
       "      <th>전날 종가_ex</th>\n",
       "      <th>종가_NDF차이</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.820</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2.690</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2.690</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1126.5</td>\n",
       "      <td>-7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>2</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2.790</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2.660</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2.660</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>1131.7</td>\n",
       "      <td>-6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>3</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>2.810</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.680</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.680</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1134.8</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>4</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>2.820</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.680</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.680</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>5</td>\n",
       "      <td>1128.3</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2.830</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.700</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.700</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1128.8</td>\n",
       "      <td>-1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>2455</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.165</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>3.235</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.205</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>2456</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>3.155</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.215</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.175</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1313.7</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>2457</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.43</td>\n",
       "      <td>3.145</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.165</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.115</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1307.6</td>\n",
       "      <td>-2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>2458</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>-17.2</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>3.175</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.205</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.165</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1313.3</td>\n",
       "      <td>7.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>2459</td>\n",
       "      <td>1299.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3.105</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>3.065</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>3.025</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1296.1</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              열1   종가_ex  대비_ex  증감률_ex  1Y_Mid_irs  1Y_전일비_irs  2Y_Mid_irs  \\\n",
       "DateTime                                                                      \n",
       "2012-08-02     1  1131.7    5.2    0.46       2.820       -0.03       2.690   \n",
       "2012-08-03     2  1134.8    3.1    0.27       2.790       -0.03       2.660   \n",
       "2012-08-06     3  1129.0   -5.8   -0.51       2.810        0.02       2.680   \n",
       "2012-08-07     4  1128.8   -0.2   -0.02       2.820        0.01       2.680   \n",
       "2012-08-08     5  1128.3   -0.5   -0.04       2.830        0.01       2.700   \n",
       "...          ...     ...    ...     ...         ...         ...         ...   \n",
       "2022-07-25  2455  1313.7    0.7    0.05       3.165       -0.04       3.235   \n",
       "2022-07-26  2456  1307.6   -6.1   -0.47       3.155       -0.01       3.215   \n",
       "2022-07-27  2457  1313.3    5.7    0.43       3.145       -0.01       3.165   \n",
       "2022-07-28  2458  1296.1  -17.2   -1.33       3.175        0.03       3.205   \n",
       "2022-07-29  2459  1299.1    3.0    0.23       3.105       -0.07       3.065   \n",
       "\n",
       "            2Y_전일비_irs  3Y_Mid_irs  3Y_전일비_irs  ...  국고10년대비  통안1년대비  통안2년대비  \\\n",
       "DateTime                                        ...                            \n",
       "2012-08-02       -0.05       2.690       -0.05  ...     0.00   -0.01    0.06   \n",
       "2012-08-03       -0.03       2.660       -0.03  ...     0.07    0.01    0.07   \n",
       "2012-08-06        0.02       2.680        0.02  ...    -0.04    0.00   -0.04   \n",
       "2012-08-07        0.00       2.680        0.00  ...     0.07   -0.01    0.10   \n",
       "2012-08-08        0.02       2.700        0.02  ...    -0.06    0.01    0.02   \n",
       "...                ...         ...         ...  ...      ...     ...     ...   \n",
       "2022-07-25       -0.08       3.205       -0.07  ...     0.00    0.00   -0.02   \n",
       "2022-07-26       -0.02       3.175       -0.03  ...    -0.05   -0.02   -0.03   \n",
       "2022-07-27       -0.05       3.115       -0.06  ...     0.02    0.00    0.01   \n",
       "2022-07-28        0.04       3.165        0.05  ...     0.01   -0.01    0.01   \n",
       "2022-07-29       -0.14       3.025       -0.14  ...     0.01    0.03    0.00   \n",
       "\n",
       "            전일비_1Y_베이시스  전일비_2Y_베이시스  전일비_3Y_베이시스  전일비_5Y_베이시스  전일비_10Y_베이시스  \\\n",
       "DateTime                                                                       \n",
       "2012-08-02          2.0          8.0          9.0          9.0           9.0   \n",
       "2012-08-03          2.0          1.5          1.0         -5.0         -13.0   \n",
       "2012-08-06         -2.0         -4.5         -5.0         -6.0          -5.0   \n",
       "2012-08-07          1.0          1.5          0.0         -8.0         -10.0   \n",
       "2012-08-08          0.0         -2.0         -2.0         -4.0          -7.0   \n",
       "...                 ...          ...          ...          ...           ...   \n",
       "2022-07-25         -4.0         -1.0          0.0         -2.0           0.0   \n",
       "2022-07-26          2.0         -1.0          0.0          1.0           1.0   \n",
       "2022-07-27          2.0          4.0          5.0          5.0           5.0   \n",
       "2022-07-28          1.0          0.0         -1.0         -2.0          -5.0   \n",
       "2022-07-29         -4.0         -3.0         -1.0          5.0           3.0   \n",
       "\n",
       "            전날 종가_ex  종가_NDF차이  \n",
       "DateTime                        \n",
       "2012-08-02    1126.5     -7.50  \n",
       "2012-08-03    1131.7     -6.30  \n",
       "2012-08-06    1134.8      6.30  \n",
       "2012-08-07    1129.0      0.00  \n",
       "2012-08-08    1128.8     -1.45  \n",
       "...              ...       ...  \n",
       "2022-07-25    1313.0      3.15  \n",
       "2022-07-26    1313.7      2.70  \n",
       "2022-07-27    1307.6     -2.90  \n",
       "2022-07-28    1313.3      7.30  \n",
       "2022-07-29    1296.1      0.35  \n",
       "\n",
       "[2459 rows x 65 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 파일 불러오기\n",
    "df = pd.read_excel(\"./xlsx/시차상관분석6Data.xlsx\", index_col = 0)    \n",
    "\n",
    "\n",
    "df = df.set_index(\"DateTime\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['열1', '종가_ex', '대비_ex', '증감률_ex', '1Y_Mid_irs', '1Y_전일비_irs',\n",
       "       '2Y_Mid_irs', '2Y_전일비_irs', '3Y_Mid_irs', '3Y_전일비_irs', '5Y_Mid_irs',\n",
       "       '5Y_전일비_irs', '10Y_Mid_irs', '10Y_전일비_irs', '1Y_Mid_crs', '1Y_전일비_crs',\n",
       "       '2Y_Mid_crs', '2Y_전일비_crs', '3Y_Mid_crs', '3Y_전일비_crs', '5Y_Mid_crs',\n",
       "       '5Y_전일비_crs', '10Y_Mid_crs', '10Y_전일비_crs', '국고1년', '국고3년', '국고5년',\n",
       "       '국고10년', '통안364일', '통안2년', 'Bid_ndf', 'Ask_ndf', 'Mid_ndf', '전일비_ndf',\n",
       "       '1Y_베이시스', '2Y_베이시스', '3Y_베이시스', '5Y_베이시스', '10Y_베이시스', 'M1_스왑포인트',\n",
       "       '전일대비_종가_ex', '등락률_종가_ex', '전일비_1Y_irs', '전일비_2Y_irs', '전일비_3Y_irs',\n",
       "       '전일비_5Y_irs', '전일비_10Y_irs', '전일비_1Y_crs', '전일비_2Y_crs', '전일비_3Y_crs',\n",
       "       '전일비_5Y_crs', '전일비_10Y_crs', '국고1년대비', '국고3년대비', '국고5년대비', '국고10년대비',\n",
       "       '통안1년대비', '통안2년대비', '전일비_1Y_베이시스', '전일비_2Y_베이시스', '전일비_3Y_베이시스',\n",
       "       '전일비_5Y_베이시스', '전일비_10Y_베이시스', '전날 종가_ex', '종가_NDF차이'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['대비_irs_1Y'] = df['1Y_Mid_irs'] - df['1Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_2Y'] = df['2Y_Mid_irs'] - df['2Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_3Y'] = df['3Y_Mid_irs'] - df['3Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_5Y'] = df['5Y_Mid_irs'] - df['5Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_10Y'] = df['10Y_Mid_irs'] - df['10Y_Mid_irs'].shift(1) \n",
    "\n",
    "df['대비_crs_1Y'] = df['1Y_Mid_crs'] - df['1Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_2Y'] = df['2Y_Mid_crs'] - df['2Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_3Y'] = df['3Y_Mid_crs'] - df['3Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_5Y'] = df['5Y_Mid_crs'] - df['5Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_10Y'] = df['10Y_Mid_crs'] - df['10Y_Mid_crs'].shift(1)\n",
    "\n",
    "df['대비_swapbasis_1Y'] = df['1Y_베이시스']-df['1Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_2Y'] = df['2Y_베이시스']-df['2Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_3Y'] = df['3Y_베이시스']-df['3Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_5Y'] = df['5Y_베이시스']-df['5Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_10Y'] = df['10Y_베이시스']-df['10Y_베이시스'].shift(1)\n",
    "\n",
    "df['대비_국고_1Y'] = df['국고1년']-df['국고1년'].shift(1)\n",
    "df['대비_국고_3Y'] = df['국고3년']-df['국고3년'].shift(1)\n",
    "df['대비_국고_5Y'] = df['국고5년']-df['국고5년'].shift(1)\n",
    "df['대비_국고_10Y'] = df['국고10년']-df['국고10년'].shift(1)\n",
    "\n",
    "df['대비_통안_1Y'] = df['통안364일']-df['통안364일'].shift(1)\n",
    "df['대비_통안_2Y'] = df['통안2년']-df['통안2년'].shift(1)\n",
    "\n",
    "df['대비_ndf'] = df['Mid_ndf']-df['Mid_ndf'].shift(1)\n",
    "df['스왑포인트_1M'] = df[\"M1_스왑포인트\"]/100 \n",
    "df['전일종가_ex'] = df['종가_ex'].shift(1)\n",
    "df['종가_NDF_차이'] = df['전일종가_ex'] - df['Mid_ndf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 칼럼만 추출\n",
    "df_1 = df[['대비_irs_1Y', '대비_irs_2Y', '대비_irs_3Y', '대비_irs_5Y', '대비_irs_10Y',\n",
    "           '대비_crs_1Y', '대비_crs_2Y', '대비_crs_3Y', '대비_crs_5Y', '대비_crs_10Y', \n",
    "           '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y', '대비_swapbasis_10Y',\n",
    "           '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y', \n",
    "           '대비_통안_1Y', '대비_통안_2Y', '대비_ndf', '스왑포인트_1M', '전일종가_ex', \n",
    "           '종가_ex', '종가_NDF_차이' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['대비_irs_1Y', '대비_irs_2Y', '대비_irs_3Y', '대비_irs_5Y', '대비_irs_10Y',\n",
       "       '대비_crs_1Y', '대비_crs_2Y', '대비_crs_3Y', '대비_crs_5Y', '대비_crs_10Y',\n",
       "       '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y',\n",
       "       '대비_swapbasis_5Y', '대비_swapbasis_10Y', '대비_국고_1Y', '대비_국고_3Y',\n",
       "       '대비_국고_5Y', '대비_국고_10Y', '대비_통안_1Y', '대비_통안_2Y', '대비_ndf', '스왑포인트_1M',\n",
       "       '전일종가_ex', '종가_ex', '종가_NDF_차이'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "대비_irs_1Y           0\n",
       "대비_irs_2Y           0\n",
       "대비_irs_3Y           0\n",
       "대비_irs_5Y           0\n",
       "대비_irs_10Y          0\n",
       "대비_crs_1Y           0\n",
       "대비_crs_2Y           0\n",
       "대비_crs_3Y           0\n",
       "대비_crs_5Y           0\n",
       "대비_crs_10Y          0\n",
       "대비_swapbasis_1Y     0\n",
       "대비_swapbasis_2Y     0\n",
       "대비_swapbasis_3Y     0\n",
       "대비_swapbasis_5Y     0\n",
       "대비_swapbasis_10Y    0\n",
       "대비_국고_1Y            0\n",
       "대비_국고_3Y            0\n",
       "대비_국고_5Y            0\n",
       "대비_국고_10Y           0\n",
       "대비_통안_1Y            0\n",
       "대비_통안_2Y            0\n",
       "대비_ndf              0\n",
       "스왑포인트_1M            0\n",
       "전일종가_ex             0\n",
       "종가_ex               0\n",
       "종가_NDF_차이           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_irs_1Y</th>\n",
       "      <th>대비_irs_2Y</th>\n",
       "      <th>대비_irs_3Y</th>\n",
       "      <th>대비_irs_5Y</th>\n",
       "      <th>대비_irs_10Y</th>\n",
       "      <th>대비_crs_1Y</th>\n",
       "      <th>대비_crs_2Y</th>\n",
       "      <th>대비_crs_3Y</th>\n",
       "      <th>대비_crs_5Y</th>\n",
       "      <th>대비_crs_10Y</th>\n",
       "      <th>대비_국고_1Y</th>\n",
       "      <th>대비_국고_3Y</th>\n",
       "      <th>대비_국고_5Y</th>\n",
       "      <th>대비_국고_10Y</th>\n",
       "      <th>대비_통안_1Y</th>\n",
       "      <th>대비_통안_2Y</th>\n",
       "      <th>대비_ndf</th>\n",
       "      <th>스왑포인트_1M</th>\n",
       "      <th>전일종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-0.848159</td>\n",
       "      <td>-0.732099</td>\n",
       "      <td>-0.562745</td>\n",
       "      <td>-0.628439</td>\n",
       "      <td>-0.403644</td>\n",
       "      <td>-0.205698</td>\n",
       "      <td>-0.364180</td>\n",
       "      <td>-0.462791</td>\n",
       "      <td>-1.872418</td>\n",
       "      <td>-3.115253</td>\n",
       "      <td>-1.133777</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>-1.798842</td>\n",
       "      <td>-0.217667</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>0.686282</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.056282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>0.559997</td>\n",
       "      <td>0.481892</td>\n",
       "      <td>0.370922</td>\n",
       "      <td>0.415773</td>\n",
       "      <td>0.202253</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>-0.602348</td>\n",
       "      <td>-0.690892</td>\n",
       "      <td>-0.939341</td>\n",
       "      <td>-0.833148</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>0.159979</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>0.123726</td>\n",
       "      <td>-1.668663</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.000487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.278366</td>\n",
       "      <td>-0.003704</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>0.206931</td>\n",
       "      <td>0.404219</td>\n",
       "      <td>0.401029</td>\n",
       "      <td>0.350324</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-1.639149</td>\n",
       "      <td>-1.663004</td>\n",
       "      <td>-0.568154</td>\n",
       "      <td>-0.001379</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>0.075741</td>\n",
       "      <td>1.911215</td>\n",
       "      <td>-0.104877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.481892</td>\n",
       "      <td>0.370922</td>\n",
       "      <td>0.415773</td>\n",
       "      <td>0.606184</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.006928</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-0.472802</td>\n",
       "      <td>-0.833148</td>\n",
       "      <td>-0.568154</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>-0.514104</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>0.206571</td>\n",
       "      <td>1.820638</td>\n",
       "      <td>-0.108476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-09</th>\n",
       "      <td>1.404890</td>\n",
       "      <td>1.453085</td>\n",
       "      <td>1.117855</td>\n",
       "      <td>1.042301</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>1.816727</td>\n",
       "      <td>0.945744</td>\n",
       "      <td>0.905815</td>\n",
       "      <td>0.926814</td>\n",
       "      <td>0.826565</td>\n",
       "      <td>2.825583</td>\n",
       "      <td>0.966767</td>\n",
       "      <td>1.351270</td>\n",
       "      <td>1.284529</td>\n",
       "      <td>0.432293</td>\n",
       "      <td>0.747946</td>\n",
       "      <td>0.381012</td>\n",
       "      <td>1.775350</td>\n",
       "      <td>-0.117475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-1.129790</td>\n",
       "      <td>-1.946090</td>\n",
       "      <td>-1.309678</td>\n",
       "      <td>-1.881494</td>\n",
       "      <td>-2.221333</td>\n",
       "      <td>-1.621396</td>\n",
       "      <td>-2.150439</td>\n",
       "      <td>-1.603295</td>\n",
       "      <td>-2.572226</td>\n",
       "      <td>-2.285397</td>\n",
       "      <td>-1.133777</td>\n",
       "      <td>-0.969524</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>-2.312737</td>\n",
       "      <td>-0.109340</td>\n",
       "      <td>-0.625337</td>\n",
       "      <td>0.572896</td>\n",
       "      <td>-0.896666</td>\n",
       "      <td>3.206786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>-0.284897</td>\n",
       "      <td>-0.489301</td>\n",
       "      <td>-0.562745</td>\n",
       "      <td>-0.628439</td>\n",
       "      <td>-0.605609</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.721432</td>\n",
       "      <td>-0.690892</td>\n",
       "      <td>-0.472802</td>\n",
       "      <td>-0.418220</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>-0.771052</td>\n",
       "      <td>0.107313</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>0.189127</td>\n",
       "      <td>-0.987243</td>\n",
       "      <td>3.219385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>-0.284897</td>\n",
       "      <td>-1.217695</td>\n",
       "      <td>-1.122944</td>\n",
       "      <td>-1.254966</td>\n",
       "      <td>-1.211506</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.245096</td>\n",
       "      <td>-0.234690</td>\n",
       "      <td>-0.239533</td>\n",
       "      <td>-0.210756</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-1.350390</td>\n",
       "      <td>-1.541894</td>\n",
       "      <td>0.215640</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>-0.098699</td>\n",
       "      <td>-0.851378</td>\n",
       "      <td>3.109596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.841628</td>\n",
       "      <td>0.967489</td>\n",
       "      <td>0.931121</td>\n",
       "      <td>1.042301</td>\n",
       "      <td>1.414046</td>\n",
       "      <td>0.805515</td>\n",
       "      <td>0.945744</td>\n",
       "      <td>0.905815</td>\n",
       "      <td>0.693544</td>\n",
       "      <td>0.411636</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>0.644052</td>\n",
       "      <td>0.810938</td>\n",
       "      <td>2.055371</td>\n",
       "      <td>0.215640</td>\n",
       "      <td>0.373414</td>\n",
       "      <td>-0.796461</td>\n",
       "      <td>-0.941955</td>\n",
       "      <td>3.212186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-1.974684</td>\n",
       "      <td>-3.402879</td>\n",
       "      <td>-2.616811</td>\n",
       "      <td>-2.925706</td>\n",
       "      <td>-2.423299</td>\n",
       "      <td>-2.228124</td>\n",
       "      <td>-4.055782</td>\n",
       "      <td>-3.428103</td>\n",
       "      <td>-2.105687</td>\n",
       "      <td>-1.870468</td>\n",
       "      <td>-2.265023</td>\n",
       "      <td>-2.099027</td>\n",
       "      <td>-3.241553</td>\n",
       "      <td>-2.055789</td>\n",
       "      <td>-0.325994</td>\n",
       "      <td>-1.374400</td>\n",
       "      <td>-1.799493</td>\n",
       "      <td>-0.896666</td>\n",
       "      <td>2.902617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2458 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_irs_1Y  대비_irs_2Y  대비_irs_3Y  대비_irs_5Y  대비_irs_10Y  대비_crs_1Y  \\\n",
       "DateTime                                                                        \n",
       "2012-08-03  -0.848159  -0.732099  -0.562745  -0.628439   -0.403644  -0.205698   \n",
       "2012-08-06   0.559997   0.481892   0.370922   0.415773    0.202253  -0.003456   \n",
       "2012-08-07   0.278366  -0.003704  -0.002545   0.206931    0.404219   0.401029   \n",
       "2012-08-08   0.278366   0.481892   0.370922   0.415773    0.606184   0.198787   \n",
       "2012-08-09   1.404890   1.453085   1.117855   1.042301    0.808150   1.816727   \n",
       "...               ...        ...        ...        ...         ...        ...   \n",
       "2022-07-25  -1.129790  -1.946090  -1.309678  -1.881494   -2.221333  -1.621396   \n",
       "2022-07-26  -0.284897  -0.489301  -0.562745  -0.628439   -0.605609   0.198787   \n",
       "2022-07-27  -0.284897  -1.217695  -1.122944  -1.254966   -1.211506   0.198787   \n",
       "2022-07-28   0.841628   0.967489   0.931121   1.042301    1.414046   0.805515   \n",
       "2022-07-29  -1.974684  -3.402879  -2.616811  -2.925706   -2.423299  -2.228124   \n",
       "\n",
       "            대비_crs_2Y  대비_crs_3Y  대비_crs_5Y  대비_crs_10Y  대비_국고_1Y  대비_국고_3Y  \\\n",
       "DateTime                                                                      \n",
       "2012-08-03  -0.364180  -0.462791  -1.872418   -3.115253 -1.133777 -0.324094   \n",
       "2012-08-06  -0.602348  -0.690892  -0.939341   -0.833148  0.563092  0.159979   \n",
       "2012-08-07   0.350324  -0.006589  -1.639149   -1.663004 -0.568154 -0.001379   \n",
       "2012-08-08  -0.006928  -0.006589  -0.472802   -0.833148 -0.568154 -0.324094   \n",
       "2012-08-09   0.945744   0.905815   0.926814    0.826565  2.825583  0.966767   \n",
       "...               ...        ...        ...         ...       ...       ...   \n",
       "2022-07-25  -2.150439  -1.603295  -2.572226   -2.285397 -1.133777 -0.969524   \n",
       "2022-07-26  -0.721432  -0.690892  -0.472802   -0.418220  0.563092 -0.485451   \n",
       "2022-07-27  -0.245096  -0.234690  -0.239533   -0.210756 -0.002531 -0.485451   \n",
       "2022-07-28   0.945744   0.905815   0.693544    0.411636  0.563092  0.644052   \n",
       "2022-07-29  -4.055782  -3.428103  -2.105687   -1.870468 -2.265023 -2.099027   \n",
       "\n",
       "            대비_국고_5Y  대비_국고_10Y  대비_통안_1Y  대비_통안_2Y    대비_ndf  스왑포인트_1M  \\\n",
       "DateTime                                                                  \n",
       "2012-08-03 -1.890723  -1.798842 -0.217667 -0.125961  0.686282  1.820638   \n",
       "2012-08-06  0.000440  -0.000209  0.107313  0.123726 -1.668663  1.820638   \n",
       "2012-08-07  0.000440  -0.000209 -0.109340 -0.125961  0.075741  1.911215   \n",
       "2012-08-08 -0.539892  -0.514104 -0.109340 -0.125961  0.206571  1.820638   \n",
       "2012-08-09  1.351270   1.284529  0.432293  0.747946  0.381012  1.775350   \n",
       "...              ...        ...       ...       ...       ...       ...   \n",
       "2022-07-25 -1.890723  -2.312737 -0.109340 -0.625337  0.572896 -0.896666   \n",
       "2022-07-26 -0.539892  -0.771052  0.107313 -0.001117  0.189127 -0.987243   \n",
       "2022-07-27 -1.350390  -1.541894  0.215640 -0.125961 -0.098699 -0.851378   \n",
       "2022-07-28  0.810938   2.055371  0.215640  0.373414 -0.796461 -0.941955   \n",
       "2022-07-29 -3.241553  -2.055789 -0.325994 -1.374400 -1.799493 -0.896666   \n",
       "\n",
       "             전일종가_ex  \n",
       "DateTime              \n",
       "2012-08-03 -0.056282  \n",
       "2012-08-06 -0.000487  \n",
       "2012-08-07 -0.104877  \n",
       "2012-08-08 -0.108476  \n",
       "2012-08-09 -0.117475  \n",
       "...              ...  \n",
       "2022-07-25  3.206786  \n",
       "2022-07-26  3.219385  \n",
       "2022-07-27  3.109596  \n",
       "2022-07-28  3.212186  \n",
       "2022-07-29  2.902617  \n",
       "\n",
       "[2458 rows x 19 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df_1[['대비_irs_1Y', '대비_irs_2Y', '대비_irs_3Y', '대비_irs_5Y', '대비_irs_10Y',\n",
    "       '대비_crs_1Y', '대비_crs_2Y', '대비_crs_3Y', '대비_crs_5Y', '대비_crs_10Y',\n",
    "       '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y', '대비_통안_1Y', '대비_통안_2Y', '대비_ndf', '스왑포인트_1M',\n",
    "       '전일종가_ex']]\n",
    "y = df_1['종가_ex']\n",
    "\n",
    "x.feature = x.columns \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor     Feature\n",
      "0     9.960819   대비_irs_1Y\n",
      "1    19.499639   대비_irs_2Y\n",
      "2     2.662912   대비_irs_3Y\n",
      "3     6.466557   대비_irs_5Y\n",
      "4     5.879774  대비_irs_10Y\n",
      "5     2.654539   대비_crs_1Y\n",
      "6     5.273229   대비_crs_2Y\n",
      "7     3.830250   대비_crs_3Y\n",
      "8     6.708559   대비_crs_5Y\n",
      "9     3.721361  대비_crs_10Y\n",
      "10    1.925744    대비_국고_1Y\n",
      "11    1.253328    대비_국고_3Y\n",
      "12    6.296529    대비_국고_5Y\n",
      "13    5.054776   대비_국고_10Y\n",
      "14    1.025470    대비_통안_1Y\n",
      "15    1.115772    대비_통안_2Y\n",
      "16    1.043977      대비_ndf\n",
      "17    1.169521    스왑포인트_1M\n",
      "18    1.182302     전일종가_ex\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.639e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:04:23</td>     <th>  Log-Likelihood:    </th> <td> -7396.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.483e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2438</td>      <th>  BIC:               </th> <td>1.495e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.099</td> <td> 1.14e+04</td> <td> 0.000</td> <td> 1134.700</td> <td> 1135.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.1486</td> <td>    0.314</td> <td>    0.474</td> <td> 0.636</td> <td>   -0.466</td> <td>    0.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_2Y</th>  <td>    0.4709</td> <td>    0.439</td> <td>    1.073</td> <td> 0.283</td> <td>   -0.389</td> <td>    1.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_3Y</th>  <td>    0.0975</td> <td>    0.162</td> <td>    0.601</td> <td> 0.548</td> <td>   -0.220</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_5Y</th>  <td>   -0.3038</td> <td>    0.253</td> <td>   -1.203</td> <td> 0.229</td> <td>   -0.799</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.4588</td> <td>    0.241</td> <td>   -1.905</td> <td> 0.057</td> <td>   -0.931</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -1.0214</td> <td>    0.162</td> <td>   -6.310</td> <td> 0.000</td> <td>   -1.339</td> <td>   -0.704</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_2Y</th>  <td>   -0.0420</td> <td>    0.228</td> <td>   -0.184</td> <td> 0.854</td> <td>   -0.489</td> <td>    0.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.3783</td> <td>    0.194</td> <td>   -1.946</td> <td> 0.052</td> <td>   -0.760</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_5Y</th>  <td>    0.0632</td> <td>    0.257</td> <td>    0.246</td> <td> 0.806</td> <td>   -0.441</td> <td>    0.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_10Y</th> <td>    0.0182</td> <td>    0.192</td> <td>    0.095</td> <td> 0.924</td> <td>   -0.358</td> <td>    0.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>   <td>    0.1709</td> <td>    0.138</td> <td>    1.239</td> <td> 0.215</td> <td>   -0.099</td> <td>    0.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.2648</td> <td>    0.111</td> <td>   -2.381</td> <td> 0.017</td> <td>   -0.483</td> <td>   -0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.5378</td> <td>    0.249</td> <td>    2.157</td> <td> 0.031</td> <td>    0.049</td> <td>    1.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>  <td>   -0.1287</td> <td>    0.223</td> <td>   -0.576</td> <td> 0.565</td> <td>   -0.567</td> <td>    0.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_1Y</th>   <td>   -0.0409</td> <td>    0.101</td> <td>   -0.406</td> <td> 0.685</td> <td>   -0.238</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>   <td>    0.0535</td> <td>    0.105</td> <td>    0.510</td> <td> 0.610</td> <td>   -0.152</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>     <td>    2.3945</td> <td>    0.102</td> <td>   23.589</td> <td> 0.000</td> <td>    2.195</td> <td>    2.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -0.1141</td> <td>    0.107</td> <td>   -1.062</td> <td> 0.288</td> <td>   -0.325</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.3182</td> <td>    0.108</td> <td>  512.087</td> <td> 0.000</td> <td>   55.106</td> <td>   55.530</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>50.171</td> <th>  Durbin-Watson:     </th> <td>   2.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 103.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.046</td> <th>  Prob(JB):          </th> <td>3.05e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.002</td> <th>  Cond. No.          </th> <td>    13.2</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 1.639e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        21:04:23   Log-Likelihood:                -7396.8\n",
       "No. Observations:                2458   AIC:                         1.483e+04\n",
       "Df Residuals:                    2438   BIC:                         1.495e+04\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.099   1.14e+04      0.000    1134.700    1135.090\n",
       "대비_irs_1Y      0.1486      0.314      0.474      0.636      -0.466       0.763\n",
       "대비_irs_2Y      0.4709      0.439      1.073      0.283      -0.389       1.331\n",
       "대비_irs_3Y      0.0975      0.162      0.601      0.548      -0.220       0.415\n",
       "대비_irs_5Y     -0.3038      0.253     -1.203      0.229      -0.799       0.192\n",
       "대비_irs_10Y    -0.4588      0.241     -1.905      0.057      -0.931       0.014\n",
       "대비_crs_1Y     -1.0214      0.162     -6.310      0.000      -1.339      -0.704\n",
       "대비_crs_2Y     -0.0420      0.228     -0.184      0.854      -0.489       0.405\n",
       "대비_crs_3Y     -0.3783      0.194     -1.946      0.052      -0.760       0.003\n",
       "대비_crs_5Y      0.0632      0.257      0.246      0.806      -0.441       0.568\n",
       "대비_crs_10Y     0.0182      0.192      0.095      0.924      -0.358       0.394\n",
       "대비_국고_1Y       0.1709      0.138      1.239      0.215      -0.099       0.441\n",
       "대비_국고_3Y      -0.2648      0.111     -2.381      0.017      -0.483      -0.047\n",
       "대비_국고_5Y       0.5378      0.249      2.157      0.031       0.049       1.027\n",
       "대비_국고_10Y     -0.1287      0.223     -0.576      0.565      -0.567       0.309\n",
       "대비_통안_1Y      -0.0409      0.101     -0.406      0.685      -0.238       0.156\n",
       "대비_통안_2Y       0.0535      0.105      0.510      0.610      -0.152       0.259\n",
       "대비_ndf         2.3945      0.102     23.589      0.000       2.195       2.594\n",
       "스왑포인트_1M      -0.1141      0.107     -1.062      0.288      -0.325       0.097\n",
       "전일종가_ex       55.3182      0.108    512.087      0.000      55.106      55.530\n",
       "==============================================================================\n",
       "Omnibus:                       50.171   Durbin-Watson:                   2.645\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.688\n",
       "Skew:                           0.046   Prob(JB):                     3.05e-23\n",
       "Kurtosis:                       4.002   Cond. No.                         13.2\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor     Feature\n",
      "0     3.929542   대비_irs_1Y\n",
      "1     2.552218   대비_irs_3Y\n",
      "2     5.801732   대비_irs_5Y\n",
      "3     5.223393  대비_irs_10Y\n",
      "4     2.651597   대비_crs_1Y\n",
      "5     5.261693   대비_crs_2Y\n",
      "6     3.827802   대비_crs_3Y\n",
      "7     6.702977   대비_crs_5Y\n",
      "8     3.721215  대비_crs_10Y\n",
      "9     1.914822    대비_국고_1Y\n",
      "10    1.247324    대비_국고_3Y\n",
      "11    6.210527    대비_국고_5Y\n",
      "12    5.040890   대비_국고_10Y\n",
      "13    1.025101    대비_통안_1Y\n",
      "14    1.115085    대비_통안_2Y\n",
      "15    1.043728      대비_ndf\n",
      "16    1.169457    스왑포인트_1M\n",
      "17    1.181526     전일종가_ex\n"
     ]
    }
   ],
   "source": [
    "x_scaled.drop(['대비_irs_2Y'], axis=1, inplace=True)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.730e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:17:43</td>     <th>  Log-Likelihood:    </th> <td> -7397.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.483e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2439</td>      <th>  BIC:               </th> <td>1.494e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    18</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.099</td> <td> 1.14e+04</td> <td> 0.000</td> <td> 1134.700</td> <td> 1135.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.4105</td> <td>    0.197</td> <td>    2.084</td> <td> 0.037</td> <td>    0.024</td> <td>    0.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_3Y</th>  <td>    0.1330</td> <td>    0.159</td> <td>    0.838</td> <td> 0.402</td> <td>   -0.178</td> <td>    0.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_5Y</th>  <td>   -0.2169</td> <td>    0.239</td> <td>   -0.906</td> <td> 0.365</td> <td>   -0.686</td> <td>    0.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.3724</td> <td>    0.227</td> <td>   -1.640</td> <td> 0.101</td> <td>   -0.818</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -1.0272</td> <td>    0.162</td> <td>   -6.349</td> <td> 0.000</td> <td>   -1.344</td> <td>   -0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_2Y</th>  <td>   -0.0306</td> <td>    0.228</td> <td>   -0.134</td> <td> 0.893</td> <td>   -0.477</td> <td>    0.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.3836</td> <td>    0.194</td> <td>   -1.974</td> <td> 0.049</td> <td>   -0.765</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_5Y</th>  <td>    0.0712</td> <td>    0.257</td> <td>    0.277</td> <td> 0.782</td> <td>   -0.433</td> <td>    0.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_10Y</th> <td>    0.0195</td> <td>    0.192</td> <td>    0.102</td> <td> 0.919</td> <td>   -0.356</td> <td>    0.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>   <td>    0.1820</td> <td>    0.137</td> <td>    1.324</td> <td> 0.186</td> <td>   -0.088</td> <td>    0.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.2566</td> <td>    0.111</td> <td>   -2.312</td> <td> 0.021</td> <td>   -0.474</td> <td>   -0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.5690</td> <td>    0.248</td> <td>    2.298</td> <td> 0.022</td> <td>    0.084</td> <td>    1.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>  <td>   -0.1412</td> <td>    0.223</td> <td>   -0.633</td> <td> 0.527</td> <td>   -0.579</td> <td>    0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_1Y</th>   <td>   -0.0429</td> <td>    0.101</td> <td>   -0.427</td> <td> 0.670</td> <td>   -0.240</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>   <td>    0.0563</td> <td>    0.105</td> <td>    0.537</td> <td> 0.592</td> <td>   -0.149</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>     <td>    2.3961</td> <td>    0.102</td> <td>   23.607</td> <td> 0.000</td> <td>    2.197</td> <td>    2.595</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -0.1133</td> <td>    0.107</td> <td>   -1.054</td> <td> 0.292</td> <td>   -0.324</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.3152</td> <td>    0.108</td> <td>  512.211</td> <td> 0.000</td> <td>   55.103</td> <td>   55.527</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>50.651</td> <th>  Durbin-Watson:     </th> <td>   2.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 104.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.050</td> <th>  Prob(JB):          </th> <td>1.77e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.006</td> <th>  Cond. No.          </th> <td>    7.56</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 1.730e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        19:17:43   Log-Likelihood:                -7397.4\n",
       "No. Observations:                2458   AIC:                         1.483e+04\n",
       "Df Residuals:                    2439   BIC:                         1.494e+04\n",
       "Df Model:                          18                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.099   1.14e+04      0.000    1134.700    1135.090\n",
       "대비_irs_1Y      0.4105      0.197      2.084      0.037       0.024       0.797\n",
       "대비_irs_3Y      0.1330      0.159      0.838      0.402      -0.178       0.444\n",
       "대비_irs_5Y     -0.2169      0.239     -0.906      0.365      -0.686       0.252\n",
       "대비_irs_10Y    -0.3724      0.227     -1.640      0.101      -0.818       0.073\n",
       "대비_crs_1Y     -1.0272      0.162     -6.349      0.000      -1.344      -0.710\n",
       "대비_crs_2Y     -0.0306      0.228     -0.134      0.893      -0.477       0.416\n",
       "대비_crs_3Y     -0.3836      0.194     -1.974      0.049      -0.765      -0.002\n",
       "대비_crs_5Y      0.0712      0.257      0.277      0.782      -0.433       0.576\n",
       "대비_crs_10Y     0.0195      0.192      0.102      0.919      -0.356       0.395\n",
       "대비_국고_1Y       0.1820      0.137      1.324      0.186      -0.088       0.452\n",
       "대비_국고_3Y      -0.2566      0.111     -2.312      0.021      -0.474      -0.039\n",
       "대비_국고_5Y       0.5690      0.248      2.298      0.022       0.084       1.055\n",
       "대비_국고_10Y     -0.1412      0.223     -0.633      0.527      -0.579       0.296\n",
       "대비_통안_1Y      -0.0429      0.101     -0.427      0.670      -0.240       0.154\n",
       "대비_통안_2Y       0.0563      0.105      0.537      0.592      -0.149       0.262\n",
       "대비_ndf         2.3961      0.102     23.607      0.000       2.197       2.595\n",
       "스왑포인트_1M      -0.1133      0.107     -1.054      0.292      -0.324       0.097\n",
       "전일종가_ex       55.3152      0.108    512.211      0.000      55.103      55.527\n",
       "==============================================================================\n",
       "Omnibus:                       50.651   Durbin-Watson:                   2.645\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              104.772\n",
       "Skew:                           0.050   Prob(JB):                     1.77e-23\n",
       "Kurtosis:                       4.006   Cond. No.                         7.56\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor     Feature\n",
      "0     3.617862   대비_irs_1Y\n",
      "1     5.680738   대비_irs_5Y\n",
      "2     5.106217  대비_irs_10Y\n",
      "3     2.649939   대비_crs_1Y\n",
      "4     5.260346   대비_crs_2Y\n",
      "5     3.826791   대비_crs_3Y\n",
      "6     6.700379   대비_crs_5Y\n",
      "7     3.720405  대비_crs_10Y\n",
      "8     1.899971    대비_국고_1Y\n",
      "9     1.246667    대비_국고_3Y\n",
      "10    6.204460    대비_국고_5Y\n",
      "11    5.040807   대비_국고_10Y\n",
      "12    1.114850    대비_통안_2Y\n",
      "13    1.042961      대비_ndf\n",
      "14    1.169440    스왑포인트_1M\n",
      "15    1.181295     전일종가_ex\n"
     ]
    }
   ],
   "source": [
    "x_scaled.drop(['대비_irs_3Y', '대비_통안_1Y'], axis=1, inplace=True)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.947e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:17:48</td>     <th>  Log-Likelihood:    </th> <td> -7397.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.483e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2441</td>      <th>  BIC:               </th> <td>1.493e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    16</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.099</td> <td> 1.14e+04</td> <td> 0.000</td> <td> 1134.700</td> <td> 1135.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.4557</td> <td>    0.189</td> <td>    2.412</td> <td> 0.016</td> <td>    0.085</td> <td>    0.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_5Y</th>  <td>   -0.1879</td> <td>    0.237</td> <td>   -0.794</td> <td> 0.427</td> <td>   -0.652</td> <td>    0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.3429</td> <td>    0.224</td> <td>   -1.528</td> <td> 0.127</td> <td>   -0.783</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -1.0308</td> <td>    0.162</td> <td>   -6.375</td> <td> 0.000</td> <td>   -1.348</td> <td>   -0.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_2Y</th>  <td>   -0.0279</td> <td>    0.228</td> <td>   -0.123</td> <td> 0.902</td> <td>   -0.475</td> <td>    0.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.3861</td> <td>    0.194</td> <td>   -1.987</td> <td> 0.047</td> <td>   -0.767</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_5Y</th>  <td>    0.0755</td> <td>    0.257</td> <td>    0.294</td> <td> 0.769</td> <td>   -0.429</td> <td>    0.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_10Y</th> <td>    0.0219</td> <td>    0.192</td> <td>    0.114</td> <td> 0.909</td> <td>   -0.354</td> <td>    0.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>   <td>    0.1825</td> <td>    0.137</td> <td>    1.333</td> <td> 0.183</td> <td>   -0.086</td> <td>    0.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.2548</td> <td>    0.111</td> <td>   -2.297</td> <td> 0.022</td> <td>   -0.472</td> <td>   -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.5714</td> <td>    0.247</td> <td>    2.310</td> <td> 0.021</td> <td>    0.086</td> <td>    1.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>  <td>   -0.1421</td> <td>    0.223</td> <td>   -0.637</td> <td> 0.524</td> <td>   -0.579</td> <td>    0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>   <td>    0.0569</td> <td>    0.105</td> <td>    0.542</td> <td> 0.588</td> <td>   -0.149</td> <td>    0.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>     <td>    2.3936</td> <td>    0.101</td> <td>   23.596</td> <td> 0.000</td> <td>    2.195</td> <td>    2.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -0.1134</td> <td>    0.107</td> <td>   -1.055</td> <td> 0.291</td> <td>   -0.324</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.3138</td> <td>    0.108</td> <td>  512.365</td> <td> 0.000</td> <td>   55.102</td> <td>   55.526</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>50.746</td> <th>  Durbin-Watson:     </th> <td>   2.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 105.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.050</td> <th>  Prob(JB):          </th> <td>1.52e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.008</td> <th>  Cond. No.          </th> <td>    7.29</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 1.947e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        19:17:48   Log-Likelihood:                -7397.8\n",
       "No. Observations:                2458   AIC:                         1.483e+04\n",
       "Df Residuals:                    2441   BIC:                         1.493e+04\n",
       "Df Model:                          16                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.099   1.14e+04      0.000    1134.700    1135.090\n",
       "대비_irs_1Y      0.4557      0.189      2.412      0.016       0.085       0.826\n",
       "대비_irs_5Y     -0.1879      0.237     -0.794      0.427      -0.652       0.276\n",
       "대비_irs_10Y    -0.3429      0.224     -1.528      0.127      -0.783       0.097\n",
       "대비_crs_1Y     -1.0308      0.162     -6.375      0.000      -1.348      -0.714\n",
       "대비_crs_2Y     -0.0279      0.228     -0.123      0.902      -0.475       0.419\n",
       "대비_crs_3Y     -0.3861      0.194     -1.987      0.047      -0.767      -0.005\n",
       "대비_crs_5Y      0.0755      0.257      0.294      0.769      -0.429       0.580\n",
       "대비_crs_10Y     0.0219      0.192      0.114      0.909      -0.354       0.398\n",
       "대비_국고_1Y       0.1825      0.137      1.333      0.183      -0.086       0.451\n",
       "대비_국고_3Y      -0.2548      0.111     -2.297      0.022      -0.472      -0.037\n",
       "대비_국고_5Y       0.5714      0.247      2.310      0.021       0.086       1.057\n",
       "대비_국고_10Y     -0.1421      0.223     -0.637      0.524      -0.579       0.295\n",
       "대비_통안_2Y       0.0569      0.105      0.542      0.588      -0.149       0.263\n",
       "대비_ndf         2.3936      0.101     23.596      0.000       2.195       2.593\n",
       "스왑포인트_1M      -0.1134      0.107     -1.055      0.291      -0.324       0.097\n",
       "전일종가_ex       55.3138      0.108    512.365      0.000      55.102      55.526\n",
       "==============================================================================\n",
       "Omnibus:                       50.746   Durbin-Watson:                   2.645\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              105.085\n",
       "Skew:                           0.050   Prob(JB):                     1.52e-23\n",
       "Kurtosis:                       4.008   Cond. No.                         7.29\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor     Feature\n",
      "0     2.809142   대비_irs_1Y\n",
      "1     3.637163  대비_irs_10Y\n",
      "2     2.646579   대비_crs_1Y\n",
      "3     4.578156   대비_crs_2Y\n",
      "4     3.455082   대비_crs_3Y\n",
      "5     2.425173  대비_crs_10Y\n",
      "6     1.899707    대비_국고_1Y\n",
      "7     1.245332    대비_국고_3Y\n",
      "8     6.144135    대비_국고_5Y\n",
      "9     5.039409   대비_국고_10Y\n",
      "10    1.114645    대비_통안_2Y\n",
      "11    1.040948      대비_ndf\n",
      "12    1.169418    스왑포인트_1M\n",
      "13    1.180869     전일종가_ex\n"
     ]
    }
   ],
   "source": [
    "x_scaled.drop(['대비_irs_5Y', '대비_crs_5Y'], axis=1, inplace=True)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.227e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:17:52</td>     <th>  Log-Likelihood:    </th> <td> -7398.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.483e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2443</td>      <th>  BIC:               </th> <td>1.491e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    14</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.099</td> <td> 1.14e+04</td> <td> 0.000</td> <td> 1134.700</td> <td> 1135.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.3834</td> <td>    0.166</td> <td>    2.303</td> <td> 0.021</td> <td>    0.057</td> <td>    0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.4355</td> <td>    0.189</td> <td>   -2.300</td> <td> 0.022</td> <td>   -0.807</td> <td>   -0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -1.0270</td> <td>    0.162</td> <td>   -6.357</td> <td> 0.000</td> <td>   -1.344</td> <td>   -0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_2Y</th>  <td>   -0.0045</td> <td>    0.212</td> <td>   -0.021</td> <td> 0.983</td> <td>   -0.421</td> <td>    0.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.3692</td> <td>    0.185</td> <td>   -2.000</td> <td> 0.046</td> <td>   -0.731</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_10Y</th> <td>    0.0479</td> <td>    0.155</td> <td>    0.310</td> <td> 0.757</td> <td>   -0.255</td> <td>    0.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>   <td>    0.1812</td> <td>    0.137</td> <td>    1.324</td> <td> 0.186</td> <td>   -0.087</td> <td>    0.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.2568</td> <td>    0.111</td> <td>   -2.317</td> <td> 0.021</td> <td>   -0.474</td> <td>   -0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.5567</td> <td>    0.246</td> <td>    2.262</td> <td> 0.024</td> <td>    0.074</td> <td>    1.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>  <td>   -0.1450</td> <td>    0.223</td> <td>   -0.651</td> <td> 0.515</td> <td>   -0.582</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>   <td>    0.0561</td> <td>    0.105</td> <td>    0.535</td> <td> 0.592</td> <td>   -0.149</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>     <td>    2.3919</td> <td>    0.101</td> <td>   23.608</td> <td> 0.000</td> <td>    2.193</td> <td>    2.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -0.1136</td> <td>    0.107</td> <td>   -1.058</td> <td> 0.290</td> <td>   -0.324</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.3149</td> <td>    0.108</td> <td>  512.604</td> <td> 0.000</td> <td>   55.103</td> <td>   55.527</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>50.196</td> <th>  Durbin-Watson:     </th> <td>   2.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 103.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.047</td> <th>  Prob(JB):          </th> <td>3.08e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.002</td> <th>  Cond. No.          </th> <td>    6.52</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 2.227e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        19:17:52   Log-Likelihood:                -7398.2\n",
       "No. Observations:                2458   AIC:                         1.483e+04\n",
       "Df Residuals:                    2443   BIC:                         1.491e+04\n",
       "Df Model:                          14                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.099   1.14e+04      0.000    1134.700    1135.090\n",
       "대비_irs_1Y      0.3834      0.166      2.303      0.021       0.057       0.710\n",
       "대비_irs_10Y    -0.4355      0.189     -2.300      0.022      -0.807      -0.064\n",
       "대비_crs_1Y     -1.0270      0.162     -6.357      0.000      -1.344      -0.710\n",
       "대비_crs_2Y     -0.0045      0.212     -0.021      0.983      -0.421       0.412\n",
       "대비_crs_3Y     -0.3692      0.185     -2.000      0.046      -0.731      -0.007\n",
       "대비_crs_10Y     0.0479      0.155      0.310      0.757      -0.255       0.351\n",
       "대비_국고_1Y       0.1812      0.137      1.324      0.186      -0.087       0.450\n",
       "대비_국고_3Y      -0.2568      0.111     -2.317      0.021      -0.474      -0.039\n",
       "대비_국고_5Y       0.5567      0.246      2.262      0.024       0.074       1.039\n",
       "대비_국고_10Y     -0.1450      0.223     -0.651      0.515      -0.582       0.292\n",
       "대비_통안_2Y       0.0561      0.105      0.535      0.592      -0.149       0.262\n",
       "대비_ndf         2.3919      0.101     23.608      0.000       2.193       2.591\n",
       "스왑포인트_1M      -0.1136      0.107     -1.058      0.290      -0.324       0.097\n",
       "전일종가_ex       55.3149      0.108    512.604      0.000      55.103      55.527\n",
       "==============================================================================\n",
       "Omnibus:                       50.196   Durbin-Watson:                   2.646\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.668\n",
       "Skew:                           0.047   Prob(JB):                     3.08e-23\n",
       "Kurtosis:                       4.002   Cond. No.                         6.52\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.835e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:17:54</td>     <th>  Log-Likelihood:    </th> <td> -7399.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.482e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2446</td>      <th>  BIC:               </th> <td>1.489e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.099</td> <td> 1.14e+04</td> <td> 0.000</td> <td> 1134.700</td> <td> 1135.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.4306</td> <td>    0.163</td> <td>    2.646</td> <td> 0.008</td> <td>    0.111</td> <td>    0.750</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.4686</td> <td>    0.187</td> <td>   -2.502</td> <td> 0.012</td> <td>   -0.836</td> <td>   -0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -1.0238</td> <td>    0.162</td> <td>   -6.339</td> <td> 0.000</td> <td>   -1.341</td> <td>   -0.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_2Y</th>  <td>    0.0205</td> <td>    0.203</td> <td>    0.101</td> <td> 0.920</td> <td>   -0.377</td> <td>    0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.3520</td> <td>    0.176</td> <td>   -2.001</td> <td> 0.046</td> <td>   -0.697</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.2352</td> <td>    0.110</td> <td>   -2.141</td> <td> 0.032</td> <td>   -0.451</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.6785</td> <td>    0.233</td> <td>    2.917</td> <td> 0.004</td> <td>    0.222</td> <td>    1.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>  <td>   -0.1347</td> <td>    0.223</td> <td>   -0.605</td> <td> 0.546</td> <td>   -0.571</td> <td>    0.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>     <td>    2.3904</td> <td>    0.101</td> <td>   23.609</td> <td> 0.000</td> <td>    2.192</td> <td>    2.589</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>   <td>   -0.1121</td> <td>    0.107</td> <td>   -1.044</td> <td> 0.297</td> <td>   -0.323</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.3252</td> <td>    0.108</td> <td>  514.358</td> <td> 0.000</td> <td>   55.114</td> <td>   55.536</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>47.711</td> <th>  Durbin-Watson:     </th> <td>   2.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  97.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.033</td> <th>  Prob(JB):          </th> <td>7.91e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.972</td> <th>  Cond. No.          </th> <td>    5.73</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 2.835e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        19:17:54   Log-Likelihood:                -7399.4\n",
       "No. Observations:                2458   AIC:                         1.482e+04\n",
       "Df Residuals:                    2446   BIC:                         1.489e+04\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.099   1.14e+04      0.000    1134.700    1135.090\n",
       "대비_irs_1Y      0.4306      0.163      2.646      0.008       0.111       0.750\n",
       "대비_irs_10Y    -0.4686      0.187     -2.502      0.012      -0.836      -0.101\n",
       "대비_crs_1Y     -1.0238      0.162     -6.339      0.000      -1.341      -0.707\n",
       "대비_crs_2Y      0.0205      0.203      0.101      0.920      -0.377       0.418\n",
       "대비_crs_3Y     -0.3520      0.176     -2.001      0.046      -0.697      -0.007\n",
       "대비_국고_3Y      -0.2352      0.110     -2.141      0.032      -0.451      -0.020\n",
       "대비_국고_5Y       0.6785      0.233      2.917      0.004       0.222       1.134\n",
       "대비_국고_10Y     -0.1347      0.223     -0.605      0.546      -0.571       0.302\n",
       "대비_ndf         2.3904      0.101     23.609      0.000       2.192       2.589\n",
       "스왑포인트_1M      -0.1121      0.107     -1.044      0.297      -0.323       0.098\n",
       "전일종가_ex       55.3252      0.108    514.358      0.000      55.114      55.536\n",
       "==============================================================================\n",
       "Omnibus:                       47.711   Durbin-Watson:                   2.648\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               97.177\n",
       "Skew:                           0.033   Prob(JB):                     7.91e-22\n",
       "Kurtosis:                       3.972   Cond. No.                         5.73\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_crs_10Y', '대비_통안_2Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.900e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:17:56</td>     <th>  Log-Likelihood:    </th> <td> -7400.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2458</td>      <th>  AIC:               </th> <td>1.482e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2449</td>      <th>  BIC:               </th> <td>1.487e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td> 1134.8952</td> <td>    0.099</td> <td> 1.14e+04</td> <td> 0.000</td> <td> 1134.701</td> <td> 1135.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_1Y</th>  <td>    0.4465</td> <td>    0.160</td> <td>    2.784</td> <td> 0.005</td> <td>    0.132</td> <td>    0.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_irs_10Y</th> <td>   -0.4892</td> <td>    0.184</td> <td>   -2.658</td> <td> 0.008</td> <td>   -0.850</td> <td>   -0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_1Y</th>  <td>   -1.0137</td> <td>    0.138</td> <td>   -7.330</td> <td> 0.000</td> <td>   -1.285</td> <td>   -0.743</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_crs_3Y</th>  <td>   -0.3419</td> <td>    0.142</td> <td>   -2.410</td> <td> 0.016</td> <td>   -0.620</td> <td>   -0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>   <td>   -0.2349</td> <td>    0.110</td> <td>   -2.139</td> <td> 0.032</td> <td>   -0.450</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>   <td>    0.5641</td> <td>    0.135</td> <td>    4.167</td> <td> 0.000</td> <td>    0.299</td> <td>    0.830</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_ndf</th>     <td>    2.3928</td> <td>    0.101</td> <td>   23.690</td> <td> 0.000</td> <td>    2.195</td> <td>    2.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>    <td>   55.3669</td> <td>    0.100</td> <td>  556.388</td> <td> 0.000</td> <td>   55.172</td> <td>   55.562</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>47.574</td> <th>  Durbin-Watson:     </th> <td>   2.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  96.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.030</td> <th>  Prob(JB):          </th> <td>8.87e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.971</td> <th>  Cond. No.          </th> <td>    3.84</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.992\n",
       "Model:                            OLS   Adj. R-squared:                  0.992\n",
       "Method:                 Least Squares   F-statistic:                 3.900e+04\n",
       "Date:                Tue, 30 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        19:17:56   Log-Likelihood:                -7400.1\n",
       "No. Observations:                2458   AIC:                         1.482e+04\n",
       "Df Residuals:                    2449   BIC:                         1.487e+04\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1134.8952      0.099   1.14e+04      0.000    1134.701    1135.090\n",
       "대비_irs_1Y      0.4465      0.160      2.784      0.005       0.132       0.761\n",
       "대비_irs_10Y    -0.4892      0.184     -2.658      0.008      -0.850      -0.128\n",
       "대비_crs_1Y     -1.0137      0.138     -7.330      0.000      -1.285      -0.743\n",
       "대비_crs_3Y     -0.3419      0.142     -2.410      0.016      -0.620      -0.064\n",
       "대비_국고_3Y      -0.2349      0.110     -2.139      0.032      -0.450      -0.020\n",
       "대비_국고_5Y       0.5641      0.135      4.167      0.000       0.299       0.830\n",
       "대비_ndf         2.3928      0.101     23.690      0.000       2.195       2.591\n",
       "전일종가_ex       55.3669      0.100    556.388      0.000      55.172      55.562\n",
       "==============================================================================\n",
       "Omnibus:                       47.574   Durbin-Watson:                   2.649\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               96.949\n",
       "Skew:                           0.030   Prob(JB):                     8.87e-22\n",
       "Kurtosis:                       3.971   Cond. No.                         3.84\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_crs_2Y', '대비_국고_10Y', '스왑포인트_1M'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_irs_1Y</th>\n",
       "      <th>대비_irs_10Y</th>\n",
       "      <th>대비_crs_1Y</th>\n",
       "      <th>대비_crs_3Y</th>\n",
       "      <th>대비_국고_3Y</th>\n",
       "      <th>대비_국고_5Y</th>\n",
       "      <th>대비_ndf</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>-0.848159</td>\n",
       "      <td>-0.403644</td>\n",
       "      <td>-0.205698</td>\n",
       "      <td>-0.462791</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>0.686282</td>\n",
       "      <td>-0.056282</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>0.559997</td>\n",
       "      <td>0.202253</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>-0.690892</td>\n",
       "      <td>0.159979</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-1.668663</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.404219</td>\n",
       "      <td>0.401029</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-0.001379</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.075741</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.606184</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-0.324094</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>0.206571</td>\n",
       "      <td>-0.108476</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-09</th>\n",
       "      <td>1.404890</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>1.816727</td>\n",
       "      <td>0.905815</td>\n",
       "      <td>0.966767</td>\n",
       "      <td>1.351270</td>\n",
       "      <td>0.381012</td>\n",
       "      <td>-0.117475</td>\n",
       "      <td>1125.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-1.129790</td>\n",
       "      <td>-2.221333</td>\n",
       "      <td>-1.621396</td>\n",
       "      <td>-1.603295</td>\n",
       "      <td>-0.969524</td>\n",
       "      <td>-1.890723</td>\n",
       "      <td>0.572896</td>\n",
       "      <td>3.206786</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>-0.284897</td>\n",
       "      <td>-0.605609</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.690892</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-0.539892</td>\n",
       "      <td>0.189127</td>\n",
       "      <td>3.219385</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>-0.284897</td>\n",
       "      <td>-1.211506</td>\n",
       "      <td>0.198787</td>\n",
       "      <td>-0.234690</td>\n",
       "      <td>-0.485451</td>\n",
       "      <td>-1.350390</td>\n",
       "      <td>-0.098699</td>\n",
       "      <td>3.109596</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.841628</td>\n",
       "      <td>1.414046</td>\n",
       "      <td>0.805515</td>\n",
       "      <td>0.905815</td>\n",
       "      <td>0.644052</td>\n",
       "      <td>0.810938</td>\n",
       "      <td>-0.796461</td>\n",
       "      <td>3.212186</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-1.974684</td>\n",
       "      <td>-2.423299</td>\n",
       "      <td>-2.228124</td>\n",
       "      <td>-3.428103</td>\n",
       "      <td>-2.099027</td>\n",
       "      <td>-3.241553</td>\n",
       "      <td>-1.799493</td>\n",
       "      <td>2.902617</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2458 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_irs_1Y  대비_irs_10Y  대비_crs_1Y  대비_crs_3Y  대비_국고_3Y  대비_국고_5Y  \\\n",
       "DateTime                                                                      \n",
       "2012-08-03  -0.848159   -0.403644  -0.205698  -0.462791 -0.324094 -1.890723   \n",
       "2012-08-06   0.559997    0.202253  -0.003456  -0.690892  0.159979  0.000440   \n",
       "2012-08-07   0.278366    0.404219   0.401029  -0.006589 -0.001379  0.000440   \n",
       "2012-08-08   0.278366    0.606184   0.198787  -0.006589 -0.324094 -0.539892   \n",
       "2012-08-09   1.404890    0.808150   1.816727   0.905815  0.966767  1.351270   \n",
       "...               ...         ...        ...        ...       ...       ...   \n",
       "2022-07-25  -1.129790   -2.221333  -1.621396  -1.603295 -0.969524 -1.890723   \n",
       "2022-07-26  -0.284897   -0.605609   0.198787  -0.690892 -0.485451 -0.539892   \n",
       "2022-07-27  -0.284897   -1.211506   0.198787  -0.234690 -0.485451 -1.350390   \n",
       "2022-07-28   0.841628    1.414046   0.805515   0.905815  0.644052  0.810938   \n",
       "2022-07-29  -1.974684   -2.423299  -2.228124  -3.428103 -2.099027 -3.241553   \n",
       "\n",
       "              대비_ndf   전일종가_ex   종가_ex  \n",
       "DateTime                                \n",
       "2012-08-03  0.686282 -0.056282  1134.8  \n",
       "2012-08-06 -1.668663 -0.000487  1129.0  \n",
       "2012-08-07  0.075741 -0.104877  1128.8  \n",
       "2012-08-08  0.206571 -0.108476  1128.3  \n",
       "2012-08-09  0.381012 -0.117475  1125.5  \n",
       "...              ...       ...     ...  \n",
       "2022-07-25  0.572896  3.206786  1313.7  \n",
       "2022-07-26  0.189127  3.219385  1307.6  \n",
       "2022-07-27 -0.098699  3.109596  1313.3  \n",
       "2022-07-28 -0.796461  3.212186  1296.1  \n",
       "2022-07-29 -1.799493  2.902617  1299.1  \n",
       "\n",
       "[2458 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.concat([x_scaled,y], axis=1)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_2[0:1945]\n",
    "test = df_2[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=1):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['대비_irs_1Y','대비_irs_10Y', '대비_crs_1Y', '대비_crs_3Y', \n",
    "                 '대비_국고_3Y', '대비_국고_5Y', '대비_ndf', '전일종가_ex']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1555, 1, 8), (389, 1, 8))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.55999688,  0.40421868, -0.20569829, ...,  0.5407717 ,\n",
       "          0.15423924,  0.56825483]],\n",
       "\n",
       "       [[-0.28489662, -0.20167797, -0.40794085, ...,  1.35126977,\n",
       "          0.37228973, -1.24056112]],\n",
       "\n",
       "       [[ 0.13755013,  0.10127036, -0.00345573, ...,  0.27060568,\n",
       "          0.49439801, -1.32695232]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.28489662, -0.20167797,  0.19878683, ..., -0.26972637,\n",
       "          0.93049901, -0.3100558 ]],\n",
       "\n",
       "       [[-0.00326545, -0.20167797, -0.00345573, ..., -0.26972637,\n",
       "          0.11935116,  1.7921303 ]],\n",
       "\n",
       "       [[-0.00326545,  0.40421868,  0.19878683, ...,  0.27060568,\n",
       "          0.72989255,  0.01931069]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512, 1, 8), (512, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 1)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1266841.3750 - mae: 1124.3763\n",
      "Epoch 1: val_loss improved from inf to 1264566.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 12s 88ms/step - loss: 1267034.3750 - mae: 1124.4797 - val_loss: 1264566.3750 - val_mae: 1123.4337\n",
      "Epoch 2/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 1265874.2500 - mae: 1123.9661\n",
      "Epoch 2: val_loss improved from 1264566.37500 to 1262652.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 1265687.0000 - mae: 1123.8817 - val_loss: 1262652.7500 - val_mae: 1122.5840\n",
      "Epoch 3/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 1262927.3750 - mae: 1122.6714\n",
      "Epoch 3: val_loss improved from 1262652.75000 to 1259132.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 1263095.8750 - mae: 1122.7286 - val_loss: 1259132.0000 - val_mae: 1121.0142\n",
      "Epoch 4/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1259062.1250 - mae: 1120.9414\n",
      "Epoch 4: val_loss improved from 1259132.00000 to 1253590.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1258726.1250 - mae: 1120.7733 - val_loss: 1253590.6250 - val_mae: 1118.5297\n",
      "Epoch 5/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1252326.1250 - mae: 1117.8926\n",
      "Epoch 5: val_loss improved from 1253590.62500 to 1246043.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1252326.1250 - mae: 1117.8926 - val_loss: 1246043.2500 - val_mae: 1115.1232\n",
      "Epoch 6/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1244594.1250 - mae: 1114.3885\n",
      "Epoch 6: val_loss improved from 1246043.25000 to 1236699.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 1244082.7500 - mae: 1114.1583 - val_loss: 1236699.5000 - val_mae: 1110.8746\n",
      "Epoch 7/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1234313.6250 - mae: 1109.7010\n",
      "Epoch 7: val_loss improved from 1236699.50000 to 1225783.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1234112.7500 - mae: 1109.6117 - val_loss: 1225783.7500 - val_mae: 1105.8716\n",
      "Epoch 8/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1222632.5000 - mae: 1104.3242\n",
      "Epoch 8: val_loss improved from 1225783.75000 to 1213312.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 1222632.5000 - mae: 1104.3242 - val_loss: 1213312.1250 - val_mae: 1100.1049\n",
      "Epoch 9/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1209810.6250 - mae: 1098.4279\n",
      "Epoch 9: val_loss improved from 1213312.12500 to 1199801.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1209757.5000 - mae: 1098.3307 - val_loss: 1199801.7500 - val_mae: 1093.7953\n",
      "Epoch 10/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1195561.7500 - mae: 1091.6747\n",
      "Epoch 10: val_loss improved from 1199801.75000 to 1184979.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1195666.2500 - mae: 1091.7238 - val_loss: 1184979.7500 - val_mae: 1086.7992\n",
      "Epoch 11/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1179806.6250 - mae: 1084.1954\n",
      "Epoch 11: val_loss improved from 1184979.75000 to 1169110.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1180557.3750 - mae: 1084.5427 - val_loss: 1169110.8750 - val_mae: 1079.2191\n",
      "Epoch 12/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1163593.8750 - mae: 1076.5742\n",
      "Epoch 12: val_loss improved from 1169110.87500 to 1152442.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1164461.2500 - mae: 1076.7931 - val_loss: 1152442.0000 - val_mae: 1071.1593\n",
      "Epoch 13/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1147475.7500 - mae: 1068.5538\n",
      "Epoch 13: val_loss improved from 1152442.00000 to 1135126.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1147545.2500 - mae: 1068.5876 - val_loss: 1135126.7500 - val_mae: 1062.6750\n",
      "Epoch 14/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1129811.0000 - mae: 1059.8308\n",
      "Epoch 14: val_loss improved from 1135126.75000 to 1116948.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1129920.1250 - mae: 1059.8868 - val_loss: 1116948.6250 - val_mae: 1053.6338\n",
      "Epoch 15/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1111913.6250 - mae: 1050.8690\n",
      "Epoch 15: val_loss improved from 1116948.62500 to 1098280.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1111672.6250 - mae: 1050.7369 - val_loss: 1098280.1250 - val_mae: 1044.2101\n",
      "Epoch 16/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1091583.2500 - mae: 1040.7720\n",
      "Epoch 16: val_loss improved from 1098280.12500 to 1079032.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 1092894.6250 - mae: 1041.3704 - val_loss: 1079032.6250 - val_mae: 1034.3269\n",
      "Epoch 17/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1075609.1250 - mae: 1032.7745\n",
      "Epoch 17: val_loss improved from 1079032.62500 to 1059450.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 1073656.8750 - mae: 1031.6074 - val_loss: 1059450.1250 - val_mae: 1024.1022\n",
      "Epoch 18/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1054082.6250 - mae: 1021.4651\n",
      "Epoch 18: val_loss improved from 1059450.12500 to 1039619.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 1054112.1250 - mae: 1021.4811 - val_loss: 1039619.1250 - val_mae: 1013.5666\n",
      "Epoch 19/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 1036436.2500 - mae: 1012.1496\n",
      "Epoch 19: val_loss improved from 1039619.12500 to 1019438.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1034335.3750 - mae: 1011.1097 - val_loss: 1019438.2500 - val_mae: 1002.6442\n",
      "Epoch 20/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 1019110.0000 - mae: 1002.8867\n",
      "Epoch 20: val_loss improved from 1019438.25000 to 999325.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 1014239.8750 - mae: 1000.2606 - val_loss: 999325.9375 - val_mae: 991.5948\n",
      "Epoch 21/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 999318.0000 - mae: 991.7558 \n",
      "Epoch 21: val_loss improved from 999325.93750 to 979017.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 994177.3750 - mae: 989.2770 - val_loss: 979017.6250 - val_mae: 980.7008\n",
      "Epoch 22/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 975580.1875 - mae: 978.7008\n",
      "Epoch 22: val_loss improved from 979017.62500 to 958605.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 974056.3125 - mae: 978.0278 - val_loss: 958605.3125 - val_mae: 969.5157\n",
      "Epoch 23/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 954027.8125 - mae: 966.5065\n",
      "Epoch 23: val_loss improved from 958605.31250 to 938548.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 953960.8125 - mae: 966.5969 - val_loss: 938548.1250 - val_mae: 958.3128\n",
      "Epoch 24/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 937124.2500 - mae: 956.9488\n",
      "Epoch 24: val_loss improved from 938548.12500 to 918469.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 933929.7500 - mae: 954.9764 - val_loss: 918469.5000 - val_mae: 946.8604\n",
      "Epoch 25/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 913321.0000 - mae: 943.1973\n",
      "Epoch 25: val_loss improved from 918469.50000 to 898351.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 913951.9375 - mae: 943.0981 - val_loss: 898351.0625 - val_mae: 935.1068\n",
      "Epoch 26/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 894147.0000 - mae: 931.2446\n",
      "Epoch 26: val_loss improved from 898351.06250 to 878701.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 894147.0000 - mae: 931.2446 - val_loss: 878701.3125 - val_mae: 923.3816\n",
      "Epoch 27/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 874609.4375 - mae: 919.3105\n",
      "Epoch 27: val_loss improved from 878701.31250 to 859315.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 874609.4375 - mae: 919.3105 - val_loss: 859315.5625 - val_mae: 911.5623\n",
      "Epoch 28/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 855519.4375 - mae: 907.3267\n",
      "Epoch 28: val_loss improved from 859315.56250 to 840049.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 855189.3750 - mae: 907.3204 - val_loss: 840049.8750 - val_mae: 899.5287\n",
      "Epoch 29/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 836040.8750 - mae: 895.3727\n",
      "Epoch 29: val_loss improved from 840049.87500 to 821156.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 836040.8750 - mae: 895.3727 - val_loss: 821156.1875 - val_mae: 887.7038\n",
      "Epoch 30/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 816973.1875 - mae: 882.9701\n",
      "Epoch 30: val_loss improved from 821156.18750 to 802031.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 817113.7500 - mae: 883.0825 - val_loss: 802031.1250 - val_mae: 875.4127\n",
      "Epoch 31/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 804569.8750 - mae: 875.0327\n",
      "Epoch 31: val_loss improved from 802031.12500 to 784044.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 798375.7500 - mae: 871.1526 - val_loss: 784044.5000 - val_mae: 863.6365\n",
      "Epoch 32/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 784085.7500 - mae: 861.7144\n",
      "Epoch 32: val_loss improved from 784044.50000 to 765960.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 780035.8125 - mae: 859.0923 - val_loss: 765960.3125 - val_mae: 851.4690\n",
      "Epoch 33/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 755971.1875 - mae: 844.6215\n",
      "Epoch 33: val_loss improved from 765960.31250 to 747999.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 761976.0000 - mae: 846.8304 - val_loss: 747999.6875 - val_mae: 839.0908\n",
      "Epoch 34/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 752732.2500 - mae: 840.6922\n",
      "Epoch 34: val_loss improved from 747999.68750 to 730937.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 744133.8750 - mae: 834.8849 - val_loss: 730937.5000 - val_mae: 827.1438\n",
      "Epoch 35/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 726377.5625 - mae: 822.6495\n",
      "Epoch 35: val_loss improved from 730937.50000 to 713908.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 726746.6875 - mae: 822.8784 - val_loss: 713908.0625 - val_mae: 815.1063\n",
      "Epoch 36/200\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 703680.5625 - mae: 808.5026\n",
      "Epoch 36: val_loss improved from 713908.06250 to 697035.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 709630.5000 - mae: 810.7136 - val_loss: 697035.0000 - val_mae: 803.2784\n",
      "Epoch 37/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 692771.3125 - mae: 798.9475\n",
      "Epoch 37: val_loss improved from 697035.00000 to 680912.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 692771.3125 - mae: 798.9475 - val_loss: 680912.4375 - val_mae: 791.8018\n",
      "Epoch 38/200\n",
      "82/98 [========================>.....] - ETA: 0s - loss: 679821.4375 - mae: 789.5005\n",
      "Epoch 38: val_loss improved from 680912.43750 to 665299.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 676580.5625 - mae: 787.3306 - val_loss: 665299.2500 - val_mae: 780.5107\n",
      "Epoch 39/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 662632.7500 - mae: 776.8430\n",
      "Epoch 39: val_loss improved from 665299.25000 to 649883.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 660571.5000 - mae: 775.7994 - val_loss: 649883.4375 - val_mae: 769.1536\n",
      "Epoch 40/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 644915.1250 - mae: 764.3635\n",
      "Epoch 40: val_loss improved from 649883.43750 to 634719.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 644915.1250 - mae: 764.3635 - val_loss: 634719.3750 - val_mae: 757.7234\n",
      "Epoch 41/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 635877.5625 - mae: 757.2383\n",
      "Epoch 41: val_loss improved from 634719.37500 to 619819.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 629582.5625 - mae: 752.7857 - val_loss: 619819.5000 - val_mae: 746.2524\n",
      "Epoch 42/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 615341.8750 - mae: 742.0478\n",
      "Epoch 42: val_loss improved from 619819.50000 to 605277.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 614580.0000 - mae: 741.6215 - val_loss: 605277.0625 - val_mae: 734.8278\n",
      "Epoch 43/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 600550.8750 - mae: 729.3020\n",
      "Epoch 43: val_loss improved from 605277.06250 to 591085.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 599804.4375 - mae: 730.5543 - val_loss: 591085.1250 - val_mae: 723.6786\n",
      "Epoch 44/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 585382.1250 - mae: 719.2516\n",
      "Epoch 44: val_loss improved from 591085.12500 to 577110.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 585392.2500 - mae: 719.3307 - val_loss: 577110.6875 - val_mae: 712.5702\n",
      "Epoch 45/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 572774.4375 - mae: 709.5867\n",
      "Epoch 45: val_loss improved from 577110.68750 to 563308.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 571239.0000 - mae: 708.4902 - val_loss: 563308.8125 - val_mae: 701.4725\n",
      "Epoch 46/200\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 558306.7500 - mae: 698.2769\n",
      "Epoch 46: val_loss improved from 563308.81250 to 550023.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 557392.1250 - mae: 697.8167 - val_loss: 550023.8750 - val_mae: 690.6747\n",
      "Epoch 47/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 546445.3750 - mae: 687.8995\n",
      "Epoch 47: val_loss improved from 550023.87500 to 536858.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 543842.8750 - mae: 686.9317 - val_loss: 536858.0625 - val_mae: 679.9349\n",
      "Epoch 48/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 530956.9375 - mae: 676.3328\n",
      "Epoch 48: val_loss improved from 536858.06250 to 524081.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 530527.3750 - mae: 676.4518 - val_loss: 524081.1875 - val_mae: 669.4153\n",
      "Epoch 49/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 517437.2188 - mae: 665.7686\n",
      "Epoch 49: val_loss improved from 524081.18750 to 511464.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 517493.7500 - mae: 665.8597 - val_loss: 511464.9375 - val_mae: 658.9437\n",
      "Epoch 50/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 504742.0312 - mae: 655.4246\n",
      "Epoch 50: val_loss improved from 511464.93750 to 499148.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 504742.0312 - mae: 655.4246 - val_loss: 499148.7500 - val_mae: 648.6051\n",
      "Epoch 51/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 492227.0000 - mae: 644.9655\n",
      "Epoch 51: val_loss improved from 499148.75000 to 487015.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 492227.0000 - mae: 644.9655 - val_loss: 487015.3750 - val_mae: 638.2925\n",
      "Epoch 52/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 483512.2812 - mae: 636.9547\n",
      "Epoch 52: val_loss improved from 487015.37500 to 475183.59375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 479946.1250 - mae: 634.8254 - val_loss: 475183.5938 - val_mae: 628.0961\n",
      "Epoch 53/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 469405.6875 - mae: 625.6339\n",
      "Epoch 53: val_loss improved from 475183.59375 to 463620.03125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 467922.1875 - mae: 624.5563 - val_loss: 463620.0312 - val_mae: 618.0083\n",
      "Epoch 54/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 456240.5625 - mae: 613.6006\n",
      "Epoch 54: val_loss improved from 463620.03125 to 452177.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 456102.6875 - mae: 614.6288 - val_loss: 452177.6875 - val_mae: 607.9520\n",
      "Epoch 55/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 446040.1250 - mae: 605.1214\n",
      "Epoch 55: val_loss improved from 452177.68750 to 441010.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 444537.3438 - mae: 604.6031 - val_loss: 441010.5625 - val_mae: 598.1293\n",
      "Epoch 56/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 433233.1562 - mae: 594.5441\n",
      "Epoch 56: val_loss improved from 441010.56250 to 430057.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 433233.1562 - mae: 594.5441 - val_loss: 430057.5312 - val_mae: 588.3056\n",
      "Epoch 57/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 420997.1875 - mae: 584.3690\n",
      "Epoch 57: val_loss improved from 430057.53125 to 419198.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 422014.5938 - mae: 584.8737 - val_loss: 419198.0625 - val_mae: 578.7314\n",
      "Epoch 58/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 413446.4375 - mae: 576.6979\n",
      "Epoch 58: val_loss improved from 419198.06250 to 408701.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 411004.8750 - mae: 575.1384 - val_loss: 408701.3125 - val_mae: 569.2377\n",
      "Epoch 59/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 400358.3438 - mae: 565.7401\n",
      "Epoch 59: val_loss improved from 408701.31250 to 398201.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 400152.0625 - mae: 565.6739 - val_loss: 398201.8750 - val_mae: 559.9675\n",
      "Epoch 60/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 389974.4062 - mae: 556.6806\n",
      "Epoch 60: val_loss improved from 398201.87500 to 387819.40625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 389506.4375 - mae: 556.2764 - val_loss: 387819.4062 - val_mae: 550.6710\n",
      "Epoch 61/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 380103.8750 - mae: 546.7475\n",
      "Epoch 61: val_loss improved from 387819.40625 to 377712.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 379077.8750 - mae: 546.7640 - val_loss: 377712.2188 - val_mae: 541.5721\n",
      "Epoch 62/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 356612.9062 - mae: 535.4302\n",
      "Epoch 62: val_loss improved from 377712.21875 to 367541.84375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 368837.9688 - mae: 537.5381 - val_loss: 367541.8438 - val_mae: 532.4365\n",
      "Epoch 63/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 356261.9688 - mae: 525.9609\n",
      "Epoch 63: val_loss improved from 367541.84375 to 357801.65625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 358630.0312 - mae: 528.4420 - val_loss: 357801.6562 - val_mae: 523.7375\n",
      "Epoch 64/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 348287.3125 - mae: 519.0648\n",
      "Epoch 64: val_loss improved from 357801.65625 to 348095.59375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 348695.9062 - mae: 519.4689 - val_loss: 348095.5938 - val_mae: 514.7423\n",
      "Epoch 65/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 338797.7188 - mae: 510.1773\n",
      "Epoch 65: val_loss improved from 348095.59375 to 338415.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 338782.6562 - mae: 510.4912 - val_loss: 338415.6250 - val_mae: 505.7734\n",
      "Epoch 66/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 329029.7500 - mae: 501.3116\n",
      "Epoch 66: val_loss improved from 338415.62500 to 328849.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 329029.7500 - mae: 501.3116 - val_loss: 328849.3125 - val_mae: 496.7711\n",
      "Epoch 67/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 319083.5312 - mae: 491.4397\n",
      "Epoch 67: val_loss improved from 328849.31250 to 319480.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 319438.5000 - mae: 492.2404 - val_loss: 319480.1875 - val_mae: 487.7764\n",
      "Epoch 68/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 309299.4688 - mae: 482.0890\n",
      "Epoch 68: val_loss improved from 319480.18750 to 310184.68750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 309964.3750 - mae: 483.4162 - val_loss: 310184.6875 - val_mae: 478.8109\n",
      "Epoch 69/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 300610.4062 - mae: 474.5549\n",
      "Epoch 69: val_loss improved from 310184.68750 to 301054.09375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 300610.4062 - mae: 474.5549 - val_loss: 301054.0938 - val_mae: 469.9480\n",
      "Epoch 70/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 291308.2188 - mae: 465.3038\n",
      "Epoch 70: val_loss improved from 301054.09375 to 292102.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 291401.0625 - mae: 465.9259 - val_loss: 292102.5312 - val_mae: 461.0058\n",
      "Epoch 71/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 284929.9375 - mae: 459.1048\n",
      "Epoch 71: val_loss improved from 292102.53125 to 282978.90625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 282336.1562 - mae: 456.9249 - val_loss: 282978.9062 - val_mae: 452.1964\n",
      "Epoch 72/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 274702.7188 - mae: 447.8040\n",
      "Epoch 72: val_loss improved from 282978.90625 to 274297.65625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 273308.1250 - mae: 448.3115 - val_loss: 274297.6562 - val_mae: 443.6354\n",
      "Epoch 73/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 266394.4062 - mae: 440.4763\n",
      "Epoch 73: val_loss improved from 274297.65625 to 265633.90625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 264524.8438 - mae: 439.6399 - val_loss: 265633.9062 - val_mae: 435.0066\n",
      "Epoch 74/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 255170.6094 - mae: 430.0843\n",
      "Epoch 74: val_loss improved from 265633.90625 to 256969.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 255806.2969 - mae: 430.8343 - val_loss: 256969.8125 - val_mae: 426.1993\n",
      "Epoch 75/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 246412.0156 - mae: 420.8702\n",
      "Epoch 75: val_loss improved from 256969.81250 to 248495.07812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 57ms/step - loss: 247147.9844 - mae: 422.3464 - val_loss: 248495.0781 - val_mae: 417.6117\n",
      "Epoch 76/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 238683.0469 - mae: 413.6993\n",
      "Epoch 76: val_loss improved from 248495.07812 to 240144.85938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 5s 48ms/step - loss: 238683.0469 - mae: 413.6993 - val_loss: 240144.8594 - val_mae: 409.0297\n",
      "Epoch 77/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 234883.0781 - mae: 407.4297\n",
      "Epoch 77: val_loss improved from 240144.85938 to 231933.85938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 4s 46ms/step - loss: 230411.6094 - mae: 405.2088 - val_loss: 231933.8594 - val_mae: 400.4761\n",
      "Epoch 78/200\n",
      "76/98 [======================>.......] - ETA: 0s - loss: 228577.8750 - mae: 400.0353\n",
      "Epoch 78: val_loss improved from 231933.85938 to 223847.20312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 222263.9688 - mae: 396.7457 - val_loss: 223847.2031 - val_mae: 391.9543\n",
      "Epoch 79/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 215052.1562 - mae: 386.8949\n",
      "Epoch 79: val_loss improved from 223847.20312 to 215982.35938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 214253.5469 - mae: 388.2748 - val_loss: 215982.3594 - val_mae: 383.6254\n",
      "Epoch 80/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 206293.8906 - mae: 379.6986\n",
      "Epoch 80: val_loss improved from 215982.35938 to 208094.93750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 206377.0938 - mae: 379.9281 - val_loss: 208094.9375 - val_mae: 375.1312\n",
      "Epoch 81/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 198608.7500 - mae: 371.4955\n",
      "Epoch 81: val_loss improved from 208094.93750 to 200448.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 198608.7500 - mae: 371.4955 - val_loss: 200448.1250 - val_mae: 366.7349\n",
      "Epoch 82/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 193248.8438 - mae: 364.8462\n",
      "Epoch 82: val_loss improved from 200448.12500 to 193003.20312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 191072.7344 - mae: 363.2907 - val_loss: 193003.2031 - val_mae: 358.4259\n",
      "Epoch 83/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 183740.3906 - mae: 354.8466\n",
      "Epoch 83: val_loss improved from 193003.20312 to 185789.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 183630.8125 - mae: 354.8244 - val_loss: 185789.0000 - val_mae: 350.1161\n",
      "Epoch 84/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 177551.1719 - mae: 346.8321\n",
      "Epoch 84: val_loss improved from 185789.00000 to 178504.95312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 176416.7188 - mae: 346.8015 - val_loss: 178504.9531 - val_mae: 342.0785\n",
      "Epoch 85/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 168467.6562 - mae: 335.9819\n",
      "Epoch 85: val_loss improved from 178504.95312 to 171481.20312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 169284.0000 - mae: 338.4125 - val_loss: 171481.2031 - val_mae: 333.9481\n",
      "Epoch 86/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 166841.9219 - mae: 333.9896\n",
      "Epoch 86: val_loss improved from 171481.20312 to 164504.73438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 162352.4688 - mae: 330.3448 - val_loss: 164504.7344 - val_mae: 325.7416\n",
      "Epoch 87/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 156634.7969 - mae: 320.6717\n",
      "Epoch 87: val_loss improved from 164504.73438 to 157988.73438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 155670.6250 - mae: 322.0930 - val_loss: 157988.7344 - val_mae: 317.9290\n",
      "Epoch 88/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 150729.3281 - mae: 314.6079\n",
      "Epoch 88: val_loss improved from 157988.73438 to 151507.39062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 149188.5000 - mae: 314.0406 - val_loss: 151507.3906 - val_mae: 309.9662\n",
      "Epoch 89/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 144594.3438 - mae: 307.6678\n",
      "Epoch 89: val_loss improved from 151507.39062 to 145316.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 142805.7344 - mae: 306.1131 - val_loss: 145316.4688 - val_mae: 302.2588\n",
      "Epoch 90/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 138922.9375 - mae: 298.6851\n",
      "Epoch 90: val_loss improved from 145316.46875 to 139196.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 136599.3906 - mae: 298.0542 - val_loss: 139196.2188 - val_mae: 294.5257\n",
      "Epoch 91/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 132924.0156 - mae: 291.7917\n",
      "Epoch 91: val_loss improved from 139196.21875 to 133331.20312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 130385.2031 - mae: 290.1250 - val_loss: 133331.2031 - val_mae: 286.7978\n",
      "Epoch 92/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 125519.0312 - mae: 283.7675\n",
      "Epoch 92: val_loss improved from 133331.20312 to 127550.53906, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 123740.1406 - mae: 282.2340 - val_loss: 127550.5391 - val_mae: 279.0832\n",
      "Epoch 93/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 113415.5312 - mae: 275.2639\n",
      "Epoch 93: val_loss improved from 127550.53906 to 121876.92188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 107ms/step - loss: 116429.4922 - mae: 274.0456 - val_loss: 121876.9219 - val_mae: 271.2736\n",
      "Epoch 94/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 111655.3047 - mae: 267.3774\n",
      "Epoch 94: val_loss improved from 121876.92188 to 116494.57031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 102ms/step - loss: 110183.7578 - mae: 266.3445 - val_loss: 116494.5703 - val_mae: 263.7603\n",
      "Epoch 95/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 98251.7812 - mae: 257.1009 \n",
      "Epoch 95: val_loss improved from 116494.57031 to 111351.18750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 100ms/step - loss: 104533.9375 - mae: 258.3190 - val_loss: 111351.1875 - val_mae: 256.0003\n",
      "Epoch 96/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 99274.3828 - mae: 250.2949\n",
      "Epoch 96: val_loss improved from 111351.18750 to 105330.20312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 112ms/step - loss: 99417.6094 - mae: 250.7714 - val_loss: 105330.2031 - val_mae: 248.5904\n",
      "Epoch 97/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 95235.5234 - mae: 243.7794\n",
      "Epoch 97: val_loss improved from 105330.20312 to 99186.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 94471.3672 - mae: 243.2272 - val_loss: 99186.2188 - val_mae: 241.0341\n",
      "Epoch 98/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 89656.3516 - mae: 234.4228\n",
      "Epoch 98: val_loss improved from 99186.21875 to 92302.07812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 89671.9922 - mae: 235.6621 - val_loss: 92302.0781 - val_mae: 233.3890\n",
      "Epoch 99/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 85697.0938 - mae: 228.5194\n",
      "Epoch 99: val_loss improved from 92302.07812 to 84904.32031, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 84980.1875 - mae: 228.3352 - val_loss: 84904.3203 - val_mae: 225.5845\n",
      "Epoch 100/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 81851.1719 - mae: 222.8375\n",
      "Epoch 100: val_loss improved from 84904.32031 to 78333.39062, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 80259.5938 - mae: 220.8906 - val_loss: 78333.3906 - val_mae: 217.8460\n",
      "Epoch 101/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 70704.9297 - mae: 213.5856\n",
      "Epoch 101: val_loss improved from 78333.39062 to 72690.41406, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 75294.1406 - mae: 213.6370 - val_loss: 72690.4141 - val_mae: 210.3844\n",
      "Epoch 102/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 73396.0938 - mae: 210.0202\n",
      "Epoch 102: val_loss improved from 72690.41406 to 68100.41406, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 70700.8906 - mae: 206.4878 - val_loss: 68100.4141 - val_mae: 203.3221\n",
      "Epoch 103/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 67647.9297 - mae: 199.8749\n",
      "Epoch 103: val_loss improved from 68100.41406 to 64177.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 66596.1953 - mae: 199.4308 - val_loss: 64177.1250 - val_mae: 196.2819\n",
      "Epoch 104/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 62771.8359 - mae: 192.5278\n",
      "Epoch 104: val_loss improved from 64177.12500 to 60815.98047, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 62825.8125 - mae: 192.7102 - val_loss: 60815.9805 - val_mae: 189.6576\n",
      "Epoch 105/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 60705.4961 - mae: 188.6521\n",
      "Epoch 105: val_loss improved from 60815.98047 to 57459.88672, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 59237.5820 - mae: 186.0218 - val_loss: 57459.8867 - val_mae: 183.1847\n",
      "Epoch 106/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 55930.9375 - mae: 179.5286\n",
      "Epoch 106: val_loss improved from 57459.88672 to 54434.95703, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 92ms/step - loss: 55854.7812 - mae: 179.3725 - val_loss: 54434.9570 - val_mae: 176.7279\n",
      "Epoch 107/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 52701.7969 - mae: 172.9867\n",
      "Epoch 107: val_loss improved from 54434.95703 to 51581.00781, saving model to .\\\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일: 모델학습을 위한 학습과정 설정단계\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "# print(\"\\n Accuracy: %.4f\" % model.evaluate(x_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25162649280>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_feature)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(test_label, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7adfe9f3775c954d01fc005199cfce03b4193bbc6ff21451e4d1eaf7785b4e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
