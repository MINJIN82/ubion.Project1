{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 파일 불러오기\n",
    "df = pd.read_excel(\"../데이터자료/Join_data.xlsx\", index_col = 0)    \n",
    "df = df.set_index(\"DateTime\")\n",
    "\n",
    "# 대비 계산\n",
    "df['대비_irs_1Y'] = df['1Y_Mid_irs'] - df['1Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_2Y'] = df['2Y_Mid_irs'] - df['2Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_3Y'] = df['3Y_Mid_irs'] - df['3Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_5Y'] = df['5Y_Mid_irs'] - df['5Y_Mid_irs'].shift(1) \n",
    "df['대비_irs_10Y'] = df['10Y_Mid_irs'] - df['10Y_Mid_irs'].shift(1) \n",
    "\n",
    "df['대비_crs_1Y'] = df['1Y_Mid_crs'] - df['1Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_2Y'] = df['2Y_Mid_crs'] - df['2Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_3Y'] = df['3Y_Mid_crs'] - df['3Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_5Y'] = df['5Y_Mid_crs'] - df['5Y_Mid_crs'].shift(1)\n",
    "df['대비_crs_10Y'] = df['10Y_Mid_crs'] - df['10Y_Mid_crs'].shift(1)\n",
    "\n",
    "df['대비_swapbasis_1Y'] = df['1Y_베이시스']-df['1Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_2Y'] = df['2Y_베이시스']-df['2Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_3Y'] = df['3Y_베이시스']-df['3Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_5Y'] = df['5Y_베이시스']-df['5Y_베이시스'].shift(1)\n",
    "df['대비_swapbasis_10Y'] = df['10Y_베이시스']-df['10Y_베이시스'].shift(1)\n",
    "\n",
    "df['대비_국고_1Y'] = df['국고1년']-df['국고1년'].shift(1)\n",
    "df['대비_국고_3Y'] = df['국고3년']-df['국고3년'].shift(1)\n",
    "df['대비_국고_5Y'] = df['국고5년']-df['국고5년'].shift(1)\n",
    "df['대비_국고_10Y'] = df['국고10년']-df['국고10년'].shift(1)\n",
    "\n",
    "df['대비_통안_1Y'] = df['통안364일']-df['통안364일'].shift(1)\n",
    "df['대비_통안_2Y'] = df['통안2년']-df['통안2년'].shift(1)\n",
    "\n",
    "df['대비_ndf'] = df['Mid_ndf']-df['Mid_ndf'].shift(1)\n",
    "df['스왑포인트_1M'] = df[\"M1_스왑포인트\"]/100 \n",
    "df['전일종가_ex'] = df['종가_ex'].shift(1)\n",
    "df['종가_NDF_차이'] = df['전일종가_ex'] - df['Mid_ndf']\n",
    "\n",
    "# 필요한 칼럼만 추출\n",
    "df_1 = df[['대비_irs_1Y', '대비_irs_2Y', '대비_irs_3Y', '대비_irs_5Y', '대비_irs_10Y',\n",
    "           '대비_crs_1Y', '대비_crs_2Y', '대비_crs_3Y', '대비_crs_5Y', '대비_crs_10Y', \n",
    "           '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y', '대비_swapbasis_10Y',\n",
    "           '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y', \n",
    "           '대비_통안_1Y', '대비_통안_2Y', '대비_ndf', '스왑포인트_1M', '전일종가_ex', \n",
    "           '종가_ex', '종가_NDF_차이' ]] \n",
    "\n",
    "# 결측치 제거\n",
    "df_1 = df_1.dropna()                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_swapbasis_1Y</th>\n",
       "      <th>대비_swapbasis_2Y</th>\n",
       "      <th>대비_swapbasis_3Y</th>\n",
       "      <th>대비_swapbasis_5Y</th>\n",
       "      <th>대비_swapbasis_10Y</th>\n",
       "      <th>대비_국고_1Y</th>\n",
       "      <th>대비_국고_3Y</th>\n",
       "      <th>대비_국고_5Y</th>\n",
       "      <th>대비_국고_10Y</th>\n",
       "      <th>대비_통안_1Y</th>\n",
       "      <th>대비_통안_2Y</th>\n",
       "      <th>스왑포인트_1M</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_NDF_차이</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>1.544080</td>\n",
       "      <td>1.437744</td>\n",
       "      <td>1.627899</td>\n",
       "      <td>1.488363</td>\n",
       "      <td>-1.698057</td>\n",
       "      <td>-0.646622</td>\n",
       "      <td>-1.079749</td>\n",
       "      <td>-1.027569</td>\n",
       "      <td>-0.325920</td>\n",
       "      <td>-0.625160</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>-1.648743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.286831</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>-0.910410</td>\n",
       "      <td>-2.158344</td>\n",
       "      <td>-1.132651</td>\n",
       "      <td>-0.323869</td>\n",
       "      <td>-1.890219</td>\n",
       "      <td>-1.798403</td>\n",
       "      <td>-0.217574</td>\n",
       "      <td>-0.125723</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>-1.366022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350946</td>\n",
       "      <td>-0.873707</td>\n",
       "      <td>-0.803456</td>\n",
       "      <td>-1.091718</td>\n",
       "      <td>-0.832269</td>\n",
       "      <td>0.563566</td>\n",
       "      <td>0.160261</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.107465</td>\n",
       "      <td>0.123996</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>1.602547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>0.286831</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>-1.454333</td>\n",
       "      <td>-1.661066</td>\n",
       "      <td>-0.567245</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>-0.109228</td>\n",
       "      <td>-0.125723</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>0.118263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.390150</td>\n",
       "      <td>-0.323199</td>\n",
       "      <td>-0.729102</td>\n",
       "      <td>-1.163788</td>\n",
       "      <td>-0.567245</td>\n",
       "      <td>-0.323869</td>\n",
       "      <td>-0.539435</td>\n",
       "      <td>-0.513680</td>\n",
       "      <td>-0.109228</td>\n",
       "      <td>-0.125723</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>-0.223358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-0.196727</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>-0.366487</td>\n",
       "      <td>-0.003472</td>\n",
       "      <td>-1.132651</td>\n",
       "      <td>-0.969375</td>\n",
       "      <td>-1.890219</td>\n",
       "      <td>-2.312292</td>\n",
       "      <td>-0.109228</td>\n",
       "      <td>-0.625160</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>0.860405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.196727</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>0.177437</td>\n",
       "      <td>0.162288</td>\n",
       "      <td>0.563566</td>\n",
       "      <td>-0.485246</td>\n",
       "      <td>-0.539435</td>\n",
       "      <td>-0.770625</td>\n",
       "      <td>0.107465</td>\n",
       "      <td>-0.000863</td>\n",
       "      <td>-0.987488</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>0.754385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>0.770388</td>\n",
       "      <td>0.797401</td>\n",
       "      <td>0.902668</td>\n",
       "      <td>0.825326</td>\n",
       "      <td>-0.001839</td>\n",
       "      <td>-0.485246</td>\n",
       "      <td>-1.349905</td>\n",
       "      <td>-1.541458</td>\n",
       "      <td>0.215812</td>\n",
       "      <td>-0.125723</td>\n",
       "      <td>-0.851696</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>-0.564979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>-0.003304</td>\n",
       "      <td>-0.163113</td>\n",
       "      <td>-0.366487</td>\n",
       "      <td>-0.832269</td>\n",
       "      <td>0.563566</td>\n",
       "      <td>0.644391</td>\n",
       "      <td>0.811350</td>\n",
       "      <td>2.055766</td>\n",
       "      <td>0.215812</td>\n",
       "      <td>0.373715</td>\n",
       "      <td>-0.942224</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>1.838148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-0.583573</td>\n",
       "      <td>-0.163113</td>\n",
       "      <td>0.902668</td>\n",
       "      <td>0.493807</td>\n",
       "      <td>-2.263463</td>\n",
       "      <td>-2.099012</td>\n",
       "      <td>-3.241004</td>\n",
       "      <td>-2.055348</td>\n",
       "      <td>-0.325920</td>\n",
       "      <td>-1.374316</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>0.200723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_swapbasis_1Y  대비_swapbasis_2Y  대비_swapbasis_3Y  \\\n",
       "DateTime                                                        \n",
       "2012-08-02         0.348741         1.544080         1.437744   \n",
       "2012-08-03         0.348741         0.286831         0.157058   \n",
       "2012-08-06        -0.350946        -0.873707        -0.803456   \n",
       "2012-08-07         0.173819         0.286831        -0.003027   \n",
       "2012-08-08        -0.001103        -0.390150        -0.323199   \n",
       "...                     ...              ...              ...   \n",
       "2022-07-25        -0.700790        -0.196727        -0.003027   \n",
       "2022-07-26         0.348741        -0.196727        -0.003027   \n",
       "2022-07-27         0.348741         0.770388         0.797401   \n",
       "2022-07-28         0.173819        -0.003304        -0.163113   \n",
       "2022-07-29        -0.700790        -0.583573        -0.163113   \n",
       "\n",
       "            대비_swapbasis_5Y  대비_swapbasis_10Y  대비_국고_1Y  대비_국고_3Y  대비_국고_5Y  \\\n",
       "DateTime                                                                      \n",
       "2012-08-02         1.627899          1.488363 -1.698057 -0.646622 -1.079749   \n",
       "2012-08-03        -0.910410         -2.158344 -1.132651 -0.323869 -1.890219   \n",
       "2012-08-06        -1.091718         -0.832269  0.563566  0.160261  0.000879   \n",
       "2012-08-07        -1.454333         -1.661066 -0.567245 -0.001116  0.000879   \n",
       "2012-08-08        -0.729102         -1.163788 -0.567245 -0.323869 -0.539435   \n",
       "...                     ...               ...       ...       ...       ...   \n",
       "2022-07-25        -0.366487         -0.003472 -1.132651 -0.969375 -1.890219   \n",
       "2022-07-26         0.177437          0.162288  0.563566 -0.485246 -0.539435   \n",
       "2022-07-27         0.902668          0.825326 -0.001839 -0.485246 -1.349905   \n",
       "2022-07-28        -0.366487         -0.832269  0.563566  0.644391  0.811350   \n",
       "2022-07-29         0.902668          0.493807 -2.263463 -2.099012 -3.241004   \n",
       "\n",
       "            대비_국고_10Y  대비_통안_1Y  대비_통안_2Y  스왑포인트_1M   전일종가_ex  종가_NDF_차이  \n",
       "DateTime                                                                  \n",
       "2012-08-02  -1.027569 -0.325920 -0.625160  1.909409 -0.149841  -1.648743  \n",
       "2012-08-03  -1.798403 -0.217574 -0.125723  1.818881 -0.056232  -1.366022  \n",
       "2012-08-06   0.000209  0.107465  0.123996  1.818881 -0.000426   1.602547  \n",
       "2012-08-07   0.000209 -0.109228 -0.125723  1.909409 -0.104837   0.118263  \n",
       "2012-08-08  -0.513680 -0.109228 -0.125723  1.818881 -0.108437  -0.223358  \n",
       "...               ...       ...       ...       ...       ...        ...  \n",
       "2022-07-25  -2.312292 -0.109228 -0.625160 -0.896960  3.207485   0.860405  \n",
       "2022-07-26  -0.770625  0.107465 -0.000863 -0.987488  3.220086   0.754385  \n",
       "2022-07-27  -1.541458  0.215812 -0.125723 -0.851696  3.110275  -0.564979  \n",
       "2022-07-28   2.055766  0.215812  0.373715 -0.942224  3.212885   1.838148  \n",
       "2022-07-29  -2.055348 -0.325920 -1.374316 -0.896960  2.903255   0.200723  \n",
       "\n",
       "[2459 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 쓸 칼럼만 남기고 feature, target 분리해 각각 x,y 에 저장\n",
    "x = df_1[[ '대비_swapbasis_1Y', '대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y', '대비_swapbasis_10Y',\n",
    "           '대비_국고_1Y', '대비_국고_3Y', '대비_국고_5Y', '대비_국고_10Y', \n",
    "           '대비_통안_1Y', '대비_통안_2Y', '스왑포인트_1M', '전일종가_ex', \n",
    "           '종가_NDF_차이']]\n",
    "y = df_1[['종가_ex']]\n",
    "\n",
    "# 이건 이렇게 해야 밑에 코드 8번째 줄 columns에 들어갈 수 있다고 하네요!\n",
    "x.feature = x.columns \n",
    "x.feature\n",
    "\n",
    "# scaling 진행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.\n",
    "scaler.fit(x)\n",
    "data_scaled = scaler.transform(x)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "x_scaled = pd.DataFrame(data = data_scaled, columns=x.feature)\n",
    "x_scaled.index = y.index # 인덱스가 달라서 똑같이 설정\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF_Factor           Feature\n",
      "0     3.169185   대비_swapbasis_1Y\n",
      "1     5.912724   대비_swapbasis_2Y\n",
      "2     2.407206   대비_swapbasis_3Y\n",
      "3     4.772400   대비_swapbasis_5Y\n",
      "4     3.269360  대비_swapbasis_10Y\n",
      "5     1.830986          대비_국고_1Y\n",
      "6     1.245142          대비_국고_3Y\n",
      "7     5.830426          대비_국고_5Y\n",
      "8     4.886685         대비_국고_10Y\n",
      "9     1.024620          대비_통안_1Y\n",
      "10    1.114166          대비_통안_2Y\n",
      "11    1.246756          스왑포인트_1M\n",
      "12    1.189179           전일종가_ex\n",
      "13    1.150172         종가_NDF_차이\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.260e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 17 Oct 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:28:19</td>     <th>  Log-Likelihood:    </th> <td> -6935.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.390e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2444</td>      <th>  BIC:               </th> <td>1.399e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    14</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8939</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.733</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.8203</td> <td>    0.146</td> <td>   -5.609</td> <td> 0.000</td> <td>   -1.107</td> <td>   -0.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_2Y</th>  <td>    0.0651</td> <td>    0.200</td> <td>    0.326</td> <td> 0.744</td> <td>   -0.327</td> <td>    0.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_3Y</th>  <td>   -0.1486</td> <td>    0.127</td> <td>   -1.166</td> <td> 0.244</td> <td>   -0.399</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_5Y</th>  <td>   -0.1569</td> <td>    0.179</td> <td>   -0.874</td> <td> 0.382</td> <td>   -0.509</td> <td>    0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.2510</td> <td>    0.149</td> <td>    1.690</td> <td> 0.091</td> <td>   -0.040</td> <td>    0.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>   -0.0954</td> <td>    0.111</td> <td>   -0.858</td> <td> 0.391</td> <td>   -0.313</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.1922</td> <td>    0.092</td> <td>   -2.096</td> <td> 0.036</td> <td>   -0.372</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>   -0.0157</td> <td>    0.198</td> <td>   -0.079</td> <td> 0.937</td> <td>   -0.405</td> <td>    0.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.0323</td> <td>    0.182</td> <td>   -0.178</td> <td> 0.859</td> <td>   -0.388</td> <td>    0.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_1Y</th>         <td>   -0.0087</td> <td>    0.083</td> <td>   -0.104</td> <td> 0.917</td> <td>   -0.172</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0318</td> <td>    0.087</td> <td>    0.367</td> <td> 0.713</td> <td>   -0.138</td> <td>    0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1897</td> <td>    0.092</td> <td>  -12.969</td> <td> 0.000</td> <td>   -1.370</td> <td>   -1.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7341</td> <td>    0.090</td> <td>  622.093</td> <td> 0.000</td> <td>   55.558</td> <td>   55.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0157</td> <td>    0.088</td> <td>  -45.576</td> <td> 0.000</td> <td>   -4.188</td> <td>   -3.843</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>243.950</td> <th>  Durbin-Watson:     </th> <td>   2.099</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1184.432</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.349</td>  <th>  Prob(JB):          </th> <td>6.36e-258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.328</td>  <th>  Cond. No.          </th> <td>    6.54</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 3.260e+04\n",
       "Date:                Mon, 17 Oct 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:28:19   Log-Likelihood:                -6935.6\n",
       "No. Observations:                2459   AIC:                         1.390e+04\n",
       "Df Residuals:                    2444   BIC:                         1.399e+04\n",
       "Df Model:                          14                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8939      0.082   1.38e+04      0.000    1134.733    1135.055\n",
       "대비_swapbasis_1Y     -0.8203      0.146     -5.609      0.000      -1.107      -0.534\n",
       "대비_swapbasis_2Y      0.0651      0.200      0.326      0.744      -0.327       0.457\n",
       "대비_swapbasis_3Y     -0.1486      0.127     -1.166      0.244      -0.399       0.101\n",
       "대비_swapbasis_5Y     -0.1569      0.179     -0.874      0.382      -0.509       0.195\n",
       "대비_swapbasis_10Y     0.2510      0.149      1.690      0.091      -0.040       0.542\n",
       "대비_국고_1Y            -0.0954      0.111     -0.858      0.391      -0.313       0.123\n",
       "대비_국고_3Y            -0.1922      0.092     -2.096      0.036      -0.372      -0.012\n",
       "대비_국고_5Y            -0.0157      0.198     -0.079      0.937      -0.405       0.373\n",
       "대비_국고_10Y           -0.0323      0.182     -0.178      0.859      -0.388       0.324\n",
       "대비_통안_1Y            -0.0087      0.083     -0.104      0.917      -0.172       0.154\n",
       "대비_통안_2Y             0.0318      0.087      0.367      0.713      -0.138       0.202\n",
       "스왑포인트_1M            -1.1897      0.092    -12.969      0.000      -1.370      -1.010\n",
       "전일종가_ex             55.7341      0.090    622.093      0.000      55.558      55.910\n",
       "종가_NDF_차이           -4.0157      0.088    -45.576      0.000      -4.188      -3.843\n",
       "==============================================================================\n",
       "Omnibus:                      243.950   Durbin-Watson:                   2.099\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1184.432\n",
       "Skew:                           0.349   Prob(JB):                    6.36e-258\n",
       "Kurtosis:                       6.328   Cond. No.                         6.54\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>4.149e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 17 Oct 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:28:19</td>     <th>  Log-Likelihood:    </th> <td> -6937.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.390e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2447</td>      <th>  BIC:               </th> <td>1.397e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8939</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.733</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.8915</td> <td>    0.107</td> <td>   -8.341</td> <td> 0.000</td> <td>   -1.101</td> <td>   -0.682</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.1240</td> <td>    0.109</td> <td>    1.139</td> <td> 0.255</td> <td>   -0.090</td> <td>    0.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>   -0.0886</td> <td>    0.111</td> <td>   -0.798</td> <td> 0.425</td> <td>   -0.306</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.1890</td> <td>    0.092</td> <td>   -2.064</td> <td> 0.039</td> <td>   -0.369</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>   -0.0047</td> <td>    0.198</td> <td>   -0.024</td> <td> 0.981</td> <td>   -0.393</td> <td>    0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.0347</td> <td>    0.182</td> <td>   -0.191</td> <td> 0.848</td> <td>   -0.391</td> <td>    0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_1Y</th>         <td>   -0.0084</td> <td>    0.083</td> <td>   -0.101</td> <td> 0.919</td> <td>   -0.171</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0340</td> <td>    0.087</td> <td>    0.392</td> <td> 0.695</td> <td>   -0.136</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1893</td> <td>    0.092</td> <td>  -12.966</td> <td> 0.000</td> <td>   -1.369</td> <td>   -1.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7336</td> <td>    0.090</td> <td>  622.137</td> <td> 0.000</td> <td>   55.558</td> <td>   55.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0131</td> <td>    0.088</td> <td>  -45.583</td> <td> 0.000</td> <td>   -4.186</td> <td>   -3.840</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>245.412</td> <th>  Durbin-Watson:     </th> <td>   2.095</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1189.383</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.354</td>  <th>  Prob(JB):          </th> <td>5.36e-259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.333</td>  <th>  Cond. No.          </th> <td>    5.43</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 4.149e+04\n",
       "Date:                Mon, 17 Oct 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:28:19   Log-Likelihood:                -6937.0\n",
       "No. Observations:                2459   AIC:                         1.390e+04\n",
       "Df Residuals:                    2447   BIC:                         1.397e+04\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8939      0.082   1.38e+04      0.000    1134.733    1135.055\n",
       "대비_swapbasis_1Y     -0.8915      0.107     -8.341      0.000      -1.101      -0.682\n",
       "대비_swapbasis_10Y     0.1240      0.109      1.139      0.255      -0.090       0.338\n",
       "대비_국고_1Y            -0.0886      0.111     -0.798      0.425      -0.306       0.129\n",
       "대비_국고_3Y            -0.1890      0.092     -2.064      0.039      -0.369      -0.009\n",
       "대비_국고_5Y            -0.0047      0.198     -0.024      0.981      -0.393       0.384\n",
       "대비_국고_10Y           -0.0347      0.182     -0.191      0.848      -0.391       0.321\n",
       "대비_통안_1Y            -0.0084      0.083     -0.101      0.919      -0.171       0.155\n",
       "대비_통안_2Y             0.0340      0.087      0.392      0.695      -0.136       0.204\n",
       "스왑포인트_1M            -1.1893      0.092    -12.966      0.000      -1.369      -1.009\n",
       "전일종가_ex             55.7336      0.090    622.137      0.000      55.558      55.909\n",
       "종가_NDF_차이           -4.0131      0.088    -45.583      0.000      -4.186      -3.840\n",
       "==============================================================================\n",
       "Omnibus:                      245.412   Durbin-Watson:                   2.095\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1189.383\n",
       "Skew:                           0.354   Prob(JB):                    5.36e-259\n",
       "Kurtosis:                       6.333   Cond. No.                         5.43\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_2Y', '대비_swapbasis_3Y', '대비_swapbasis_5Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VIF_Factor           Feature\n",
      "0    1.692389   대비_swapbasis_1Y\n",
      "1    1.757274  대비_swapbasis_10Y\n",
      "2    1.814188          대비_국고_1Y\n",
      "3    1.243174          대비_국고_3Y\n",
      "4    5.812902          대비_국고_5Y\n",
      "5    4.882217         대비_국고_10Y\n",
      "6    1.113392          대비_통안_2Y\n",
      "7    1.246520          스왑포인트_1M\n",
      "8    1.189005           전일종가_ex\n",
      "9    1.148435         종가_NDF_차이\n"
     ]
    }
   ],
   "source": [
    "x_scaled.drop(['대비_통안_1Y'], axis=1, inplace=True)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_train = x_scaled\n",
    "def feature_engineering_XbyVIF(X_train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF_Factor\"] = [variance_inflation_factor(X_train.values,i)\n",
    "                         for i in range(X_train.shape[1])]\n",
    "    vif[\"Feature\"] = X_train.columns\n",
    "    return vif\n",
    "vif = feature_engineering_XbyVIF(X_train)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>4.566e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 17 Oct 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:28:20</td>     <th>  Log-Likelihood:    </th> <td> -6937.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.390e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2448</td>      <th>  BIC:               </th> <td>1.396e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8939</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.733</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.8914</td> <td>    0.107</td> <td>   -8.343</td> <td> 0.000</td> <td>   -1.101</td> <td>   -0.682</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.1240</td> <td>    0.109</td> <td>    1.139</td> <td> 0.255</td> <td>   -0.090</td> <td>    0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_1Y</th>         <td>   -0.0895</td> <td>    0.111</td> <td>   -0.809</td> <td> 0.418</td> <td>   -0.306</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.1891</td> <td>    0.092</td> <td>   -2.065</td> <td> 0.039</td> <td>   -0.369</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>   -0.0052</td> <td>    0.198</td> <td>   -0.026</td> <td> 0.979</td> <td>   -0.393</td> <td>    0.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.0347</td> <td>    0.181</td> <td>   -0.191</td> <td> 0.848</td> <td>   -0.391</td> <td>    0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_통안_2Y</th>         <td>    0.0339</td> <td>    0.087</td> <td>    0.391</td> <td> 0.696</td> <td>   -0.136</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1893</td> <td>    0.092</td> <td>  -12.969</td> <td> 0.000</td> <td>   -1.369</td> <td>   -1.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7335</td> <td>    0.090</td> <td>  622.289</td> <td> 0.000</td> <td>   55.558</td> <td>   55.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0131</td> <td>    0.088</td> <td>  -45.593</td> <td> 0.000</td> <td>   -4.186</td> <td>   -3.840</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>245.403</td> <th>  Durbin-Watson:     </th> <td>   2.095</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1189.518</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.353</td>  <th>  Prob(JB):          </th> <td>5.01e-259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.333</td>  <th>  Cond. No.          </th> <td>    5.40</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 4.566e+04\n",
       "Date:                Mon, 17 Oct 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:28:20   Log-Likelihood:                -6937.0\n",
       "No. Observations:                2459   AIC:                         1.390e+04\n",
       "Df Residuals:                    2448   BIC:                         1.396e+04\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8939      0.082   1.38e+04      0.000    1134.733    1135.055\n",
       "대비_swapbasis_1Y     -0.8914      0.107     -8.343      0.000      -1.101      -0.682\n",
       "대비_swapbasis_10Y     0.1240      0.109      1.139      0.255      -0.090       0.337\n",
       "대비_국고_1Y            -0.0895      0.111     -0.809      0.418      -0.306       0.127\n",
       "대비_국고_3Y            -0.1891      0.092     -2.065      0.039      -0.369      -0.010\n",
       "대비_국고_5Y            -0.0052      0.198     -0.026      0.979      -0.393       0.383\n",
       "대비_국고_10Y           -0.0347      0.181     -0.191      0.848      -0.391       0.321\n",
       "대비_통안_2Y             0.0339      0.087      0.391      0.696      -0.136       0.204\n",
       "스왑포인트_1M            -1.1893      0.092    -12.969      0.000      -1.369      -1.009\n",
       "전일종가_ex             55.7335      0.090    622.289      0.000      55.558      55.909\n",
       "종가_NDF_차이           -4.0131      0.088    -45.593      0.000      -4.186      -3.840\n",
       "==============================================================================\n",
       "Omnibus:                      245.403   Durbin-Watson:                   2.095\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1189.518\n",
       "Skew:                           0.353   Prob(JB):                    5.01e-259\n",
       "Kurtosis:                       6.333   Cond. No.                         5.40\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>5.710e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 17 Oct 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:28:20</td>     <th>  Log-Likelihood:    </th> <td> -6937.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.389e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2450</td>      <th>  BIC:               </th> <td>1.395e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8939</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.733</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.8892</td> <td>    0.107</td> <td>   -8.330</td> <td> 0.000</td> <td>   -1.099</td> <td>   -0.680</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.1239</td> <td>    0.109</td> <td>    1.139</td> <td> 0.255</td> <td>   -0.090</td> <td>    0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.1984</td> <td>    0.091</td> <td>   -2.190</td> <td> 0.029</td> <td>   -0.376</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>   -0.0488</td> <td>    0.186</td> <td>   -0.262</td> <td> 0.794</td> <td>   -0.414</td> <td>    0.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_10Y</th>        <td>   -0.0354</td> <td>    0.181</td> <td>   -0.195</td> <td> 0.846</td> <td>   -0.391</td> <td>    0.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1903</td> <td>    0.092</td> <td>  -12.984</td> <td> 0.000</td> <td>   -1.370</td> <td>   -1.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7275</td> <td>    0.089</td> <td>  624.695</td> <td> 0.000</td> <td>   55.553</td> <td>   55.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0131</td> <td>    0.088</td> <td>  -45.611</td> <td> 0.000</td> <td>   -4.186</td> <td>   -3.841</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>250.440</td> <th>  Durbin-Watson:     </th> <td>   2.093</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1227.804</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.362</td>  <th>  Prob(JB):          </th> <td>2.43e-267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.385</td>  <th>  Cond. No.          </th> <td>    4.72</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 5.710e+04\n",
       "Date:                Mon, 17 Oct 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:28:20   Log-Likelihood:                -6937.4\n",
       "No. Observations:                2459   AIC:                         1.389e+04\n",
       "Df Residuals:                    2450   BIC:                         1.395e+04\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8939      0.082   1.38e+04      0.000    1134.733    1135.055\n",
       "대비_swapbasis_1Y     -0.8892      0.107     -8.330      0.000      -1.099      -0.680\n",
       "대비_swapbasis_10Y     0.1239      0.109      1.139      0.255      -0.090       0.337\n",
       "대비_국고_3Y            -0.1984      0.091     -2.190      0.029      -0.376      -0.021\n",
       "대비_국고_5Y            -0.0488      0.186     -0.262      0.794      -0.414       0.317\n",
       "대비_국고_10Y           -0.0354      0.181     -0.195      0.846      -0.391       0.320\n",
       "스왑포인트_1M            -1.1903      0.092    -12.984      0.000      -1.370      -1.010\n",
       "전일종가_ex             55.7275      0.089    624.695      0.000      55.553      55.902\n",
       "종가_NDF_차이           -4.0131      0.088    -45.611      0.000      -4.186      -3.841\n",
       "==============================================================================\n",
       "Omnibus:                      250.440   Durbin-Watson:                   2.093\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1227.804\n",
       "Skew:                           0.362   Prob(JB):                    2.43e-267\n",
       "Kurtosis:                       6.385   Cond. No.                         4.72\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_통안_2Y','대비_국고_1Y' ], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>6.529e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 17 Oct 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:28:20</td>     <th>  Log-Likelihood:    </th> <td> -6937.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.389e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2451</td>      <th>  BIC:               </th> <td>1.394e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1134.8939</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.733</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th>  <td>   -0.8903</td> <td>    0.107</td> <td>   -8.352</td> <td> 0.000</td> <td>   -1.099</td> <td>   -0.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_10Y</th> <td>    0.1255</td> <td>    0.108</td> <td>    1.157</td> <td> 0.247</td> <td>   -0.087</td> <td>    0.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>         <td>   -0.1980</td> <td>    0.091</td> <td>   -2.188</td> <td> 0.029</td> <td>   -0.376</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>         <td>   -0.0802</td> <td>    0.094</td> <td>   -0.857</td> <td> 0.391</td> <td>   -0.264</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>         <td>   -1.1902</td> <td>    0.092</td> <td>  -12.986</td> <td> 0.000</td> <td>   -1.370</td> <td>   -1.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>          <td>   55.7276</td> <td>    0.089</td> <td>  624.823</td> <td> 0.000</td> <td>   55.553</td> <td>   55.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>        <td>   -4.0136</td> <td>    0.088</td> <td>  -45.644</td> <td> 0.000</td> <td>   -4.186</td> <td>   -3.841</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>250.759</td> <th>  Durbin-Watson:     </th> <td>   2.093</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1231.080</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.362</td>  <th>  Prob(JB):          </th> <td>4.73e-268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.390</td>  <th>  Cond. No.          </th> <td>    2.33</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 6.529e+04\n",
       "Date:                Mon, 17 Oct 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:28:20   Log-Likelihood:                -6937.4\n",
       "No. Observations:                2459   AIC:                         1.389e+04\n",
       "Df Residuals:                    2451   BIC:                         1.394e+04\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1134.8939      0.082   1.38e+04      0.000    1134.733    1135.055\n",
       "대비_swapbasis_1Y     -0.8903      0.107     -8.352      0.000      -1.099      -0.681\n",
       "대비_swapbasis_10Y     0.1255      0.108      1.157      0.247      -0.087       0.338\n",
       "대비_국고_3Y            -0.1980      0.091     -2.188      0.029      -0.376      -0.021\n",
       "대비_국고_5Y            -0.0802      0.094     -0.857      0.391      -0.264       0.103\n",
       "스왑포인트_1M            -1.1902      0.092    -12.986      0.000      -1.370      -1.011\n",
       "전일종가_ex             55.7276      0.089    624.823      0.000      55.553      55.903\n",
       "종가_NDF_차이           -4.0136      0.088    -45.644      0.000      -4.186      -3.841\n",
       "==============================================================================\n",
       "Omnibus:                      250.759   Durbin-Watson:                   2.093\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1231.080\n",
       "Skew:                           0.362   Prob(JB):                    4.73e-268\n",
       "Kurtosis:                       6.390   Cond. No.                         2.33\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_국고_10Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>종가_ex</td>      <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>7.616e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 17 Oct 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:28:20</td>     <th>  Log-Likelihood:    </th> <td> -6938.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2459</td>      <th>  AIC:               </th> <td>1.389e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2452</td>      <th>  BIC:               </th> <td>1.393e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td> 1134.8939</td> <td>    0.082</td> <td> 1.38e+04</td> <td> 0.000</td> <td> 1134.733</td> <td> 1135.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_swapbasis_1Y</th> <td>   -0.8133</td> <td>    0.083</td> <td>   -9.764</td> <td> 0.000</td> <td>   -0.977</td> <td>   -0.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_3Y</th>        <td>   -0.1967</td> <td>    0.091</td> <td>   -2.173</td> <td> 0.030</td> <td>   -0.374</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>대비_국고_5Y</th>        <td>   -0.1034</td> <td>    0.091</td> <td>   -1.132</td> <td> 0.258</td> <td>   -0.283</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>스왑포인트_1M</th>        <td>   -1.1922</td> <td>    0.092</td> <td>  -13.008</td> <td> 0.000</td> <td>   -1.372</td> <td>   -1.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>전일종가_ex</th>         <td>   55.7264</td> <td>    0.089</td> <td>  624.808</td> <td> 0.000</td> <td>   55.552</td> <td>   55.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>종가_NDF_차이</th>       <td>   -4.0161</td> <td>    0.088</td> <td>  -45.683</td> <td> 0.000</td> <td>   -4.188</td> <td>   -3.844</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>251.487</td> <th>  Durbin-Watson:     </th> <td>   2.097</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1235.400</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.364</td>  <th>  Prob(JB):          </th> <td>5.45e-269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.395</td>  <th>  Cond. No.          </th> <td>    1.71</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  종가_ex   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 7.616e+04\n",
       "Date:                Mon, 17 Oct 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:28:20   Log-Likelihood:                -6938.1\n",
       "No. Observations:                2459   AIC:                         1.389e+04\n",
       "Df Residuals:                    2452   BIC:                         1.393e+04\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const            1134.8939      0.082   1.38e+04      0.000    1134.733    1135.055\n",
       "대비_swapbasis_1Y    -0.8133      0.083     -9.764      0.000      -0.977      -0.650\n",
       "대비_국고_3Y           -0.1967      0.091     -2.173      0.030      -0.374      -0.019\n",
       "대비_국고_5Y           -0.1034      0.091     -1.132      0.258      -0.283       0.076\n",
       "스왑포인트_1M           -1.1922      0.092    -13.008      0.000      -1.372      -1.012\n",
       "전일종가_ex            55.7264      0.089    624.808      0.000      55.552      55.901\n",
       "종가_NDF_차이          -4.0161      0.088    -45.683      0.000      -4.188      -3.844\n",
       "==============================================================================\n",
       "Omnibus:                      251.487   Durbin-Watson:                   2.097\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1235.400\n",
       "Skew:                           0.364   Prob(JB):                    5.45e-269\n",
       "Kurtosis:                       6.395   Cond. No.                         1.71\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.drop(['대비_swapbasis_10Y'], axis=1, inplace=True)\n",
    "\n",
    "feature_add = sm.add_constant(x_scaled, has_constant='add')\n",
    "\n",
    "# sm OLS 적합\n",
    "model = sm.OLS(y , feature_add)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# summary 함수통해 결과출력\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대비_swapbasis_1Y</th>\n",
       "      <th>대비_국고_3Y</th>\n",
       "      <th>대비_국고_5Y</th>\n",
       "      <th>스왑포인트_1M</th>\n",
       "      <th>전일종가_ex</th>\n",
       "      <th>종가_NDF_차이</th>\n",
       "      <th>종가_ex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-08-02</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.646622</td>\n",
       "      <td>-1.079749</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.149841</td>\n",
       "      <td>-1.648743</td>\n",
       "      <td>1131.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-03</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.323869</td>\n",
       "      <td>-1.890219</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>-1.366022</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-06</th>\n",
       "      <td>-0.350946</td>\n",
       "      <td>0.160261</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>1.602547</td>\n",
       "      <td>1129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-07</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>0.118263</td>\n",
       "      <td>1128.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-08</th>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.323869</td>\n",
       "      <td>-0.539435</td>\n",
       "      <td>1.818881</td>\n",
       "      <td>-0.108437</td>\n",
       "      <td>-0.223358</td>\n",
       "      <td>1128.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-25</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-0.969375</td>\n",
       "      <td>-1.890219</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>3.207485</td>\n",
       "      <td>0.860405</td>\n",
       "      <td>1313.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-26</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.485246</td>\n",
       "      <td>-0.539435</td>\n",
       "      <td>-0.987488</td>\n",
       "      <td>3.220086</td>\n",
       "      <td>0.754385</td>\n",
       "      <td>1307.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-27</th>\n",
       "      <td>0.348741</td>\n",
       "      <td>-0.485246</td>\n",
       "      <td>-1.349905</td>\n",
       "      <td>-0.851696</td>\n",
       "      <td>3.110275</td>\n",
       "      <td>-0.564979</td>\n",
       "      <td>1313.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-28</th>\n",
       "      <td>0.173819</td>\n",
       "      <td>0.644391</td>\n",
       "      <td>0.811350</td>\n",
       "      <td>-0.942224</td>\n",
       "      <td>3.212885</td>\n",
       "      <td>1.838148</td>\n",
       "      <td>1296.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>-0.700790</td>\n",
       "      <td>-2.099012</td>\n",
       "      <td>-3.241004</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>2.903255</td>\n",
       "      <td>0.200723</td>\n",
       "      <td>1299.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            대비_swapbasis_1Y  대비_국고_3Y  대비_국고_5Y  스왑포인트_1M   전일종가_ex  \\\n",
       "DateTime                                                              \n",
       "2012-08-02         0.348741 -0.646622 -1.079749  1.909409 -0.149841   \n",
       "2012-08-03         0.348741 -0.323869 -1.890219  1.818881 -0.056232   \n",
       "2012-08-06        -0.350946  0.160261  0.000879  1.818881 -0.000426   \n",
       "2012-08-07         0.173819 -0.001116  0.000879  1.909409 -0.104837   \n",
       "2012-08-08        -0.001103 -0.323869 -0.539435  1.818881 -0.108437   \n",
       "...                     ...       ...       ...       ...       ...   \n",
       "2022-07-25        -0.700790 -0.969375 -1.890219 -0.896960  3.207485   \n",
       "2022-07-26         0.348741 -0.485246 -0.539435 -0.987488  3.220086   \n",
       "2022-07-27         0.348741 -0.485246 -1.349905 -0.851696  3.110275   \n",
       "2022-07-28         0.173819  0.644391  0.811350 -0.942224  3.212885   \n",
       "2022-07-29        -0.700790 -2.099012 -3.241004 -0.896960  2.903255   \n",
       "\n",
       "            종가_NDF_차이   종가_ex  \n",
       "DateTime                       \n",
       "2012-08-02  -1.648743  1131.7  \n",
       "2012-08-03  -1.366022  1134.8  \n",
       "2012-08-06   1.602547  1129.0  \n",
       "2012-08-07   0.118263  1128.8  \n",
       "2012-08-08  -0.223358  1128.3  \n",
       "...               ...     ...  \n",
       "2022-07-25   0.860405  1313.7  \n",
       "2022-07-26   0.754385  1307.6  \n",
       "2022-07-27  -0.564979  1313.3  \n",
       "2022-07-28   1.838148  1296.1  \n",
       "2022-07-29   0.200723  1299.1  \n",
       "\n",
       "[2459 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = pd.concat([x_scaled,y], axis=1)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_scaled[0:1945]\n",
    "test = df_scaled[1945:]\n",
    "\n",
    "def make_dataset(data, label, window_size=1):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "feature_cols = ['대비_swapbasis_1Y', '대비_국고_3Y', '대비_국고_5Y', '스왑포인트_1M', '전일종가_ex', '종가_NDF_차이']\n",
    "label_cols = ['종가_ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1555, 1, 6), (389, 1, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]\n",
    "\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, 1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.73819238e-01, -1.11565786e-03,  8.78916251e-04,\n",
       "          4.15696260e-01,  5.57627728e-01, -2.20240440e+00]],\n",
       "\n",
       "       [[-5.25868106e-01, -3.23868914e-01, -2.69277966e-01,\n",
       "          9.58864361e-01, -6.17886362e-01, -2.35138425e-01]],\n",
       "\n",
       "       [[-1.10259799e-03, -1.11565786e-03,  8.78916251e-04,\n",
       "         -7.61167960e-01, -2.95655088e-01,  3.30303293e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.50946270e-01, -1.11565786e-03, -2.69277966e-01,\n",
       "         -8.22078332e-02,  1.37374049e-03, -1.88018282e-01]],\n",
       "\n",
       "       [[-1.10259799e-03,  3.21637599e-01,  5.41192681e-01,\n",
       "          5.96752294e-01, -2.48850545e-01, -2.82674630e+00]],\n",
       "\n",
       "       [[ 3.48741074e-01,  4.83014227e-01,  8.78916251e-04,\n",
       "          1.41150445e+00, -2.86654214e-01,  2.83183150e-01]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((513, 1, 6), (513, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature, test_label = make_dataset(test_feature, test_label, 1)\n",
    "test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 1267958.0000 - mae: 1124.9081\n",
      "Epoch 1: val_loss improved from inf to 1260448.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 15s 93ms/step - loss: 1268143.7500 - mae: 1124.9962 - val_loss: 1260448.7500 - val_mae: 1121.5050\n",
      "Epoch 2/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1267916.7500 - mae: 1124.9049\n",
      "Epoch 2: val_loss improved from 1260448.75000 to 1258769.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 1267023.8750 - mae: 1124.4957 - val_loss: 1258769.2500 - val_mae: 1120.7498\n",
      "Epoch 3/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1264180.7500 - mae: 1123.2174\n",
      "Epoch 3: val_loss improved from 1258769.25000 to 1255480.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 1264783.2500 - mae: 1123.4907 - val_loss: 1255480.7500 - val_mae: 1119.2668\n",
      "Epoch 4/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1260800.6250 - mae: 1121.7053\n",
      "Epoch 4: val_loss improved from 1255480.75000 to 1250171.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1260808.2500 - mae: 1121.7063 - val_loss: 1250171.7500 - val_mae: 1116.8737\n",
      "Epoch 5/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 1254943.8750 - mae: 1119.0620\n",
      "Epoch 5: val_loss improved from 1250171.75000 to 1242984.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1254996.1250 - mae: 1119.0941 - val_loss: 1242984.5000 - val_mae: 1113.6277\n",
      "Epoch 6/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1248598.7500 - mae: 1116.2163\n",
      "Epoch 6: val_loss improved from 1242984.50000 to 1234152.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1247436.5000 - mae: 1115.6884 - val_loss: 1234152.8750 - val_mae: 1109.6266\n",
      "Epoch 7/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1239178.8750 - mae: 1111.9580\n",
      "Epoch 7: val_loss improved from 1234152.87500 to 1223805.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 1238362.0000 - mae: 1111.5884 - val_loss: 1223805.0000 - val_mae: 1104.9199\n",
      "Epoch 8/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1227592.7500 - mae: 1106.7089\n",
      "Epoch 8: val_loss improved from 1223805.00000 to 1212195.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 94ms/step - loss: 1227946.7500 - mae: 1106.8610 - val_loss: 1212195.7500 - val_mae: 1099.6127\n",
      "Epoch 9/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1215975.0000 - mae: 1101.3987\n",
      "Epoch 9: val_loss improved from 1212195.75000 to 1199393.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 12s 121ms/step - loss: 1216331.2500 - mae: 1101.5618 - val_loss: 1199393.3750 - val_mae: 1093.7246\n",
      "Epoch 10/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1204134.6250 - mae: 1095.9629\n",
      "Epoch 10: val_loss improved from 1199393.37500 to 1185569.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 1203644.8750 - mae: 1095.7433 - val_loss: 1185569.1250 - val_mae: 1087.3286\n",
      "Epoch 11/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1189938.2500 - mae: 1089.4176\n",
      "Epoch 11: val_loss improved from 1185569.12500 to 1170692.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 1189894.1250 - mae: 1089.3955 - val_loss: 1170692.2500 - val_mae: 1080.3990\n",
      "Epoch 12/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1175624.1250 - mae: 1082.7424\n",
      "Epoch 12: val_loss improved from 1170692.25000 to 1154858.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 98ms/step - loss: 1175207.6250 - mae: 1082.5691 - val_loss: 1154858.5000 - val_mae: 1072.9670\n",
      "Epoch 13/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1160697.2500 - mae: 1075.7754\n",
      "Epoch 13: val_loss improved from 1154858.50000 to 1138305.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 99ms/step - loss: 1159698.5000 - mae: 1075.3081 - val_loss: 1138305.6250 - val_mae: 1065.1340\n",
      "Epoch 14/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1143430.5000 - mae: 1067.6324\n",
      "Epoch 14: val_loss improved from 1138305.62500 to 1120978.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 95ms/step - loss: 1143430.5000 - mae: 1067.6324 - val_loss: 1120978.6250 - val_mae: 1056.8657\n",
      "Epoch 15/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1126120.1250 - mae: 1059.3800\n",
      "Epoch 15: val_loss improved from 1120978.62500 to 1102948.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1126465.8750 - mae: 1059.5609 - val_loss: 1102948.8750 - val_mae: 1048.1833\n",
      "Epoch 16/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1109302.6250 - mae: 1051.3204\n",
      "Epoch 16: val_loss improved from 1102948.87500 to 1084284.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 1108831.2500 - mae: 1051.0907 - val_loss: 1084284.3750 - val_mae: 1039.1111\n",
      "Epoch 17/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1090076.8750 - mae: 1042.0248\n",
      "Epoch 17: val_loss improved from 1084284.37500 to 1065054.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 1090594.8750 - mae: 1042.2550 - val_loss: 1065054.8750 - val_mae: 1029.6664\n",
      "Epoch 18/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1072267.2500 - mae: 1033.3033\n",
      "Epoch 18: val_loss improved from 1065054.87500 to 1045380.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1071871.1250 - mae: 1033.1053 - val_loss: 1045380.3750 - val_mae: 1019.9142\n",
      "Epoch 19/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1053713.7500 - mae: 1024.1428\n",
      "Epoch 19: val_loss improved from 1045380.37500 to 1025262.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 1052690.7500 - mae: 1023.6290 - val_loss: 1025262.0625 - val_mae: 1009.8310\n",
      "Epoch 20/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 1033285.8750 - mae: 1013.9156\n",
      "Epoch 20: val_loss improved from 1025262.06250 to 1004646.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 1033051.0625 - mae: 1013.8300 - val_loss: 1004646.0000 - val_mae: 999.3718\n",
      "Epoch 21/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1013394.1875 - mae: 1003.9075\n",
      "Epoch 21: val_loss improved from 1004646.00000 to 983788.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 1013019.5625 - mae: 1003.7137 - val_loss: 983788.0000 - val_mae: 988.6689\n",
      "Epoch 22/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 991670.3125 - mae: 992.8245\n",
      "Epoch 22: val_loss improved from 983788.00000 to 962588.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 992710.3125 - mae: 993.3575 - val_loss: 962588.4375 - val_mae: 977.6603\n",
      "Epoch 23/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 973053.3125 - mae: 983.2012\n",
      "Epoch 23: val_loss improved from 962588.43750 to 941097.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 67ms/step - loss: 972070.9375 - mae: 982.6972 - val_loss: 941097.4375 - val_mae: 966.3602\n",
      "Epoch 24/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 951065.3125 - mae: 971.7207\n",
      "Epoch 24: val_loss improved from 941097.43750 to 919421.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 951168.6875 - mae: 971.7652 - val_loss: 919421.6250 - val_mae: 954.8169\n",
      "Epoch 25/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 930975.0000 - mae: 961.0883\n",
      "Epoch 25: val_loss improved from 919421.62500 to 897537.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 930078.8125 - mae: 960.5958 - val_loss: 897537.8125 - val_mae: 942.9901\n",
      "Epoch 26/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 909030.7500 - mae: 949.3276\n",
      "Epoch 26: val_loss improved from 897537.81250 to 875550.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 908834.3125 - mae: 949.2160 - val_loss: 875550.6250 - val_mae: 930.9547\n",
      "Epoch 27/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 887313.7500 - mae: 937.5037\n",
      "Epoch 27: val_loss improved from 875550.62500 to 853410.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 887426.2500 - mae: 937.5861 - val_loss: 853410.5625 - val_mae: 918.6420\n",
      "Epoch 28/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 866698.6875 - mae: 926.1788\n",
      "Epoch 28: val_loss improved from 853410.56250 to 831264.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 865906.9375 - mae: 925.7164 - val_loss: 831264.0000 - val_mae: 906.1600\n",
      "Epoch 29/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 845051.4375 - mae: 914.0561\n",
      "Epoch 29: val_loss improved from 831264.00000 to 809000.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 844291.3125 - mae: 913.6254 - val_loss: 809000.5625 - val_mae: 893.4020\n",
      "Epoch 30/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 822476.5000 - mae: 901.2588\n",
      "Epoch 30: val_loss improved from 809000.56250 to 786725.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 822632.6250 - mae: 901.3442 - val_loss: 786725.4375 - val_mae: 880.4411\n",
      "Epoch 31/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 800827.6250 - mae: 888.6927\n",
      "Epoch 31: val_loss improved from 786725.43750 to 764462.62500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 67ms/step - loss: 800936.3125 - mae: 888.8411 - val_loss: 764462.6250 - val_mae: 867.2594\n",
      "Epoch 32/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 778746.5625 - mae: 875.8018\n",
      "Epoch 32: val_loss improved from 764462.62500 to 742304.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 779272.6250 - mae: 876.1871 - val_loss: 742304.8750 - val_mae: 853.9190\n",
      "Epoch 33/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 758499.0625 - mae: 863.8276\n",
      "Epoch 33: val_loss improved from 742304.87500 to 720233.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 757657.3750 - mae: 863.3133 - val_loss: 720233.5625 - val_mae: 840.3950\n",
      "Epoch 34/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 738097.7500 - mae: 851.4949\n",
      "Epoch 34: val_loss improved from 720233.56250 to 698235.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 736101.5625 - mae: 850.2454 - val_loss: 698235.5000 - val_mae: 826.6639\n",
      "Epoch 35/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 715458.5625 - mae: 837.4471\n",
      "Epoch 35: val_loss improved from 698235.50000 to 676377.31250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 714637.9375 - mae: 837.0410 - val_loss: 676377.3125 - val_mae: 812.7638\n",
      "Epoch 36/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 695800.6250 - mae: 825.3369\n",
      "Epoch 36: val_loss improved from 676377.31250 to 654679.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 693285.1875 - mae: 823.6678 - val_loss: 654679.5625 - val_mae: 798.6923\n",
      "Epoch 37/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 672084.9375 - mae: 810.2309\n",
      "Epoch 37: val_loss improved from 654679.56250 to 633164.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 672084.9375 - mae: 810.2309 - val_loss: 633164.7500 - val_mae: 784.4464\n",
      "Epoch 38/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 649780.6250 - mae: 795.7569\n",
      "Epoch 38: val_loss improved from 633164.75000 to 611888.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 651062.5625 - mae: 796.6683 - val_loss: 611888.0625 - val_mae: 770.1035\n",
      "Epoch 39/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 631089.7500 - mae: 783.5356\n",
      "Epoch 39: val_loss improved from 611888.06250 to 590821.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 630220.7500 - mae: 782.9363 - val_loss: 590821.8125 - val_mae: 755.5862\n",
      "Epoch 40/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 609616.3125 - mae: 768.9986\n",
      "Epoch 40: val_loss improved from 590821.81250 to 570038.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 609601.8125 - mae: 769.0013 - val_loss: 570038.0625 - val_mae: 740.9352\n",
      "Epoch 41/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 589291.4375 - mae: 755.1945\n",
      "Epoch 41: val_loss improved from 570038.06250 to 549545.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 589211.6250 - mae: 755.0778 - val_loss: 549545.5625 - val_mae: 726.1761\n",
      "Epoch 42/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 569383.1875 - mae: 741.4108\n",
      "Epoch 42: val_loss improved from 549545.56250 to 529388.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 96ms/step - loss: 569097.8125 - mae: 741.0450 - val_loss: 529388.5625 - val_mae: 711.3271\n",
      "Epoch 43/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 548936.8750 - mae: 726.5359\n",
      "Epoch 43: val_loss improved from 529388.56250 to 509537.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 549272.6250 - mae: 726.9971 - val_loss: 509537.4688 - val_mae: 696.5701\n",
      "Epoch 44/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 530283.6250 - mae: 713.0914\n",
      "Epoch 44: val_loss improved from 509537.46875 to 490087.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 529755.2500 - mae: 712.7538 - val_loss: 490087.8125 - val_mae: 681.7777\n",
      "Epoch 45/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 510347.1875 - mae: 698.1753\n",
      "Epoch 45: val_loss improved from 490087.81250 to 471026.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 510562.0938 - mae: 698.5323 - val_loss: 471026.2500 - val_mae: 666.8621\n",
      "Epoch 46/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 491739.6562 - mae: 684.2231\n",
      "Epoch 46: val_loss improved from 471026.25000 to 452300.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 491694.0938 - mae: 684.2206 - val_loss: 452300.2500 - val_mae: 651.8758\n",
      "Epoch 47/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 473994.1250 - mae: 670.5478\n",
      "Epoch 47: val_loss improved from 452300.25000 to 434038.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 473202.7812 - mae: 669.8375 - val_loss: 434038.1250 - val_mae: 636.8477\n",
      "Epoch 48/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 455024.2812 - mae: 655.3873\n",
      "Epoch 48: val_loss improved from 434038.12500 to 416217.34375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 455096.0938 - mae: 655.4566 - val_loss: 416217.3438 - val_mae: 621.7701\n",
      "Epoch 49/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 437663.9688 - mae: 641.0581\n",
      "Epoch 49: val_loss improved from 416217.34375 to 398816.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 437364.0625 - mae: 641.0420 - val_loss: 398816.8750 - val_mae: 606.6350\n",
      "Epoch 50/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 420147.3750 - mae: 626.7385\n",
      "Epoch 50: val_loss improved from 398816.87500 to 381919.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 95ms/step - loss: 420040.6875 - mae: 626.6574 - val_loss: 381919.3750 - val_mae: 591.6072\n",
      "Epoch 51/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 403135.3125 - mae: 612.1956\n",
      "Epoch 51: val_loss improved from 381919.37500 to 365420.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 403135.3125 - mae: 612.1956 - val_loss: 365420.0625 - val_mae: 576.7765\n",
      "Epoch 52/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 387945.9375 - mae: 598.9719\n",
      "Epoch 52: val_loss improved from 365420.06250 to 349553.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 386730.9375 - mae: 597.9173 - val_loss: 349553.7188 - val_mae: 562.3747\n",
      "Epoch 53/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 370665.2500 - mae: 583.4570\n",
      "Epoch 53: val_loss improved from 349553.71875 to 334005.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 370665.2500 - mae: 583.4570 - val_loss: 334005.1250 - val_mae: 547.7990\n",
      "Epoch 54/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 356732.3438 - mae: 570.4721\n",
      "Epoch 54: val_loss improved from 334005.12500 to 319022.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 355034.7812 - mae: 569.1084 - val_loss: 319022.2188 - val_mae: 533.2688\n",
      "Epoch 55/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 341021.4062 - mae: 555.7548\n",
      "Epoch 55: val_loss improved from 319022.21875 to 304553.37500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 339878.1875 - mae: 554.7679 - val_loss: 304553.3750 - val_mae: 518.7834\n",
      "Epoch 56/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 325268.7500 - mae: 540.6195\n",
      "Epoch 56: val_loss improved from 304553.37500 to 290593.65625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 325158.0625 - mae: 540.4683 - val_loss: 290593.6562 - val_mae: 504.3969\n",
      "Epoch 57/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 311856.7812 - mae: 527.1249\n",
      "Epoch 57: val_loss improved from 290593.65625 to 277086.84375, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 310916.2500 - mae: 526.1243 - val_loss: 277086.8438 - val_mae: 490.2970\n",
      "Epoch 58/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 295547.0000 - mae: 510.7010\n",
      "Epoch 58: val_loss improved from 277086.84375 to 264112.90625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 297159.3750 - mae: 511.9914 - val_loss: 264112.9062 - val_mae: 476.2878\n",
      "Epoch 59/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 284292.5312 - mae: 498.3375\n",
      "Epoch 59: val_loss improved from 264112.90625 to 251927.17188, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 284027.6875 - mae: 498.2209 - val_loss: 251927.1719 - val_mae: 462.7093\n",
      "Epoch 60/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 271395.9688 - mae: 484.6405\n",
      "Epoch 60: val_loss improved from 251927.17188 to 240025.75000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 271270.1875 - mae: 484.4319 - val_loss: 240025.7500 - val_mae: 448.9690\n",
      "Epoch 61/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 258972.1875 - mae: 470.9132\n",
      "Epoch 61: val_loss improved from 240025.75000 to 228624.76562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 258950.5625 - mae: 470.8581 - val_loss: 228624.7656 - val_mae: 435.5750\n",
      "Epoch 62/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 247353.2656 - mae: 458.5527\n",
      "Epoch 62: val_loss improved from 228624.76562 to 217682.00000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 247067.7812 - mae: 457.7397 - val_loss: 217682.0000 - val_mae: 422.6606\n",
      "Epoch 63/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 236253.7188 - mae: 445.1142\n",
      "Epoch 63: val_loss improved from 217682.00000 to 207273.96875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 235659.7344 - mae: 444.9525 - val_loss: 207273.9688 - val_mae: 410.0400\n",
      "Epoch 64/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 224104.8594 - mae: 431.9477\n",
      "Epoch 64: val_loss improved from 207273.96875 to 197374.32812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 224767.4375 - mae: 432.5057 - val_loss: 197374.3281 - val_mae: 397.8733\n",
      "Epoch 65/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 214579.5625 - mae: 420.6658\n",
      "Epoch 65: val_loss improved from 197374.32812 to 187990.76562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 77ms/step - loss: 214322.5469 - mae: 420.5569 - val_loss: 187990.7656 - val_mae: 386.2273\n",
      "Epoch 66/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 204298.2344 - mae: 408.8533\n",
      "Epoch 66: val_loss improved from 187990.76562 to 179042.12500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 204342.3750 - mae: 409.1266 - val_loss: 179042.1250 - val_mae: 375.1767\n",
      "Epoch 67/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 195044.1250 - mae: 398.5981\n",
      "Epoch 67: val_loss improved from 179042.12500 to 170538.43750, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 194782.0312 - mae: 398.2180 - val_loss: 170538.4375 - val_mae: 364.7550\n",
      "Epoch 68/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 185719.8750 - mae: 387.6502\n",
      "Epoch 68: val_loss improved from 170538.43750 to 162591.26562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 185719.8750 - mae: 387.6502 - val_loss: 162591.2656 - val_mae: 354.8344\n",
      "Epoch 69/200\n",
      "84/98 [========================>.....] - ETA: 0s - loss: 178114.8125 - mae: 379.4271\n",
      "Epoch 69: val_loss improved from 162591.26562 to 154972.01562, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 177058.6562 - mae: 377.6635 - val_loss: 154972.0156 - val_mae: 345.3038\n",
      "Epoch 70/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 170032.5312 - mae: 369.4052\n",
      "Epoch 70: val_loss improved from 154972.01562 to 147792.50000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 168803.5469 - mae: 367.9522 - val_loss: 147792.5000 - val_mae: 336.1937\n",
      "Epoch 71/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 159666.6719 - mae: 357.3074\n",
      "Epoch 71: val_loss improved from 147792.50000 to 141055.06250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 160970.7969 - mae: 358.5005 - val_loss: 141055.0625 - val_mae: 327.4444\n",
      "Epoch 72/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 153564.0000 - mae: 349.5862\n",
      "Epoch 72: val_loss improved from 141055.06250 to 134864.25000, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 153669.7031 - mae: 349.3625 - val_loss: 134864.2500 - val_mae: 319.3410\n",
      "Epoch 73/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 146755.8281 - mae: 340.8837\n",
      "Epoch 73: val_loss improved from 134864.25000 to 128992.78906, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 146639.9531 - mae: 340.6375 - val_loss: 128992.7891 - val_mae: 311.5237\n",
      "Epoch 74/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 140226.7969 - mae: 332.1333\n",
      "Epoch 74: val_loss improved from 128992.78906 to 123412.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 139984.8594 - mae: 331.7980 - val_loss: 123412.5312 - val_mae: 304.1227\n",
      "Epoch 75/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 135884.4531 - mae: 326.3253\n",
      "Epoch 75: val_loss improved from 123412.53125 to 118190.40625, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 133689.4844 - mae: 323.6447 - val_loss: 118190.4062 - val_mae: 297.1257\n",
      "Epoch 76/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 129516.2812 - mae: 317.9836\n",
      "Epoch 76: val_loss improved from 118190.40625 to 113314.21875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 127785.2188 - mae: 315.6397 - val_loss: 113314.2188 - val_mae: 290.2439\n",
      "Epoch 77/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 121582.5000 - mae: 307.8723\n",
      "Epoch 77: val_loss improved from 113314.21875 to 108710.69531, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 122124.0938 - mae: 307.7687 - val_loss: 108710.6953 - val_mae: 283.5065\n",
      "Epoch 78/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 117328.0547 - mae: 300.7264\n",
      "Epoch 78: val_loss improved from 108710.69531 to 104398.77344, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 116787.7891 - mae: 300.3448 - val_loss: 104398.7734 - val_mae: 276.9881\n",
      "Epoch 79/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 112548.1719 - mae: 294.0536\n",
      "Epoch 79: val_loss improved from 104398.77344 to 100332.60938, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 111738.4453 - mae: 292.8419 - val_loss: 100332.6094 - val_mae: 270.4967\n",
      "Epoch 80/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 106977.1016 - mae: 285.9092\n",
      "Epoch 80: val_loss improved from 100332.60938 to 96561.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 106967.3438 - mae: 285.9788 - val_loss: 96561.5312 - val_mae: 264.4043\n",
      "Epoch 81/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 102522.6797 - mae: 279.0675\n",
      "Epoch 81: val_loss improved from 96561.53125 to 93065.88281, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 102522.6797 - mae: 279.0675 - val_loss: 93065.8828 - val_mae: 258.6759\n",
      "Epoch 82/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 98827.6172 - mae: 273.1973\n",
      "Epoch 82: val_loss improved from 93065.88281 to 89749.63281, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 98305.3984 - mae: 272.4001 - val_loss: 89749.6328 - val_mae: 253.1349\n",
      "Epoch 83/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 94197.2734 - mae: 265.0371\n",
      "Epoch 83: val_loss improved from 89749.63281 to 86727.87500, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 94351.7031 - mae: 266.1113 - val_loss: 86727.8750 - val_mae: 247.9206\n",
      "Epoch 84/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 91288.5625 - mae: 260.6446\n",
      "Epoch 84: val_loss improved from 86727.87500 to 83850.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 90630.8438 - mae: 260.0475 - val_loss: 83850.5312 - val_mae: 242.6995\n",
      "Epoch 85/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 87652.7969 - mae: 254.9518\n",
      "Epoch 85: val_loss improved from 83850.53125 to 81140.47656, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 87141.9688 - mae: 254.1564 - val_loss: 81140.4766 - val_mae: 237.5697\n",
      "Epoch 86/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 82626.0078 - mae: 247.1387\n",
      "Epoch 86: val_loss improved from 81140.47656 to 78618.22656, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 83796.6797 - mae: 248.3678 - val_loss: 78618.2266 - val_mae: 232.7137\n",
      "Epoch 87/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 79526.5781 - mae: 241.5210\n",
      "Epoch 87: val_loss improved from 78618.22656 to 76217.81250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 80640.5312 - mae: 242.6900 - val_loss: 76217.8125 - val_mae: 228.0313\n",
      "Epoch 88/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 77395.1094 - mae: 236.5069\n",
      "Epoch 88: val_loss improved from 76217.81250 to 73953.67969, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 77634.6875 - mae: 237.0687 - val_loss: 73953.6797 - val_mae: 223.6646\n",
      "Epoch 89/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 74809.4375 - mae: 231.8004\n",
      "Epoch 89: val_loss improved from 73953.67969 to 71848.94531, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 74809.4375 - mae: 231.8004 - val_loss: 71848.9453 - val_mae: 219.5130\n",
      "Epoch 90/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 71022.7656 - mae: 225.5506\n",
      "Epoch 90: val_loss improved from 71848.94531 to 69855.71875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 72151.4453 - mae: 226.6393 - val_loss: 69855.7188 - val_mae: 215.4379\n",
      "Epoch 91/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 71057.5312 - mae: 223.7959\n",
      "Epoch 91: val_loss improved from 69855.71875 to 67914.44531, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 69608.0703 - mae: 221.7352 - val_loss: 67914.4453 - val_mae: 211.3899\n",
      "Epoch 92/200\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 68512.6016 - mae: 217.8669\n",
      "Epoch 92: val_loss improved from 67914.44531 to 66156.21094, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 67225.7656 - mae: 216.8880 - val_loss: 66156.2109 - val_mae: 207.6121\n",
      "Epoch 93/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 65346.7383 - mae: 212.6899\n",
      "Epoch 93: val_loss improved from 66156.21094 to 64382.79688, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 64953.0039 - mae: 212.1294 - val_loss: 64382.7969 - val_mae: 203.7585\n",
      "Epoch 94/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 62788.2617 - mae: 207.6103\n",
      "Epoch 94: val_loss improved from 64382.79688 to 62773.83594, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 62788.2617 - mae: 207.6103 - val_loss: 62773.8359 - val_mae: 200.2130\n",
      "Epoch 95/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 60742.0547 - mae: 203.4587\n",
      "Epoch 95: val_loss improved from 62773.83594 to 61284.30859, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 60749.6016 - mae: 203.1754 - val_loss: 61284.3086 - val_mae: 196.7308\n",
      "Epoch 96/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 58916.6211 - mae: 198.9684\n",
      "Epoch 96: val_loss improved from 61284.30859 to 59751.34766, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 58782.0977 - mae: 198.9072 - val_loss: 59751.3477 - val_mae: 193.2909\n",
      "Epoch 97/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 56352.1484 - mae: 194.6997\n",
      "Epoch 97: val_loss improved from 59751.34766 to 58247.55859, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 56899.9102 - mae: 194.8216 - val_loss: 58247.5586 - val_mae: 189.7984\n",
      "Epoch 98/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 55048.0742 - mae: 190.1432\n",
      "Epoch 98: val_loss improved from 58247.55859 to 56882.05469, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 55105.4062 - mae: 190.6423 - val_loss: 56882.0547 - val_mae: 186.5573\n",
      "Epoch 99/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 53890.9492 - mae: 187.0173\n",
      "Epoch 99: val_loss improved from 56882.05469 to 55499.01953, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 53368.1406 - mae: 186.8573 - val_loss: 55499.0195 - val_mae: 183.3460\n",
      "Epoch 100/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 53191.0469 - mae: 184.6725\n",
      "Epoch 100: val_loss improved from 55499.01953 to 54141.08594, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 51673.4258 - mae: 182.9121 - val_loss: 54141.0859 - val_mae: 180.0938\n",
      "Epoch 101/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 50121.0859 - mae: 179.6398\n",
      "Epoch 101: val_loss improved from 54141.08594 to 52895.17578, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 50090.7305 - mae: 179.1513 - val_loss: 52895.1758 - val_mae: 177.0322\n",
      "Epoch 102/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 48528.6445 - mae: 175.5874\n",
      "Epoch 102: val_loss improved from 52895.17578 to 51644.84766, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 79ms/step - loss: 48515.8398 - mae: 175.6359 - val_loss: 51644.8477 - val_mae: 174.0084\n",
      "Epoch 103/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 47305.6250 - mae: 171.9718\n",
      "Epoch 103: val_loss improved from 51644.84766 to 50393.98438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 63ms/step - loss: 47023.7227 - mae: 172.1254 - val_loss: 50393.9844 - val_mae: 171.0088\n",
      "Epoch 104/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 45568.1445 - mae: 168.6584\n",
      "Epoch 104: val_loss improved from 50393.98438 to 49198.30859, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 45561.9961 - mae: 168.6812 - val_loss: 49198.3086 - val_mae: 168.0025\n",
      "Epoch 105/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 44639.6719 - mae: 165.6857\n",
      "Epoch 105: val_loss improved from 49198.30859 to 48009.39453, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 44160.5156 - mae: 165.1386 - val_loss: 48009.3945 - val_mae: 164.9670\n",
      "Epoch 106/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 42140.7852 - mae: 161.9910\n",
      "Epoch 106: val_loss improved from 48009.39453 to 46868.55469, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 42781.6914 - mae: 161.7114 - val_loss: 46868.5547 - val_mae: 162.0375\n",
      "Epoch 107/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 41698.3789 - mae: 157.6233\n",
      "Epoch 107: val_loss improved from 46868.55469 to 45700.07812, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 41462.0742 - mae: 158.5152 - val_loss: 45700.0781 - val_mae: 159.2285\n",
      "Epoch 108/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 40542.3242 - mae: 155.8327\n",
      "Epoch 108: val_loss improved from 45700.07812 to 44529.77344, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 40166.7891 - mae: 155.3897 - val_loss: 44529.7734 - val_mae: 156.3620\n",
      "Epoch 109/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 38507.6133 - mae: 152.2394\n",
      "Epoch 109: val_loss improved from 44529.77344 to 43424.95703, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 38912.1016 - mae: 152.1877 - val_loss: 43424.9570 - val_mae: 153.5398\n",
      "Epoch 110/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 37397.2734 - mae: 148.5385\n",
      "Epoch 110: val_loss improved from 43424.95703 to 42327.44531, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 37681.5703 - mae: 148.9485 - val_loss: 42327.4453 - val_mae: 150.6549\n",
      "Epoch 111/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 35936.3516 - mae: 145.6669\n",
      "Epoch 111: val_loss improved from 42327.44531 to 41230.87109, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 36490.4141 - mae: 145.9631 - val_loss: 41230.8711 - val_mae: 147.8745\n",
      "Epoch 112/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 35326.3008 - mae: 142.7270\n",
      "Epoch 112: val_loss improved from 41230.87109 to 40143.45312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 35308.5508 - mae: 142.7560 - val_loss: 40143.4531 - val_mae: 145.0844\n",
      "Epoch 113/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 32013.3047 - mae: 139.2674\n",
      "Epoch 113: val_loss improved from 40143.45312 to 39125.80078, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 34169.4961 - mae: 139.7795 - val_loss: 39125.8008 - val_mae: 142.3032\n",
      "Epoch 114/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 33454.5312 - mae: 137.2773\n",
      "Epoch 114: val_loss improved from 39125.80078 to 38049.53125, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 33061.6172 - mae: 136.8010 - val_loss: 38049.5312 - val_mae: 139.6159\n",
      "Epoch 115/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 32717.1719 - mae: 134.9520\n",
      "Epoch 115: val_loss improved from 38049.53125 to 37012.70312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 31975.6562 - mae: 133.8398 - val_loss: 37012.7031 - val_mae: 136.9476\n",
      "Epoch 116/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 30926.0273 - mae: 131.0361\n",
      "Epoch 116: val_loss improved from 37012.70312 to 36017.48438, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 63ms/step - loss: 30926.0273 - mae: 131.0361 - val_loss: 36017.4844 - val_mae: 134.3616\n",
      "Epoch 117/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 29335.5547 - mae: 127.4347\n",
      "Epoch 117: val_loss improved from 36017.48438 to 35022.27344, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 29897.7227 - mae: 128.2532 - val_loss: 35022.2734 - val_mae: 131.7286\n",
      "Epoch 118/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 29532.4805 - mae: 126.5139\n",
      "Epoch 118: val_loss improved from 35022.27344 to 34004.52344, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 28881.2734 - mae: 125.5165 - val_loss: 34004.5234 - val_mae: 129.1251\n",
      "Epoch 119/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 27781.9043 - mae: 122.3393\n",
      "Epoch 119: val_loss improved from 34004.52344 to 33059.70312, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 27919.2266 - mae: 122.6914 - val_loss: 33059.7031 - val_mae: 126.5697\n",
      "Epoch 120/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 26286.2148 - mae: 119.0013\n",
      "Epoch 120: val_loss improved from 33059.70312 to 32157.66602, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 26959.9824 - mae: 120.0343 - val_loss: 32157.6660 - val_mae: 124.1215\n",
      "Epoch 121/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 24452.1504 - mae: 116.0208\n",
      "Epoch 121: val_loss improved from 32157.66602 to 31228.78320, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 26024.7051 - mae: 117.3204 - val_loss: 31228.7832 - val_mae: 121.5457\n",
      "Epoch 122/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 25215.7812 - mae: 114.8109\n",
      "Epoch 122: val_loss improved from 31228.78320 to 30289.95117, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 25123.8672 - mae: 114.7743 - val_loss: 30289.9512 - val_mae: 119.1002\n",
      "Epoch 123/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 24901.8535 - mae: 112.3892\n",
      "Epoch 123: val_loss improved from 30289.95117 to 29315.56250, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 24201.2148 - mae: 112.1845 - val_loss: 29315.5625 - val_mae: 116.6141\n",
      "Epoch 124/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 21731.0898 - mae: 108.7756\n",
      "Epoch 124: val_loss improved from 29315.56250 to 28351.82227, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 23271.2773 - mae: 109.3620 - val_loss: 28351.8223 - val_mae: 113.8526\n",
      "Epoch 125/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 22495.1133 - mae: 106.5224\n",
      "Epoch 125: val_loss improved from 28351.82227 to 27187.16211, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 22275.9102 - mae: 106.2953 - val_loss: 27187.1621 - val_mae: 111.0117\n",
      "Epoch 126/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 19537.8672 - mae: 102.0161\n",
      "Epoch 126: val_loss improved from 27187.16211 to 26009.14648, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 21248.2383 - mae: 102.9671 - val_loss: 26009.1465 - val_mae: 107.8143\n",
      "Epoch 127/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 20475.0977 - mae: 100.5567\n",
      "Epoch 127: val_loss improved from 26009.14648 to 24786.60156, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 20177.1387 - mae: 99.7303 - val_loss: 24786.6016 - val_mae: 104.4573\n",
      "Epoch 128/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 19430.3809 - mae: 97.2449\n",
      "Epoch 128: val_loss improved from 24786.60156 to 23699.06445, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 19116.7363 - mae: 96.4766 - val_loss: 23699.0645 - val_mae: 101.1363\n",
      "Epoch 129/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 18465.8359 - mae: 93.2504\n",
      "Epoch 129: val_loss improved from 23699.06445 to 22658.46875, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 18022.2617 - mae: 92.8158 - val_loss: 22658.4688 - val_mae: 97.6846\n",
      "Epoch 130/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 17367.3379 - mae: 90.2310\n",
      "Epoch 130: val_loss improved from 22658.46875 to 21710.60742, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 16956.8691 - mae: 89.4852 - val_loss: 21710.6074 - val_mae: 94.3787\n",
      "Epoch 131/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 16392.4727 - mae: 86.5522\n",
      "Epoch 131: val_loss improved from 21710.60742 to 20769.69727, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 15945.0635 - mae: 86.1173 - val_loss: 20769.6973 - val_mae: 90.9788\n",
      "Epoch 132/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 15679.0771 - mae: 83.6027\n",
      "Epoch 132: val_loss improved from 20769.69727 to 19893.75391, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 14987.1250 - mae: 82.7876 - val_loss: 19893.7539 - val_mae: 87.7929\n",
      "Epoch 133/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 13363.1250 - mae: 78.8052\n",
      "Epoch 133: val_loss improved from 19893.75391 to 19075.51758, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 14104.2549 - mae: 79.4203 - val_loss: 19075.5176 - val_mae: 84.5825\n",
      "Epoch 134/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 13057.8291 - mae: 76.4752\n",
      "Epoch 134: val_loss improved from 19075.51758 to 18252.00977, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 13297.2412 - mae: 76.2740 - val_loss: 18252.0098 - val_mae: 81.4829\n",
      "Epoch 135/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 12769.1572 - mae: 73.4401\n",
      "Epoch 135: val_loss improved from 18252.00977 to 17392.77344, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 12532.8047 - mae: 73.1089 - val_loss: 17392.7734 - val_mae: 78.4261\n",
      "Epoch 136/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 12286.7861 - mae: 71.0954\n",
      "Epoch 136: val_loss improved from 17392.77344 to 16582.90039, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 67ms/step - loss: 11842.9951 - mae: 70.1249 - val_loss: 16582.9004 - val_mae: 75.4898\n",
      "Epoch 137/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 11184.7227 - mae: 66.8745\n",
      "Epoch 137: val_loss improved from 16582.90039 to 15813.78711, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 11196.3477 - mae: 67.1768 - val_loss: 15813.7871 - val_mae: 72.4513\n",
      "Epoch 138/200\n",
      "93/98 [===========================>..] - ETA: 0s - loss: 10146.3975 - mae: 63.7785\n",
      "Epoch 138: val_loss improved from 15813.78711 to 15100.43066, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 10594.4365 - mae: 64.3391 - val_loss: 15100.4307 - val_mae: 69.6004\n",
      "Epoch 139/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 10290.1455 - mae: 61.8255\n",
      "Epoch 139: val_loss improved from 15100.43066 to 14270.20215, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 10012.2158 - mae: 61.6512 - val_loss: 14270.2021 - val_mae: 66.5101\n",
      "Epoch 140/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 8996.6143 - mae: 58.0149\n",
      "Epoch 140: val_loss improved from 14270.20215 to 13287.12305, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 9414.3125 - mae: 58.3939 - val_loss: 13287.1230 - val_mae: 62.9524\n",
      "Epoch 141/200\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 8810.2617 - mae: 54.9165\n",
      "Epoch 141: val_loss improved from 13287.12305 to 12009.35645, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 67ms/step - loss: 8803.4727 - mae: 55.0504 - val_loss: 12009.3564 - val_mae: 59.2932\n",
      "Epoch 142/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 8189.4570 - mae: 51.9423\n",
      "Epoch 142: val_loss improved from 12009.35645 to 11092.22656, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 8261.6914 - mae: 52.2144 - val_loss: 11092.2266 - val_mae: 56.2851\n",
      "Epoch 143/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 7419.7686 - mae: 49.3093\n",
      "Epoch 143: val_loss improved from 11092.22656 to 10352.57324, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 7778.9404 - mae: 49.5836 - val_loss: 10352.5732 - val_mae: 53.7944\n",
      "Epoch 144/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 7580.7773 - mae: 47.7556\n",
      "Epoch 144: val_loss improved from 10352.57324 to 9788.04590, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 7364.4956 - mae: 47.4656 - val_loss: 9788.0459 - val_mae: 51.6759\n",
      "Epoch 145/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 6984.9629 - mae: 45.3563\n",
      "Epoch 145: val_loss improved from 9788.04590 to 9266.76465, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 6984.9629 - mae: 45.3563 - val_loss: 9266.7646 - val_mae: 49.9004\n",
      "Epoch 146/200\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 6958.6372 - mae: 44.1814\n",
      "Epoch 146: val_loss improved from 9266.76465 to 8573.98828, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 6622.6011 - mae: 43.8369 - val_loss: 8573.9883 - val_mae: 48.1375\n",
      "Epoch 147/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 6076.4922 - mae: 41.8910\n",
      "Epoch 147: val_loss improved from 8573.98828 to 7777.71973, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 6274.2368 - mae: 41.7948 - val_loss: 7777.7197 - val_mae: 46.2115\n",
      "Epoch 148/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 5887.6021 - mae: 40.1546\n",
      "Epoch 148: val_loss improved from 7777.71973 to 6859.73828, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 5927.6040 - mae: 40.1583 - val_loss: 6859.7383 - val_mae: 44.2869\n",
      "Epoch 149/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 5670.8384 - mae: 38.9352\n",
      "Epoch 149: val_loss improved from 6859.73828 to 6339.34570, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 5609.4165 - mae: 38.7239 - val_loss: 6339.3457 - val_mae: 42.6451\n",
      "Epoch 150/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 5352.5962 - mae: 37.5523\n",
      "Epoch 150: val_loss improved from 6339.34570 to 5959.74609, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 5294.0576 - mae: 37.3257 - val_loss: 5959.7461 - val_mae: 41.1369\n",
      "Epoch 151/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 5004.1260 - mae: 35.6603\n",
      "Epoch 151: val_loss improved from 5959.74609 to 5615.11230, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 5004.1260 - mae: 35.6603 - val_loss: 5615.1123 - val_mae: 39.6972\n",
      "Epoch 152/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 4736.9434 - mae: 34.3650\n",
      "Epoch 152: val_loss improved from 5615.11230 to 5249.89893, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 4736.9434 - mae: 34.3650 - val_loss: 5249.8989 - val_mae: 38.3563\n",
      "Epoch 153/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 4508.5073 - mae: 33.0729\n",
      "Epoch 153: val_loss improved from 5249.89893 to 4897.08496, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 4486.2383 - mae: 33.1801 - val_loss: 4897.0850 - val_mae: 36.9520\n",
      "Epoch 154/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 4255.4473 - mae: 31.8166\n",
      "Epoch 154: val_loss improved from 4897.08496 to 4584.54297, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 4248.1313 - mae: 31.9462 - val_loss: 4584.5430 - val_mae: 35.6212\n",
      "Epoch 155/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 4048.5569 - mae: 30.6795\n",
      "Epoch 155: val_loss improved from 4584.54297 to 4282.83789, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 4018.7942 - mae: 30.6602 - val_loss: 4282.8379 - val_mae: 34.2710\n",
      "Epoch 156/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 3800.6401 - mae: 29.5790\n",
      "Epoch 156: val_loss improved from 4282.83789 to 4004.68311, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 3800.6401 - mae: 29.5790 - val_loss: 4004.6831 - val_mae: 33.0026\n",
      "Epoch 157/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 3668.7593 - mae: 28.6837\n",
      "Epoch 157: val_loss improved from 4004.68311 to 3752.12939, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 68ms/step - loss: 3588.9441 - mae: 28.5138 - val_loss: 3752.1294 - val_mae: 31.9342\n",
      "Epoch 158/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 3493.3604 - mae: 27.6796\n",
      "Epoch 158: val_loss improved from 3752.12939 to 3513.94409, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 3386.6287 - mae: 27.5146 - val_loss: 3513.9441 - val_mae: 30.6556\n",
      "Epoch 159/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 3197.5361 - mae: 26.5500\n",
      "Epoch 159: val_loss improved from 3513.94409 to 3307.03638, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 3192.4575 - mae: 26.5362 - val_loss: 3307.0364 - val_mae: 29.6439\n",
      "Epoch 160/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 3050.9331 - mae: 25.8649\n",
      "Epoch 160: val_loss improved from 3307.03638 to 3091.90356, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 3008.4551 - mae: 25.7803 - val_loss: 3091.9036 - val_mae: 28.5261\n",
      "Epoch 161/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 2734.3386 - mae: 24.5234\n",
      "Epoch 161: val_loss improved from 3091.90356 to 2913.86084, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 2819.3887 - mae: 24.7476 - val_loss: 2913.8608 - val_mae: 27.5498\n",
      "Epoch 162/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 2615.7620 - mae: 23.9231\n",
      "Epoch 162: val_loss improved from 2913.86084 to 2732.73608, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 2615.7620 - mae: 23.9231 - val_loss: 2732.7361 - val_mae: 26.6867\n",
      "Epoch 163/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 2413.2300 - mae: 23.0323\n",
      "Epoch 163: val_loss improved from 2732.73608 to 2570.53711, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 71ms/step - loss: 2408.6443 - mae: 22.9967 - val_loss: 2570.5371 - val_mae: 25.8084\n",
      "Epoch 164/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 2243.4192 - mae: 22.0705\n",
      "Epoch 164: val_loss improved from 2570.53711 to 2428.04443, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 2243.4192 - mae: 22.0705 - val_loss: 2428.0444 - val_mae: 25.0204\n",
      "Epoch 165/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 2121.8242 - mae: 21.3367\n",
      "Epoch 165: val_loss improved from 2428.04443 to 2291.64038, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 2118.9084 - mae: 21.3386 - val_loss: 2291.6404 - val_mae: 24.1351\n",
      "Epoch 166/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 2079.2856 - mae: 20.6775\n",
      "Epoch 166: val_loss improved from 2291.64038 to 2168.10132, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 2010.1765 - mae: 20.5934 - val_loss: 2168.1013 - val_mae: 23.4536\n",
      "Epoch 167/200\n",
      "85/98 [=========================>....] - ETA: 0s - loss: 2030.2955 - mae: 20.5066\n",
      "Epoch 167: val_loss improved from 2168.10132 to 2048.19067, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 1904.7753 - mae: 20.1259 - val_loss: 2048.1907 - val_mae: 22.7352\n",
      "Epoch 168/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 1794.7327 - mae: 19.2884\n",
      "Epoch 168: val_loss improved from 2048.19067 to 1942.79932, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 1808.3014 - mae: 19.4626 - val_loss: 1942.7993 - val_mae: 22.0445\n",
      "Epoch 169/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1749.4792 - mae: 19.1633\n",
      "Epoch 169: val_loss improved from 1942.79932 to 1843.67175, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1718.6937 - mae: 19.0750 - val_loss: 1843.6718 - val_mae: 21.5322\n",
      "Epoch 170/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1635.2906 - mae: 18.6330\n",
      "Epoch 170: val_loss improved from 1843.67175 to 1750.19141, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 1631.2611 - mae: 18.6190 - val_loss: 1750.1914 - val_mae: 20.8731\n",
      "Epoch 171/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 1571.5699 - mae: 18.1262\n",
      "Epoch 171: val_loss improved from 1750.19141 to 1659.08337, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 1550.8701 - mae: 18.0671 - val_loss: 1659.0834 - val_mae: 20.3927\n",
      "Epoch 172/200\n",
      "96/98 [============================>.] - ETA: 0s - loss: 1485.8428 - mae: 17.7015\n",
      "Epoch 172: val_loss improved from 1659.08337 to 1574.95630, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 97ms/step - loss: 1471.5273 - mae: 17.6439 - val_loss: 1574.9563 - val_mae: 19.9022\n",
      "Epoch 173/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1462.7828 - mae: 17.5723\n",
      "Epoch 173: val_loss improved from 1574.95630 to 1482.43457, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 1402.5959 - mae: 17.3263 - val_loss: 1482.4346 - val_mae: 19.4375\n",
      "Epoch 174/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1266.8917 - mae: 17.0088\n",
      "Epoch 174: val_loss improved from 1482.43457 to 1410.70593, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 72ms/step - loss: 1330.7313 - mae: 17.0542 - val_loss: 1410.7059 - val_mae: 18.8478\n",
      "Epoch 175/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1269.4723 - mae: 16.4944\n",
      "Epoch 175: val_loss improved from 1410.70593 to 1343.98059, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1269.4723 - mae: 16.4944 - val_loss: 1343.9806 - val_mae: 18.3402\n",
      "Epoch 176/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1266.5105 - mae: 16.4265\n",
      "Epoch 176: val_loss improved from 1343.98059 to 1279.04858, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 1208.2205 - mae: 16.2985 - val_loss: 1279.0486 - val_mae: 17.9464\n",
      "Epoch 177/200\n",
      "89/98 [==========================>...] - ETA: 0s - loss: 1097.6271 - mae: 15.7993\n",
      "Epoch 177: val_loss improved from 1279.04858 to 1221.30066, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 70ms/step - loss: 1151.6747 - mae: 15.9612 - val_loss: 1221.3007 - val_mae: 17.4791\n",
      "Epoch 178/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 1079.5227 - mae: 15.3790\n",
      "Epoch 178: val_loss improved from 1221.30066 to 1163.60095, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 1101.5157 - mae: 15.6550 - val_loss: 1163.6010 - val_mae: 17.0058\n",
      "Epoch 179/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1053.1835 - mae: 15.4488\n",
      "Epoch 179: val_loss improved from 1163.60095 to 1123.75342, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 1053.1835 - mae: 15.4488 - val_loss: 1123.7534 - val_mae: 16.6415\n",
      "Epoch 180/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 1009.4160 - mae: 15.1428\n",
      "Epoch 180: val_loss improved from 1123.75342 to 1074.51709, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 1009.4160 - mae: 15.1428 - val_loss: 1074.5171 - val_mae: 16.2356\n",
      "Epoch 181/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1000.6918 - mae: 14.9769\n",
      "Epoch 181: val_loss improved from 1074.51709 to 1035.91882, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 965.9564 - mae: 15.0146 - val_loss: 1035.9188 - val_mae: 15.8692\n",
      "Epoch 182/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 904.9429 - mae: 14.5205\n",
      "Epoch 182: val_loss improved from 1035.91882 to 1005.59979, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 925.1407 - mae: 14.6021 - val_loss: 1005.5998 - val_mae: 15.5913\n",
      "Epoch 183/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 886.5719 - mae: 14.3724\n",
      "Epoch 183: val_loss improved from 1005.59979 to 974.31079, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 97ms/step - loss: 885.8946 - mae: 14.3801 - val_loss: 974.3108 - val_mae: 15.4393\n",
      "Epoch 184/200\n",
      "90/98 [==========================>...] - ETA: 0s - loss: 854.6567 - mae: 14.0728\n",
      "Epoch 184: val_loss improved from 974.31079 to 948.79224, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 80ms/step - loss: 850.0018 - mae: 14.1552 - val_loss: 948.7922 - val_mae: 15.2296\n",
      "Epoch 185/200\n",
      "97/98 [============================>.] - ETA: 0s - loss: 814.1617 - mae: 13.9363\n",
      "Epoch 185: val_loss improved from 948.79224 to 928.72589, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 813.5106 - mae: 13.9387 - val_loss: 928.7259 - val_mae: 15.0293\n",
      "Epoch 186/200\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 764.4611 - mae: 13.4752\n",
      "Epoch 186: val_loss improved from 928.72589 to 905.27686, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 779.5822 - mae: 13.6431 - val_loss: 905.2769 - val_mae: 14.8495\n",
      "Epoch 187/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 778.5534 - mae: 13.7302\n",
      "Epoch 187: val_loss improved from 905.27686 to 892.62811, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 75ms/step - loss: 747.9303 - mae: 13.6098 - val_loss: 892.6281 - val_mae: 14.7669\n",
      "Epoch 188/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 726.7067 - mae: 13.2752\n",
      "Epoch 188: val_loss improved from 892.62811 to 879.14832, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 716.3479 - mae: 13.2505 - val_loss: 879.1483 - val_mae: 14.5620\n",
      "Epoch 189/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 708.8472 - mae: 13.1301\n",
      "Epoch 189: val_loss improved from 879.14832 to 863.96423, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 690.3743 - mae: 13.1326 - val_loss: 863.9642 - val_mae: 14.5681\n",
      "Epoch 190/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 603.9644 - mae: 12.8413\n",
      "Epoch 190: val_loss improved from 863.96423 to 856.90363, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 660.5160 - mae: 12.9774 - val_loss: 856.9036 - val_mae: 14.2709\n",
      "Epoch 191/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 636.2236 - mae: 12.7851\n",
      "Epoch 191: val_loss improved from 856.90363 to 835.64465, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 635.0682 - mae: 12.7803 - val_loss: 835.6447 - val_mae: 14.0500\n",
      "Epoch 192/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 629.3107 - mae: 12.5896\n",
      "Epoch 192: val_loss improved from 835.64465 to 831.86078, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 610.4250 - mae: 12.5462 - val_loss: 831.8608 - val_mae: 14.1106\n",
      "Epoch 193/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 612.9006 - mae: 12.5928\n",
      "Epoch 193: val_loss improved from 831.86078 to 819.51117, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 585.8103 - mae: 12.4835 - val_loss: 819.5112 - val_mae: 13.8358\n",
      "Epoch 194/200\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 585.6022 - mae: 12.3227\n",
      "Epoch 194: val_loss improved from 819.51117 to 803.20966, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 76ms/step - loss: 560.2765 - mae: 12.2391 - val_loss: 803.2097 - val_mae: 13.6544\n",
      "Epoch 195/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 539.2150 - mae: 12.1291\n",
      "Epoch 195: val_loss improved from 803.20966 to 783.39355, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 539.2150 - mae: 12.1291 - val_loss: 783.3936 - val_mae: 13.3955\n",
      "Epoch 196/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 510.5198 - mae: 11.8577\n",
      "Epoch 196: val_loss improved from 783.39355 to 782.61310, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 516.8368 - mae: 11.8552 - val_loss: 782.6131 - val_mae: 13.3440\n",
      "Epoch 197/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 517.4120 - mae: 12.1149\n",
      "Epoch 197: val_loss improved from 782.61310 to 761.68829, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 7s 77ms/step - loss: 497.1489 - mae: 11.9372 - val_loss: 761.6883 - val_mae: 13.2471\n",
      "Epoch 198/200\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 477.4312 - mae: 11.6367\n",
      "Epoch 198: val_loss improved from 761.68829 to 759.33740, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 8s 78ms/step - loss: 474.9243 - mae: 11.6592 - val_loss: 759.3374 - val_mae: 13.0408\n",
      "Epoch 199/200\n",
      "95/98 [============================>.] - ETA: 0s - loss: 462.1150 - mae: 11.5246\n",
      "Epoch 199: val_loss improved from 759.33740 to 735.27753, saving model to .\\\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "98/98 [==============================] - 9s 96ms/step - loss: 456.9351 - mae: 11.5286 - val_loss: 735.2775 - val_mae: 12.9221\n",
      "Epoch 200/200\n",
      "98/98 [==============================] - ETA: 0s - loss: 438.0416 - mae: 11.3526\n",
      "Epoch 200: val_loss did not improve from 735.27753\n",
      "98/98 [==============================] - 1s 7ms/step - loss: 438.0416 - mae: 11.3526 - val_loss: 745.3318 - val_mae: 12.7860\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='relu', \n",
    "               return_sequences=False)\n",
    "          )\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델의 컴파일: 모델학습을 위한 학습과정 설정단계\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "modelpath = './'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "# filename = os.path.join(file_path=model_path, 'tmp_checkpoint.h5')\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# 테스트 정확도 출력                    \n",
    "# print(\"\\n Accuracy: %.4f\" % model.evaluate(x_valid, y_valid)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25a92ef4a00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_feature)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(test_label, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a073e8c3e7241d11bd26aeacf78abcb385fecb9ce1a8fb10ee8efd278424d43e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
